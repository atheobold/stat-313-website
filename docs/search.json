[
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "",
    "text": "Instructor: Dr. Allison Theobold – call me Dr. Theobold or Dr. T! I use they / she pronouns (that is my order of preference).\nOf course, if your question is truly private, such as a grade inquiry or a personal concern, you may send me a private email. To help both you and me, here are some specific expectations about emails:",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "3.1 Textbooks",
    "text": "3.1 Textbooks\n\n\n\n\n\nÇetinkaya-Rundel and Hardin, Introduction to Modern Statistics. https://openintro-ims.netlify.app/\n\n\n\n\n\n\n\n\nIsmay & Kim, Modern Dive: Statistical Inference via Data Science. https://moderndive.com",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#required-technology",
    "href": "course-syllabus.html#required-technology",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "3.2 Required Technology",
    "text": "3.2 Required Technology\n\n\n\n\n\nR is the statistical software we will be using in this course (https://cran.r-project.org/)\n\n\n\n\n\n\n\n\nRStudio is the most popular way to interact with the R software. We will be interacting with RStudio through Posit Cloud (Posit is the company that owns RStudio). You will join the Stat 313 workspace, and then be able to access the course homework and lab assignments. We will be walking through this in the first week of lab!\n\n\n\n\n\n\n\n\nFor questions of general interest, such as course clarifications or conceptual questions, please use the Class Discord Server. Refer to the Day One Class Setup materials for more information on how to effectively use this server.\n\n\n\n\n\n\n\n\n\n\nPosit Cloud Student Subscription\n\n\n\nFor this course, you will be required to enroll in the Cloud Student, which costs $5 per month. The subscription plan gives you access to the STAT 313 workspace, 75-hours of computing time each month, and collaborative editing.\nNote, to qualify for the student subscription you must use your Cal Poly email address for your account.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#my-course-goals",
    "href": "course-syllabus.html#my-course-goals",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "4.1 My course goals",
    "text": "4.1 My course goals\n\n\nData Visualization & Summarization\n\ncreate visualizations for one and two numerical variables\nuse facets and / or color to include additional variables into a visualization\ncalculate numerical summaries of variables\nfind summaries of variables across multiple groups\n\nWorking with Data & Reproducibility\n\nselect necessary columns from a dataset\nfilter rows from a dataset for numerical and categorical variables\nmodify existing numerical and categorical variables and / or create new variables\ncreate professional-looking, reproducible analyses using Posit projects, Quarto documents, and the here package\n\nLinear Models & Model Selection\n\nidentify which linear model is appropriate for a given research question\ndescribe the conditions required to obtain reliable estimates from linear models\nuse visualizations, summary statistics, and critical thinking to evaluate if linear model conditions are violated\nidentify methods to remedy condition violations\nfit additive and interactive linear models in R\ninterpret the coefficient estimates of a linear model\nuse visualizations and model selection techniques to determine if a specific variable should be included in a linear model\n\n\n\n\nStudy Design\n\ndistinguish between an experiment and an observational study\nidentify sources of variation and describe how to account for them\nargue what population a given sample is representative of\n\nFundamentals of Statistical Inference\n\nidentify the parameter of interest for a given linear model and associated research question\noutline the null (\\(H_0\\)) and alternative (\\(H_A\\)) hypotheses for a given research question\ndescribe what a null distribution is and how it is used to obtain a p-value\ninterpret a p-value in the context of a research question\nuse a p-value to make a decision about a hypothesis test and reach a conclusion about a research question\ndistinguish between Type I and Type II errors\ndescribe how sample size and significance level effect Type I and Type II errors\noutline the strengths and weaknesses of significance testing\ndescribe what a bootstrap distribution is and how it is used to obtain a confidence interval\ninterpret a confidence interval in the context of the parameter of interest\ndescribe the connection between confidence intervals and hypothesis testing",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-assessments-will-there-be",
    "href": "course-syllabus.html#what-assessments-will-there-be",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "5.1 What assessments will there be?",
    "text": "5.1 What assessments will there be?\nThe main idea of assessment in this course is communication and feedback. There are several different types of assessments or assignments in this class, but they will all allow you to check your own understanding and progress towards the learning goals, get in-depth feedback from me, and let me know where to spend more time or approach something differently.\nEach one is described briefly here, grouped into categories by course goal. See Section 5 for an explanation of how these contribute to your final grade. Our class schedule with topics is in Section 11.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#interpret-and-use-statistical-concepts",
    "href": "course-syllabus.html#interpret-and-use-statistical-concepts",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "5.2 Interpret and use statistical concepts",
    "text": "5.2 Interpret and use statistical concepts\n\nReadings and videos (every week)\nI favor a “flipped classroom,” as it gives you more time to clarify and solidify statistical concepts through hands-on exercises. Each week, you will read the required chapter(s), completing a required reading guide to walk you through the central concepts for each week. You are required to submit your completed reading guides by the start of class on Monday.\n\n\nConcept quizzes (every week)\nEach week there will be a short (~10 questions) quiz over the reading and videos from the week. These quizzes are intended to ensure that you grasped the key concepts from the week’s readings. The quizzes are not timed, so you can feel free to check your answers with the textbook and / or videos if you so wish. The quizzes are marked on completion as complete or incomplete. You are required to complete the concept quiz by the start of class on Monday.\n\n\nTutorials (every week)\nYou can think of the tutorials as an “interactive” textbook, as they interweave statistical ideas alongside examples of how to work with data in R and hands-on exercises writing the R code necessary to complete a given task. Each exercise has hints available if you get stuck!\nThe tutorials are work at your own pace, so you can complete them all at once or slowly throughout the week. The lab assignments will require for you to put the skills you learned in the tutorials to work, so you are required to complete each week’s tutorial before Wednesday’s lab.\nThe tutorials are marked on completion as complete or incomplete. You will submit a screenshot of the completion page at the end of the tutorial to confirm that you completed the tutorial for the week. You are required to complete the tutorial by the start of class on Thursday.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#critically-evaluate-the-use-of-statistics",
    "href": "course-syllabus.html#critically-evaluate-the-use-of-statistics",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "5.3 Critically evaluate the use of Statistics",
    "text": "5.3 Critically evaluate the use of Statistics\n\nStatistical critiques (every 4-weeks)\nThese assignments are case studies in which you will evaluate a data visualization or statistical analysis, determining how well-performed and presented the analysis was and making recommendations for improving or using the analysis. Critiques are due roughly 1-week after they are assigned and should take 1-2 hours. You will receive feedback and a mark of Success or Growing (elaborated more on later), and you will be able to revise based on that feedback. There will be two total critiques.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#perform-statistical-analyses-to-answer-research-questions",
    "href": "course-syllabus.html#perform-statistical-analyses-to-answer-research-questions",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "5.4 Perform statistical analyses to answer research questions",
    "text": "5.4 Perform statistical analyses to answer research questions\n\nLab Assignments (every week)\nLabs will be assigned on Thursday every week, providing the opportunity to explore the course concepts in the context of real data. Lab assignments will require for you to work through the tutorial for the week, thus the tutorials are due before the start of class on Wednesday.\nYou will complete the lab assignments in the same teams you collaborate with in class. You will access the lab assignment through Posit Cloud, which you will be walked through during the first lab. Your group will be expected to submit your completed lab on Canvas. You will need to submit only the HTML document (not the Quarto document). The individual assigned as the Report Manager (described below) will submit the team’s completed Lab assignment by Monday at 5pm.\n\nSuccess / Growing Grading\nI expect that you will approach each lab assignment seriously, investing the necessary time and energy to prepare your responses. Different from what you may have experienced, lab assignments are graded for “proficiency” of specific learning targets, which describe what you should be able to do after taking this course. You’ll receive a score for each problem on an assignment according to the Success / Not Yet rubric below, as well as feedback to help you improve.\n\n\n\n\n\n\n\nScore\nJustification\n\n\n\n\nSuccess\nThe solution to the problem is correct, legible, and easy to follow, with all reasoning provided. Any error does not bring into question your understanding of the topic.\n\n\nGrowing\nThe solution shows growth toward mastering the topic; however, elements of the solution bring into question your understanding of the topic, and thus further attention is needed.\n\n\n\n\n\nCompleting Revisions\nEvery week, your Lab assignment and / or Statistical Critique from the previous week will be graded and returned to you no later than Wednesday at 9am. Meaning, Lab 2 (completed in Week 2) will be graded and returned to you by Wednesday of Week 3. After the first submission of your Lab or Critique you will have the option to retry any problems for which you scored a Growing (G). A written reflection on how your understanding of the problem changed will accompany any revision.\nRevisions for a Lab or Critique are due on Wednesdays at 5pm, one week from when they were returned. If you do not complete your revision within one week of when it was returned, you are no longer permitted to submit a revision. If you don’t earn a Success by your second revision, you are expected to make an appointment with me to meet during my office hours (or another agreed-upon time) to create a reassessment strategy.\n\n\nReflections on Your Learning\nRevisions must include a reflection describing how you revised your thinking when completing your revision. It’s not enough to say “[x] was wrong, so I fixed it”—you have to talk about why you got [x] wrong in the first place and what you learned that changed your mind. What do you know now that you didn’t know before? Who or what helped you learn?\n\nIf your revision does not include reflections, I’ll ask you to add them.\nSee some examples of really good reflections here – they’re (mostly) from an introductory statistics class, but I think you’ll get the idea.\n\nSubmit your revision to the same assignment box on Canvas as your original. This helps me keep track of who has outstanding revisions.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#synthesize-statistical-ideas",
    "href": "course-syllabus.html#synthesize-statistical-ideas",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "5.5 Synthesize statistical ideas",
    "text": "5.5 Synthesize statistical ideas\n\nMidterm & Final Projects\nThere will be two projects throughout the quarter, where you will be asked to synthesize the statistical concepts you have learned in a formal statistical report. Your critiques will help guide you toward how you do / don’t want your report to look. Each project will be done independently, and requires you to submit a project proposal and draft report before the final deadline. You are encouraged to use the feedback received on these assignments to improve your final report. The final reports will be graded as Excellent, Satisfactory, Progressing, or No Credit based on a rubric that will be shared with the initial assignment.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-have-accommodations-or-feel-that-accommodations-would-be-beneficial-to-my-learning",
    "href": "course-syllabus.html#what-if-i-have-accommodations-or-feel-that-accommodations-would-be-beneficial-to-my-learning",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "8.1 What if I have accommodations or feel that accommodations would be beneficial to my learning?",
    "text": "8.1 What if I have accommodations or feel that accommodations would be beneficial to my learning?\nI enthusiastically support the mission of Disability Resource Center to make education accessible to all. I design all my courses with accessibility at the forefront of my thinking, but if you have any suggestions for ways I can make things more accessible, please let me know. Come talk to me if you need accommodation for your disabilities. I honor self-diagnosis: let’s talk to each other about how we can make the course as accessible as possible. See also the standard syllabus statements, which include more information about formal processes.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#im-having-difficulty-paying-for-food-and-rent-what-can-i-do",
    "href": "course-syllabus.html#im-having-difficulty-paying-for-food-and-rent-what-can-i-do",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "8.2 I’m having difficulty paying for food and rent, what can I do?",
    "text": "8.2 I’m having difficulty paying for food and rent, what can I do?\nIf you have difficulty affording groceries or accessing sufficient food to eat every day, or if you lack a safe and stable place to live, and you believe this may affect your performance in the course, I urge you to contact the Dean of Students for support. Furthermore, please notify me if you are comfortable in doing so. This will enable me to advocate for you and to connect you with other campus resources.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#my-mental-health-is-impairing-my-ability-to-engage-in-my-classes-what-should-i-do",
    "href": "course-syllabus.html#my-mental-health-is-impairing-my-ability-to-engage-in-my-classes-what-should-i-do",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "8.3 My mental health is impairing my ability to engage in my classes, what should I do?",
    "text": "8.3 My mental health is impairing my ability to engage in my classes, what should I do?\nNational surveys of college students have consistently found that stress, sleep problems, anxiety, depression, interpersonal concerns, death of a significant other and alcohol use are among the top ten health impediments to academic performance. If you are experiencing any mental health issues, I and Cal Poly are here to help you. Cal Poly’s Counseling Services (805-756-2511) is a free and confidential resource for assistance, support and advocacy.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#someone-is-threatening-me-what-can-i-do",
    "href": "course-syllabus.html#someone-is-threatening-me-what-can-i-do",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "8.4 Someone is threatening me, what can I do?",
    "text": "8.4 Someone is threatening me, what can I do?\nI will listen and believe you if someone is threatening you. I will help you get the help you need. I commit to changing campus culture that responds poorly to dating violence and stalking.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-cant-arrange-for-childcare",
    "href": "course-syllabus.html#what-if-i-cant-arrange-for-childcare",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "8.5 What if I can’t arrange for childcare?",
    "text": "8.5 What if I can’t arrange for childcare?\nIf you are responsible for childcare on short notice, you are welcome to bring children to class with you. If you are a lactating parent, you many take breaks to feed your infant or express milk as needed. If I can support yo in navigating parenting, coursework, and other obligations in any way, please let me know.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-need-to-miss-class",
    "href": "course-syllabus.html#what-if-i-need-to-miss-class",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "9.1 What if I need to miss class?",
    "text": "9.1 What if I need to miss class?\nI encourage you to attend every class session, but policies are for narcs. I put a great deal of time into making each class session engaging and worth your time. Attendance in this course is not explicitly required, but it degrades your team’s trust in you when they don’t see you in class.\nHere’s what you should do if you do miss a class:\n\nTalk to a classmate to figure out what information you missed\nCheck Canvas for any necessary handouts or changes to assignments\nEmail me with any questions you have after reviewing notes and handouts\n\nIf you miss a bunch of classes, please come talk to me. I’m working from the assumption that you care and are trying, but something is getting in your way (health issues? depression / anxiety? college stress?); let’s figure out what that is and how I can help.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-if-i-need-to-turn-something-in-late",
    "href": "course-syllabus.html#what-if-i-need-to-turn-something-in-late",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "9.2 What if I need to turn something in late?",
    "text": "9.2 What if I need to turn something in late?\nAssignments are expected to be submitted on time. However, every student will be permitted to submit up to three individual assignments up to 3-days late, by completing a deadline extension form. Similar to the “real world,” deadline extensions must be requested before an assignment is due.\nWhen you complete the deadline extension form you will be required to state (1) what assignment you need an extension for, and (2) your proposed new deadline. Your new deadline must be within 3-days of the original deadline.\nAll deadline extensions must be done through the form, so I can keep track of who has used their allotment of extensions. If you are registered with DRC to have deadline extensions, you are required to complete a deadline extension request and make a note if your extension is related to a need related to DRC accommodations.\nAny late work is required to have a deadline extension request, meaning if you do not complete a deadline extension request for an assignment you are not permitted to turn it in late.\nThe link to the deadline extension form can be found in Canvas in the Course Information module (at the top of the page).\n\n\n\n\n\n\nDeadline extensions are not permitted for the “final version” of the Midterm Project and Final Project",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#do-i-need-to-bring-a-computer-to-class",
    "href": "course-syllabus.html#do-i-need-to-bring-a-computer-to-class",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "9.3 Do I need to bring a computer to class?",
    "text": "9.3 Do I need to bring a computer to class?\nYou are allowed to use technology in the classroom! In fact, we will often do so as part of in-class activities. However, our class is held in a computer laboratory, so bringing a laptop is not required. You are permitted to use the lab computers, but if you would like to take notes on your computer / surface you are welcome to bring it to class.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#how-can-i-expect-to-be-treated-in-this-course",
    "href": "course-syllabus.html#how-can-i-expect-to-be-treated-in-this-course",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "10.1 How can I expect to be treated in this course?",
    "text": "10.1 How can I expect to be treated in this course?\nFollowing Ihab Hassan, I strive to teach statistics so that people will stop killing each other. In my classroom, diversity and individual differences are a sources of strength. One of the greatest failures of Statistics, historically and in the present, has been the exclusion of voices from the field. Everyone here can learn from each other, and doing so is vital to the structure of the course. Significant portions of this course involve group work and discussion in class. Some discussions will touch on sensitive topics. So that everyone feels comfortable participating in these activities, we must listen to each other and treat each other with respect. Any attitude or belief that espouses the superiority of one group of people over another is not welcome in my classroom. Such beliefs are directly destructive to the sense of community that we strive to create, and will sabotage our ability to learn from each other (and thus sabotage the entire structure of the course).\nIn summary: Be good to each other.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#working-in-teams",
    "href": "course-syllabus.html#working-in-teams",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "10.2 Working in teams",
    "text": "10.2 Working in teams\nWhen completing lab assignments, you will work in a team of three students. In Week 2, you will be assigned to a group and you will work in this same group for three total weeks (Weeks 2 - 4). Then, in Week 5 you will be assigned to a new group of three students, which you will collaborate with for the next three weeks (Weeks 5 - 7). Finally, in Week 8, you will be assigned to one final group which you will collaborate with for the final three weeks of the quarter (Weeks 8 - 10).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearching Group Collaborations\nFor quite some time I have been interested in what we, as your professors, can do to make group collaborations better for every student. This is a substantial area of my research, and, as such, I will be collecting data on your experiences collaborating with your peers in STAT 313. Every week, I will request for your to complete a pre-lab and a post-lab survey, detailing your feelings about your upcoming collaboration and your perceptions for how the collaboration went. Results from this study will provide our research team with a better understanding of (1) the impact of collaborative experiences on student self-perception, and (2) how to facilitate collaborative experiences that are beneficial for every student\nDuring Week 1, you will be asked to complete a form consenting to participate in this study. Your participation in this research will not directly affect your course grade. If you agree to participate, you will complete a weekly surveys regarding your experiences before and after collaborating in your group. Based on the results of your surveys from Weeks 2-8, you may be invited to participate in a 30-minute interview with an undergraduate student researcher, discussing your experiences collaborating with your peers.\nIf you have questions regarding this study or would like to be informed of the results when the study is completed, please contact me (Dr. Theobold) at atheobol@calpoly.edu. If you have concerns regarding the manner in which the study is conducted, you may contact Dr. Michael Black, Chair of the Cal Poly Institutional Review Board, at (805) 756-2894, mblack@calpoly.edu, or Trish Brock, Director of Research Compliance, at (805) 756-1450, pbrock@calpoly.edu.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#what-constitutes-plagiarism-in-a-statistics-class",
    "href": "course-syllabus.html#what-constitutes-plagiarism-in-a-statistics-class",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "10.3 What constitutes plagiarism in a statistics class?",
    "text": "10.3 What constitutes plagiarism in a statistics class?\nParaphrasing or quoting another’s work without citing the source is a form of academic misconduct. This included the R code produced by someone else! Writing code is like writing a paper, it is obvious if you copied-and-pasted a sentence from someone else into your paper because the way each person writes is different.\nEven inadvertent or unintentional misuse or appropriation of another’s work (such as relying heavily on source material that is not expressly acknowledged) is considered plagiarism. If you are struggling with writing the R code for an assignment, please reach out to me. I would prefer that I get to help you rather than you spending hours Googling things and get nowhere!\nAny incident of dishonesty, copying or plagiarism will be reported to the Office of Student Rights and Responsibilities. Cheating or plagiarism will earn you an “Incomplete” grade on the assignment and you will not be able to submit revisions for that assignment.\nIf you have any questions about using and citing sources, you are expected to ask for clarification.\nFor more information about what constitutes cheating and plagiarism, please see https://academicprograms.calpoly.edu/content/academicpolicies/Cheating.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-1-foundations-of-statistics-week-1",
    "href": "course-syllabus.html#unit-1-foundations-of-statistics-week-1",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "11.1 Unit 1: Foundations of Statistics (Week 1)",
    "text": "11.1 Unit 1: Foundations of Statistics (Week 1)\nThis introductory unit has three big tasks, (1) review statistical and data oriented concepts you have (likely) seen before, (2) think critically about why statistics is used in science, and (3) think about how (historically) statistics has been used for inference.\nReading: Chapters 1 and 2 in Introduction to Modern Statistics (IMS)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-2-exploratory-data-analysis-weeks-2-3",
    "href": "course-syllabus.html#unit-2-exploratory-data-analysis-weeks-2-3",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "11.2 Unit 2: Exploratory Data Analysis (Weeks 2 & 3)",
    "text": "11.2 Unit 2: Exploratory Data Analysis (Weeks 2 & 3)\nThis unit focuses on building skills for working with and visualizing different types of data. First, we will focus on numerical data–calculating summary statistics, histograms, scatterplots, and linegraphs. Next, we incorporate categorical variables into these summarizes (with groups) and visualizations (with colors and facets)!\nReading: Chapters 4 & 5 in IMS and Chapter 2 in Modern Dive",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-3-regression-modeling-weeks-4-5",
    "href": "course-syllabus.html#unit-3-regression-modeling-weeks-4-5",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "11.3 Unit 3: Regression Modeling (Weeks 4 & 5)",
    "text": "11.3 Unit 3: Regression Modeling (Weeks 4 & 5)\nIn this unit we begin exploring formal statistical methods. You will put the tools you learned for wrangling and visualizing to work in the context of linear regression. We will start in a familiar context—linear regression. Once we’ve explored the concepts of “simple” / basic regression we will turn up the heat and add some additional explanatory variables using multiple linear regression.\nReading: Chapters 5 & 6 in Modern Dive",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-4-model-selection-inference-for-regression-weeks-6-7",
    "href": "course-syllabus.html#unit-4-model-selection-inference-for-regression-weeks-6-7",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "11.4 Unit 4: Model Selection & Inference for Regression (Weeks 6 & 7)",
    "text": "11.4 Unit 4: Model Selection & Inference for Regression (Weeks 6 & 7)\nThis unit focuses on how we decide what variables should be included in our regression models and what we can say about the final models we obtain. We will explore these ideas using concepts you have seen before: hypothesis tests and confidence intervals. We will visit the ideas of p-values and significance testing, with a emphasis on making (and justifying) sound scientific decisions with the intention of obtaining the best regression model we can.\nReading: Supplementary resources written / compiled by Dr. Theobold",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-5-condition-violations-week-8",
    "href": "course-syllabus.html#unit-5-condition-violations-week-8",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "11.5 Unit 5: Condition Violations (Week 8)",
    "text": "11.5 Unit 5: Condition Violations (Week 8)\nThere are occasions where the conditions required for linear regression are violated. Rather than throwing up our hands and saying “Oh, well!”, we can use variable transformations to lessen condition violations. This unit will explore the use of log transformations to remedy non-linear relationships and non-constant variance.\nReading: Supplementary resources written / compiled by Dr. Theobold",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-6-anova-weeks-9-10",
    "href": "course-syllabus.html#unit-6-anova-weeks-9-10",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "11.6 Unit 6: ANOVA (Weeks 9 & 10)",
    "text": "11.6 Unit 6: ANOVA (Weeks 9 & 10)\nTo wrap up the quarter, we will look at a special case of linear regression–ANOVA. In this special case, our regression will include only categorical variables as explanatory variables. We will first review how we compare the means of two groups and then connect with what we learned about categorical variables in multiple linear to conceptualize how we can compare the means of three or more groups.\nThis unit will explore Chapter 22 in IMS, with supplementary materials created by Dr. Theobold.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#footnotes",
    "href": "course-syllabus.html#footnotes",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Section 6.0.2 for information on what you can expect when you email me.↩︎\nA “Satisfactory” lab or critique occurs when every problem has been marked Satisfactory.↩︎",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "critique/stat-critique-2-template.html",
    "href": "critique/stat-critique-2-template.html",
    "title": "Statistical Critique 2: Exploring p-values",
    "section": "",
    "text": "# This is where you will load in your data! \n# Reference the R package you data live inside, since you will need to load that package in!\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(moderndive)\n\n# Package for your data:"
  },
  {
    "objectID": "critique/stat-critique-2-template.html#visualizations",
    "href": "critique/stat-critique-2-template.html#visualizations",
    "title": "Statistical Critique 2: Exploring p-values",
    "section": "Visualizations",
    "text": "Visualizations\nIn the code chunk below, copy-and-paste the plots you created in your Midterm Project."
  },
  {
    "objectID": "critique/stat-critique-2-template.html#model-justification",
    "href": "critique/stat-critique-2-template.html#model-justification",
    "title": "Statistical Critique 2: Exploring p-values",
    "section": "Model Justification",
    "text": "Model Justification\nJustify (in 2-3 sentences) why you chose this model!"
  },
  {
    "objectID": "critique/stat-critique-2-template.html#part-four-lessons-learned",
    "href": "critique/stat-critique-2-template.html#part-four-lessons-learned",
    "title": "Statistical Critique 2: Exploring p-values",
    "section": "Part Four: Lessons Learned",
    "text": "Part Four: Lessons Learned\nNow that you have explored the use of p-values for model selection and publication criteria, write down two things you have learned that you will take with you."
  },
  {
    "objectID": "critique/critique-1.html",
    "href": "critique/critique-1.html",
    "title": "Statistical Critique 1",
    "section": "",
    "text": "In your first statistical critique will focus on critiquing a key aspect of any statistical argument—data visualization. You have explored data visualizations over the last two weeks, thinking about what makes a plot more or less clear. You will use your knowledge of data visualizations to provide a critique for two different styles of visualizations, (1) a “pop” visualization, and (2) a “scientific” visualization.\n\n\nYou are expected to include the two visualizations you decide to critique. These visualizations must be included in the paragraph surrounding your critique. Please do not include the images at the end of your document.\n\n\nFor the New York Times visualization, you can left click on the image and save it to your computer.\n\n\n\nFigure 1: Options when left clicking on NYT visualization – select “Save Image As…” to save it to your computer\n\n\nFor the visualization from your research article, you may need to take screenshot of the page and crop it to only include the image.\n\n\n\nFigure 2: Cropped image from scientific article\n\n\n\n\n\n\n\n\nNo Visualization in Article\n\n\n\nIf the article you selected does not have a visualization, you are permitted to substitute a table for a visualization. Similar to the instructions above, you are required to take a screenshot of the page and crop it to only include the table.\n\n\n\n\n\n\nYou are allowed to use any text editing software to make your critique (e.g., Word, Pages, Google Docs), but your submission must be a PDF. If you are unsure how to save your file as a PDF, I recommend using Google!"
  },
  {
    "objectID": "critique/critique-1.html#including-your-visualizations",
    "href": "critique/critique-1.html#including-your-visualizations",
    "title": "Statistical Critique 1",
    "section": "",
    "text": "You are expected to include the two visualizations you decide to critique. These visualizations must be included in the paragraph surrounding your critique. Please do not include the images at the end of your document.\n\n\nFor the New York Times visualization, you can left click on the image and save it to your computer.\n\n\n\nFigure 1: Options when left clicking on NYT visualization – select “Save Image As…” to save it to your computer\n\n\nFor the visualization from your research article, you may need to take screenshot of the page and crop it to only include the image.\n\n\n\nFigure 2: Cropped image from scientific article\n\n\n\n\n\n\n\n\nNo Visualization in Article\n\n\n\nIf the article you selected does not have a visualization, you are permitted to substitute a table for a visualization. Similar to the instructions above, you are required to take a screenshot of the page and crop it to only include the table."
  },
  {
    "objectID": "critique/critique-1.html#submission",
    "href": "critique/critique-1.html#submission",
    "title": "Statistical Critique 1",
    "section": "",
    "text": "You are allowed to use any text editing software to make your critique (e.g., Word, Pages, Google Docs), but your submission must be a PDF. If you are unsure how to save your file as a PDF, I recommend using Google!"
  },
  {
    "objectID": "critique/critique-1.html#getting-started",
    "href": "critique/critique-1.html#getting-started",
    "title": "Statistical Critique 1",
    "section": "2.1 Getting Started",
    "text": "2.1 Getting Started\n\nGo to the What’s Going on in This Graph website\nScroll through the weekly graphs and click on a plot that you are interested in\nSave the image to your computer (as instructed above)\nInsert the image in your document (before your written critique)"
  },
  {
    "objectID": "critique/critique-1.html#visualization-critique",
    "href": "critique/critique-1.html#visualization-critique",
    "title": "Statistical Critique 1",
    "section": "2.2 Visualization Critique",
    "text": "2.2 Visualization Critique\nYour critique of the visualization you selected needs to address the following questions:\n\nWhat aesthetics are being used in the plot?\n\n\n\n\n\n\n\nAesthetics are Mapped to Variables\n\n\n\nRemember, aesthetics map variables to aspects of the plot. If your plot uses color, but the color is not associated with a variable, then color is not an aesthetic in the plot.\n\n\n\nName at least two things the visualization does well — What makes the visualization clear to the reader?\nName at least two ways the visualization could be improved — What would the reader struggle to understand?"
  },
  {
    "objectID": "critique/critique-1.html#getting-started-1",
    "href": "critique/critique-1.html#getting-started-1",
    "title": "Statistical Critique 1",
    "section": "3.1 Getting Started",
    "text": "3.1 Getting Started\n\nAccess the research article you selected in Week 1. If you cannot find it on your computer, go to the Week 1: Statitics in Your Field assignment\nLocate the visualization (or table)\nTake a screenshot of the visualization / table\nCrop the image to only include the visualization / table you are critiquing\nSave the image to your computer (as instructed above)\nInsert the image in your document (before your written critique)"
  },
  {
    "objectID": "critique/critique-1.html#visualization-table-critique",
    "href": "critique/critique-1.html#visualization-table-critique",
    "title": "Statistical Critique 1",
    "section": "3.2 Visualization / Table Critique",
    "text": "3.2 Visualization / Table Critique\nYour critique of the visualization you selected needs to address the following questions:\n\nWhat aesthetics are being used in the visualization / table?\nName at least two things the visualization / table does well — What makes the visualization clear to the reader?\nName at least two ways the visualization / table could be improved — What would the reader struggle to understand?\n\n\n\n\n\n\n\nIf you are referencing a table\n\n\n\nSimilar to a visualization, the aesthetics of a table are variables being mapped to aspects of the table. Below is a table from Coyne et al. (2020). I like to think of the rows and columns of a table as similar to the x- and y-axis of a visualization.\n\n\nI start by noticing that the “study variables” are mapped to the rows (e.g., Social Network, Depressive Sym., Anxiety).\nThen, I notice that the columns are associated with different values of Age.\nFinally, I notice tha there are actually two rows per study variable, one associated with the mean and one associated with the standard deviation.\n\nIf I were to sketch out how this table would translate into a visualization, I would imagine the x-axis would be Age, the y-axis would be the value of the variable, there would be three facets (one per study variable), and there would be two types of points (one for the mean, one for the standard deviation). Here is a rough sketch of my mental image:\n\nIn this plot, there are four aesthetics:\n\nthe age (included on the x-axis)\nthe study variable (included as facets)\nthe statistic measured (included as a color)\nthe value of the statistic (included on the y-axis)"
  },
  {
    "objectID": "project/midterm-project-directions.html",
    "href": "project/midterm-project-directions.html",
    "title": "Midterm Project Guidelines",
    "section": "",
    "text": "Your Task\nFor this project, you are expected to use a multiple linear regression to investigate the relationship between the variables you outlined in your Midterm Project Proposal. Your project is required to have at least two explanatory variables, one of which must be numerical. You are expected to justify why you chose each explanatory variable in the Introduction section of your report.\n\n\nIntroduction\n\nIn 4-6 sentences describe how the data were collected, the context of the data (e.g., Are they from a study or a publication?), and the background of the research problem (e.g., What question do these data address? Why were the data collected?)\n\n\n\n\n\n\n\nTip\n\n\n\nYou may need to look up the publication(s) the data are associated with to obtain information on how they were collected.\n\n\n\nState the question(s) of interest you will address with your statistical analysis. The more specific you define the question of interest here, the easier the rest of the analysis and report will be. The research questions should start with, “What is the relationship between…” and should be as specific as possible. Your Findings section should directly address the question(s) you pose here.\n\n\n\nMethods\nThis section should lay out the steps, decisions, and logic leading to the statistical model you will use to answer the research question of interest.\n\nDescribe the response and explanatory variables, how they were measured and their associated units. For categorical variables, describe the levels of the categorical variable.\nProduce data visualizations exploring the relationship(s) you are interested in investigating, contrasting the need for a second explanatory variable.\n\nFor a multiple linear regression with one numerical variable and one categorical variable, you should produce two visualizations: one with different slopes and one with parallel slopes.\nFor a multiple linear regression with two numerical variables, you should produce three visualizations: one with both explanatory variables included, and two with each explanatory variable on its own.\n\n\n\n\n\n\n\n\nEvery visualization should have nicely formatted axis labels!\n\n\n\n\n\n\n\nDescribe what you see in the visualizations, making direct references to the plots!\nOutline the appropriate statistical model you will use to answer the question(s) of interest that you stated previously. Be specific about why the method being used are appropriate for the investigation at hand making direct reference to the visualizations.\n\n\n\n\n\n\n\nDeciding on a Statistical Model\n\n\n\nThe statistical model you fit in the next section depends on what you see in your visualization. Click here for a flowchart of how you should select the statistical model that is best for your situation.\n\n\n\n\nFindings\nIn this section you will write up your findings for each research question of interest.\n\nFit the statistical model stated at the end of the Methods section\nObtain the coefficients for the model\n\n\n\n\n\n\n\nTable of Coefficents\n\n\n\nUse the get_regression_table() function from the moderndive package to provide nicely formatted output from your regression model.\n\n\n\nWrite out the estimated regression equation for your statistical model.\n\nIf your regression contains categorical variables, you can either write your equation out with indicator variables or write out a different equation for each group.\n\nInterpret in the context of the data the coefficients from the regression equation.\n\n\n\nStudy Limitations\nWrite a 3-4 sentence statement on what can be inferred from the design of the study and the results of your statistical analysis. Specifically, answer these two questions and comment on their implications:\n\nBased on the sampling method used, what larger population can you infer the results or your analysis onto?\nBased on the design of the study, what type of statements can be made about the relationship between the explanatory and response variables. Specifically, can cause-and-effect statements be made?\n\n\n\nConclusions\nBased on your visualizations and the regression model, what is your conclusion for the questions of interest?\n\n\n\n\n\n\nCaution\n\n\n\nThere should be no mention of p-values in your conclusion!",
    "crumbs": [
      "Projects",
      "Midterm Project Description"
    ]
  },
  {
    "objectID": "project/final_project_template-513.html#variables",
    "href": "project/final_project_template-513.html#variables",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Variables",
    "text": "Variables"
  },
  {
    "objectID": "project/final_project_template-513.html#data-visualizations",
    "href": "project/final_project_template-513.html#data-visualizations",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Data Visualizations",
    "text": "Data Visualizations"
  },
  {
    "objectID": "project/final_project_template-513.html#description-of-relationships",
    "href": "project/final_project_template-513.html#description-of-relationships",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Description of Relationships",
    "text": "Description of Relationships"
  },
  {
    "objectID": "project/final_project_template-513.html#one-way-anova-of-and",
    "href": "project/final_project_template-513.html#one-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "One-Way ANOVA of  and ",
    "text": "One-Way ANOVA of  and"
  },
  {
    "objectID": "project/final_project_template-513.html#one-way-anova-of-and-1",
    "href": "project/final_project_template-513.html#one-way-anova-of-and-1",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "One-Way ANOVA of  and ",
    "text": "One-Way ANOVA of  and"
  },
  {
    "objectID": "project/final_project_template-513.html#additive-two-way-anova-of-and",
    "href": "project/final_project_template-513.html#additive-two-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Additive Two-Way ANOVA of , , AND ",
    "text": "Additive Two-Way ANOVA of , , AND"
  },
  {
    "objectID": "project/final_project_template-513.html#model-validity",
    "href": "project/final_project_template-513.html#model-validity",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Model Validity",
    "text": "Model Validity"
  },
  {
    "objectID": "project/data-formatting-513.html",
    "href": "project/data-formatting-513.html",
    "title": "Midterm Project Data Formatting",
    "section": "",
    "text": "Although R is a component of this course, it is not one of the learning outcomes. Thus, I have decided we will not be spending time writing the R code to wrangle your data into the format R expects them to be in. Instead, you are responsible for formatting your data so they are ready to be analyzed in class on Thursday."
  },
  {
    "objectID": "project/data-formatting-513.html#required-format",
    "href": "project/data-formatting-513.html#required-format",
    "title": "Midterm Project Data Formatting",
    "section": "Required Format",
    "text": "Required Format\nR expects every dataset to be in “tidy” format, meaning\n\nevery column is a variable\nevery row is an observation\nevery cell has one value\n\nEach variable you selected to analyze for your midterm project will need to be included as one column in your dataset.\nFor example, suppose the ntl_icecover data we worked with for Lab 4 had one row for each lake and column for every (displayed below):\n\n\n\n\n\n\n\n\nlakeid\n1852\n1853\n1855\n1856\n1857\n1858\n1859\n1860\n1861\n1862\n1863\n\n\n\n\nLake Mendota\nNA\nNA\n118\n151\n121\n96\n110\n117\n132\n104\n125\n\n\nLake Monona\nNA\nNA\n118\n151\n119\n94\n111\n129\n133\n119\n131\n\n\n\n\n\n\n\nThese data could not be analyzed by R, as the variable year occupies more than one column. To format these data, we would need to create two news columns, one to store the years and one to store the ice durations. The reformatted data would look like:\n\n\n\n\n\n\n\n\nlakeid\nyear\nice_duration\n\n\n\n\nLake Mendota\n1852\nNA\n\n\nLake Mendota\n1853\nNA\n\n\nLake Mendota\n1855\n118\n\n\nLake Mendota\n1856\n151\n\n\nLake Mendota\n1857\n121\n\n\nLake Mendota\n1858\n96\n\n\nLake Mendota\n1859\n110\n\n\nLake Mendota\n1860\n117\n\n\nLake Mendota\n1861\n132\n\n\nLake Mendota\n1862\n104\n\n\nLake Mendota\n1863\n125\n\n\nLake Monona\n1852\nNA\n\n\nLake Monona\n1853\nNA\n\n\nLake Monona\n1855\n118\n\n\nLake Monona\n1856\n151\n\n\nLake Monona\n1857\n119\n\n\nLake Monona\n1858\n94\n\n\nLake Monona\n1859\n111\n\n\nLake Monona\n1860\n129\n\n\nLake Monona\n1861\n133\n\n\nLake Monona\n1862\n119\n\n\nLake Monona\n1863\n131"
  },
  {
    "objectID": "project/data-formatting-513.html#how-to-format-your-data",
    "href": "project/data-formatting-513.html#how-to-format-your-data",
    "title": "Midterm Project Data Formatting",
    "section": "How to Format Your Data",
    "text": "How to Format Your Data\nTo get your data in the correct format, you have two options:\n\ncopy-and-paste each variable into a column in a new dataset\nwrite the R (or Python) code to reformat your data\n\nYou should not spend time Googling or using Chat GPT to write the R code to format your data. I will not provide assistance writing R code to format your data. Yes, copying and pasting is a pain, but this is a lesson in good data storage practices."
  },
  {
    "objectID": "project/data-formatting-513.html#file-format",
    "href": "project/data-formatting-513.html#file-format",
    "title": "Midterm Project Data Formatting",
    "section": "File Format",
    "text": "File Format\nYou are expected to save your new dataset as either an Excel file (.xlsx) or as a comma separated values file (.csv). No other format will be accepted."
  },
  {
    "objectID": "project/data-formatting-513.html#example-video",
    "href": "project/data-formatting-513.html#example-video",
    "title": "Midterm Project Data Formatting",
    "section": "Example Video",
    "text": "Example Video\nIn case you are unsure what I mean by “copy-and-paste” your variables into a dataset, I have recorded this short video walking you through how I would reformat a dataset to be ready for analysis.\nLink to Zoom recording\nPasscode: dXEZ@Yp5"
  },
  {
    "objectID": "project/midterm_project_template-313.html#methods",
    "href": "project/midterm_project_template-313.html#methods",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Methods",
    "text": "Methods\n\nVariables\n\n\nData Visualizations\n\n\n\n\n\n\n\n\nProposed Statistical Model"
  },
  {
    "objectID": "project/midterm_project_template-313.html#findings",
    "href": "project/midterm_project_template-313.html#findings",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Findings",
    "text": "Findings\n\nMultiple Linear Regression Model\n\n\nModel Interpretations"
  },
  {
    "objectID": "project/midterm_project_template-313.html#study-limitations",
    "href": "project/midterm_project_template-313.html#study-limitations",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Study Limitations",
    "text": "Study Limitations"
  },
  {
    "objectID": "project/midterm_project_template-313.html#conclusions",
    "href": "project/midterm_project_template-313.html#conclusions",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "project/final-project-help/no-gos.html",
    "href": "project/final-project-help/no-gos.html",
    "title": "Things that cannot go in your project!",
    "section": "",
    "text": "Statements of hypotheses (e.g., \\(H_0\\) or \\(H_A\\))\nStatements that say your condition has been “met” 😡\n\nIf you have no evidence that the condition is violated, simply state so.\n\nThe s-word (significance) 😱\n\nProvide a more descriptive summary of your findings!\n\nLong paragraphs!\n\nBreak into smaller 3-4 sentence paragraphs surrounding the same topic.\n\nThe same sentence written in multiple sections. 🙃"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html",
    "href": "project/final-project-help/twa-model-selection-process.html",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "",
    "text": "We will be using a “backward” selection process for deciding on the “best” ANOVA model. Meaning, we will start with a more complicated model and decide if it is “worth it.”"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#plot-the-model",
    "href": "project/final-project-help/twa-model-selection-process.html#plot-the-model",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Plot the model",
    "text": "Plot the model\n\n\n\n\n\n\n\n\n\nWhat are we looking for?\nWe are looking to see if relationship between the mean of \\(y\\) and explanatory variable 1 changes over the values of explanatory variable 2. This is akin to looking for evidence of different slopes in a multiple linear regression.\nFor these data we are looking to see if the relationship between the mean movie rating and the genre of the movie changes over the different eras."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#fit-the-model",
    "href": "project/final-project-help/twa-model-selection-process.html#fit-the-model",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Fit the model",
    "text": "Fit the model\nNotice I’m using a * for an interaction model!\n\naov(rating ~ era * genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 4 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.659   0.581\n2 genre         1   4.19   4.19     1.81    0.185\n3 era:genre     3   3.32   1.11     0.477   0.700\n4 Residuals    48 111.     2.32    NA      NA"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test",
    "href": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nThe era:genre line of the ANOVA table is testing if the mean movie rating and the genre of the movie changes over the different eras. It has the following hypotheses:\n\\(H_0\\): The relationship between the mean movie rating and the genre of the movie does not change over the eras\n\\(H_A\\): The relationship between the mean movie rating and the genre of the movie is different for at least one era\nWith a p-value of 0.7 (from an F-statistic of 0.477 with 3 and 48 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating and the genre of the movie is different for at least one era.\nSo, my next step is to fit an additive two-way ANOVA."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#fit-the-model-1",
    "href": "project/final-project-help/twa-model-selection-process.html#fit-the-model-1",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Fit the Model",
    "text": "Fit the Model\nNotice I’m using a + for an interaction model!\n\naov(rating ~ era + genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 3 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.680   0.568\n2 genre         1   4.19   4.19     1.86    0.178\n3 Residuals    51 115.     2.25    NA      NA"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-1",
    "href": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-1",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nFor an additive two-way ANOVA, we have two hypothesis tests, one test per explanatory variable.\n\nHypothesis Test for genre\nThe genre line of the ANOVA table is testing if the mean movie rating is the same for all genres. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every genre, given the era of the movie\n\\(H_A\\): The mean movie rating is different for at least one genre, given the era of the movie\nNotice the hypotheses are similar to a multiple linear regression with two explanatory variables, where the test for each variable is conditional on the other variable(s) included in the model.\nWith a p-value of 0.178 (from an F-statistic of 1.86 with 1 and 51 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that at least one genre has a different mean movie rating, given the era of the movie.\n\n\nHypothesis Test for era\nThe era line of the ANOVA table is testing if the mean movie rating is the same for all eras. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every era, given the genre of the movie\n\\(H_A\\): The mean movie rating is different for at least one era, given the genre of the movie\nWith a p-value of 0.568 (from an F-statistic of 0.68 with 3 and 51 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that at least one era has a different mean movie rating, given the genre of the movie."
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#fit-the-model-2",
    "href": "project/final-project-help/twa-model-selection-process.html#fit-the-model-2",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Fit the Model",
    "text": "Fit the Model\n\naov(rating ~ genre, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 genre         1   4.68   4.68      2.13   0.150\n2 Residuals    54 119.     2.20     NA     NA"
  },
  {
    "objectID": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-2",
    "href": "project/final-project-help/twa-model-selection-process.html#perform-a-hypothesis-test-2",
    "title": "Two-Way ANOVA Model Selection Process",
    "section": "Perform a Hypothesis Test",
    "text": "Perform a Hypothesis Test\nThe genre line of the ANOVA table is testing if the mean movie rating is the same for all genres. It has the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every genre\n\\(H_A\\): The mean movie rating is different for at least one genre\nNotice the hypotheses don’t include era this time, since that variable has been removed from the model!\nWith a p-value of 0.15 (from an F-statistic of 2.13 with 1 and 54 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that at least one genre has a different mean movie rating."
  },
  {
    "objectID": "project/midterm-proposal-313.html",
    "href": "project/midterm-proposal-313.html",
    "title": "STAT 313: Midterm Project Proposal",
    "section": "",
    "text": "This week you will get started on your midterm project by selecting what dataset you wish to analyze and writing an introduction about the dataset you chose."
  },
  {
    "objectID": "project/midterm-proposal-313.html#option-1-use-your-own-data",
    "href": "project/midterm-proposal-313.html#option-1-use-your-own-data",
    "title": "STAT 313: Midterm Project Proposal",
    "section": "1.1 Option 1 – Use your own data",
    "text": "1.1 Option 1 – Use your own data\nIf you have a dataset from another course or your own research which you believe can be appropriately modeled with a linear regression, you can propose to use these data.\n\nDeliverable\nFor the Midterm Project Proposal assignment on Canvas, you are required to state at the beginning of the document, that you have chosen to use your own dataset. You are also required to submit your dataset as an Excel file (stored as either a .csv or .xlsx)."
  },
  {
    "objectID": "project/midterm-proposal-313.html#option-2-use-a-public-dataset",
    "href": "project/midterm-proposal-313.html#option-2-use-a-public-dataset",
    "title": "STAT 313: Midterm Project Proposal",
    "section": "1.2 Option 2 – Use a public dataset",
    "text": "1.2 Option 2 – Use a public dataset\nI’ve compiled a list of datasets from a variety of contexts, all which are relatively tidy and ready for analysis. Each of these datasets are included in an R package, so there is no need for you to download the dataset! All you will need to do is load in the necessary package (e.g., library(lterdatasampler)) at the beginning of your analysis.\nFrom the lterdatasampler package:\n\nand_vertebrates: Size data for Cutthroat trout and salamanders in different sections of forest (from Lab 3).\nntl_icecover: Data on duration of ice cover of lakes in the Madison, WI area (from Lab 4).\nhbr_maples: Data on the growth of Sugar Maple (Acer saccharum) seedlings in response to calcium addition.\npie_crab: Data on Fiddler crab body size in salt marshes from Florida to Massachusetts.\n\nFrom the openintro package:\n\nbirths14: Data from US births from 2014 (similar to ncbirths dataset from Week 4).\npossum: Data representing possums in Australia and New Guinea – link to scientific article referenced in possum information\n\nFrom the moderndive package:\n\nevals: Data from end of semester student evaluations from University of Texas at Austin (discussed in ModernDive textbook) – link to article analyzing the evals dataset\n\nFrom the gapminder package:\n\ngapminder: Data on life expectancy, GDP per capita, and population by country (discussed in ModernDive textbook).\n\n\nDeliverable\nFor the Midterm Project Proposal assignment on Canvas, you are required to state at the beginning of the document, the name of the dataset you have chosen to use."
  },
  {
    "objectID": "project/midterm-proposal-313.html#if-you-chose-to-use-your-own-data",
    "href": "project/midterm-proposal-313.html#if-you-chose-to-use-your-own-data",
    "title": "STAT 313: Midterm Project Proposal",
    "section": "2.1 If you chose to use your own data",
    "text": "2.1 If you chose to use your own data\nStep 1: Describe the context of your dataset in your own words! How were the data collected? Was there a study these data came from?\nI don’t know anything about your data, so I expect for this description to be extensive. If the data you are using are from a class, you are responsible for obtaining the context of the data–you cannot simply say “These were data used in BIO 253.”\nStep 2: Choose your variables\nWe will using a simple linear regression to analyze the data you chose. Thus, there are some stipulations for the variables you can choose. You must choose\n\none numeric variable for the response variable\none numeric variable for the explanatory variable\n\nOnce you have each of these variables decided, you then need to choose one additional explanatory variable. This additional explanatory variable can be either numerical or categorical.\n\n\n\n\n\n\nWrite-up\n\n\n\nDescribe each variable you chose for your analysis—how was the variable measured? What unit was the variable measured in? What types of values does the variable take on (this is especially important if you chose a categorical variable)?"
  },
  {
    "objectID": "project/midterm-proposal-313.html#if-you-chose-a-dataset-from-the-list-above",
    "href": "project/midterm-proposal-313.html#if-you-chose-a-dataset-from-the-list-above",
    "title": "STAT 313: Midterm Project Proposal",
    "section": "2.2 If you chose a dataset from the list above",
    "text": "2.2 If you chose a dataset from the list above\nStep 1: Describe the context of your dataset in your own words! How were the data collected? Was there a study these data came from? Were these data included in any publications?\n\n\n\n\n\n\nGetting information about your dataset\n\n\n\nTo obtain information on the dataset, click on the link provided in its name!\n\n\nStep 2: Choose your variables\nWe will using a simple linear regression to analyze the data you chose. Thus, there are some stipulations for the variables you can choose. You must choose\n\none numeric variable for the response variable\none numeric variable for the explanatory variable\n\nOnce you have each of these variables decided, you then need to choose one additional explanatory variable. This additional explanatory variable can be either numerical or categorical.\n\n\n\n\n\n\nWrite-up\n\n\n\nDescribe each variable you chose for your analysis—how was the variable measured? What unit was the variable measured in? What types of values does the variable take on (this is especially important if you chose a categorical variable)?"
  },
  {
    "objectID": "project/midterm_project_template-513.html#methods",
    "href": "project/midterm_project_template-513.html#methods",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Methods",
    "text": "Methods\n\nVariables\n\n\nData Visualizations\n\n\n\n\n\n\n\n\nProposed Statistical Model"
  },
  {
    "objectID": "project/midterm_project_template-513.html#findings",
    "href": "project/midterm_project_template-513.html#findings",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Findings",
    "text": "Findings\n\nMultiple Linear Regression Model\n\n\nModel Interpretations"
  },
  {
    "objectID": "project/midterm_project_template-513.html#study-limitations",
    "href": "project/midterm_project_template-513.html#study-limitations",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Study Limitations",
    "text": "Study Limitations"
  },
  {
    "objectID": "project/midterm_project_template-513.html#conclusions",
    "href": "project/midterm_project_template-513.html#conclusions",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "slides/week7-day1.html#each-table-has-a-different-sample-of-25-uc-csu-coach-salaries.",
    "href": "slides/week7-day1.html#each-table-has-a-different-sample-of-25-uc-csu-coach-salaries.",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "Each table has a different sample of 25 UC & CSU coach salaries.",
    "text": "Each table has a different sample of 25 UC & CSU coach salaries.\n\n\n\nFirst…\n\n\neach person samples 10 salaries\ncalculate the median\nwrite your median on a post-it note\nplace your median on the class plot\n\n\n\n\n\nThen…\n\neveryone works together to calculate the median of all 25 salaries"
  },
  {
    "objectID": "slides/week7-day1.html#section",
    "href": "slides/week7-day1.html#section",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Would you feel comfortable inferring that the median salary of your sample is close to the median salary of all UC & CSU coaches?\n\n\n\n\nWhy or why not?"
  },
  {
    "objectID": "slides/week7-day1.html#precision-accuracy",
    "href": "slides/week7-day1.html#precision-accuracy",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "Precision & Accuracy",
    "text": "Precision & Accuracy\n\n\n\n\n\n\n\nRandom sampling ensures our point estimates are accurate.\n\n\n\nLarger sample sizes ensure our point estimates are precise."
  },
  {
    "objectID": "slides/week7-day1.html#statistical-inference",
    "href": "slides/week7-day1.html#statistical-inference",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nThere were 252 “Head Coaches” at University of California and California State Universities in 2021 (that satisfied my search criteria)\n\n\n\nMedian salary for all 252 coaches\n$137,619\n\n\n\n\nWas the median of your sample of 25 coaches a good estimate of the salary for all 252 coaches?"
  },
  {
    "objectID": "slides/week7-day1.html#sampling-framework",
    "href": "slides/week7-day1.html#sampling-framework",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "Sampling Framework",
    "text": "Sampling Framework\npopulation – collection of observations / individuals we are interested in\npopulation parameter – numerical summary about the population that is unknown but you wish you knew\n\n\nsample – a collection of observations from the population\nsample statistic – a summary statistic computed from a sample that estimates the unknown population parameter."
  },
  {
    "objectID": "slides/week7-day1.html#section-1",
    "href": "slides/week7-day1.html#section-1",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Statistical Inference Reasoning\n\n\n\n\nIf the sampling is done at random\nthen the sample should be representative of the population\nany result based on the sample can generalize to the population\nthe point estimate is a “good guess” of the unknown population parameter\n\n\n\n\n\nEvery group had a random sample of 25 coach salaries, so why were some of the medians really far off?"
  },
  {
    "objectID": "slides/week7-day1.html#why-sample-more-than-once",
    "href": "slides/week7-day1.html#why-sample-more-than-once",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "Why sample more than once?",
    "text": "Why sample more than once?\n\n\n\n\nWhat do we get when we take multiple samples?\n\n\n\n\nA distribution of statistics!\n\n\n\n\n\n\n\nWhy do we want a distribution of statistics?\n\n\n\n\n\nUnderstanding the variability of a statistic is the heart of statistical inference!"
  },
  {
    "objectID": "slides/week7-day1.html#virtual-sampling",
    "href": "slides/week7-day1.html#virtual-sampling",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "Virtual Sampling",
    "text": "Virtual Sampling\n\nrep_sample_n(coaches, \n             size = 25, \n             reps = 1, \n             replace = FALSE)\n\n\n\n\n\n\n\n\nEmployee Name\nJob Title\nTotal Pay & Benefits\n\n\n\n\nIsabelle Harvey\nHEAD COACH - 12 MONTH\n51685.07\n\n\nNathan Miles Garcia\nHead Coach 5\n83672.00\n\n\nWilliam Clark\nHead Coach 5\n468.00\n\n\nJoe Pasternack\nHead Coach 5\n449881.00\n\n\nAlonzo J Carter\nHEAD COACH - 12 MONTH\n197763.89\n\n\nNeil Mcguire\nHead Coach 5\n189662.00\n\n\n\n\n\n\n\n\n\n\\(\\vdots\\)"
  },
  {
    "objectID": "slides/week7-day1.html#section-2",
    "href": "slides/week7-day1.html#section-2",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Distribution of 1000 medians from samples of 25 coaches"
  },
  {
    "objectID": "slides/week7-day1.html#section-3",
    "href": "slides/week7-day1.html#section-3",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Sampling Distributions\n\n\nVisualize the effect of sampling variation on the distribution of any point estimate\n\nIn this case, the sample median!\n\nWe can use sampling distributions to make statements about what values we can typically expect.\n\n\n\n\n\n\n\n\nSampling distribution vs. Sample distribution\n\n\nBe careful! A sampling distribution is different from a sample’s distribution!"
  },
  {
    "objectID": "slides/week7-day1.html#section-4",
    "href": "slides/week7-day1.html#section-4",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Distributions of 1000 medians from different sample sizes\n\n\n\nWhat differences do you see?"
  },
  {
    "objectID": "slides/week7-day1.html#section-5",
    "href": "slides/week7-day1.html#section-5",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Variability for Different Sample Sizes\n\n\n\n\n\n\n\nSample Size\nStandard Error of Median\n\n\n\n\n25\n19408.079\n\n\n50\n12459.358\n\n\n100\n8279.311\n\n\n\n\n\n\n\n\n\n\n\nStandard errors quantify the variability of point estimates\nAs a general rule, as sample size increases, the standard error decreases.\n\n\n\n\n\n\n\n\n\n\nStandard error vs. Standard deviation\n\n\nCareful! There are important differences between standard errors and standard deviations."
  },
  {
    "objectID": "slides/week7-day1.html#section-6",
    "href": "slides/week7-day1.html#section-6",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Does sample size change how accurate the estimate is?\n\n\nDoes sample size change how precise the estimate is?"
  },
  {
    "objectID": "slides/week7-day1.html#section-7",
    "href": "slides/week7-day1.html#section-7",
    "title": "Sampling Variability – The Heart of Statistical Inference",
    "section": "",
    "text": "Sampling Activity!"
  },
  {
    "objectID": "slides/week6-day2.html#deadline-reminders",
    "href": "slides/week6-day2.html#deadline-reminders",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Deadline Reminders",
    "text": "Deadline Reminders\n\nLab 4 revisions are due tonight\nStatistical Critique 1 revisions are due next Thursday\nThe final version of your Midterm Project is due Sunday at midnight\n\n\n\n\n\n\n\nDeadline Extensions\n\n\nYou cannot request deadline extensions for the final version of your Midterm Project. The assignment portal closes at 12:00am on Monday. Do not ride the line.\n\n\n\n\n\n\n\n\n\n\nResponding to Questions / Issues\n\n\nI will not be responding to emails / Discord messages this weekend."
  },
  {
    "objectID": "slides/week6-day2.html#help-your-peers",
    "href": "slides/week6-day2.html#help-your-peers",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Help your peers!",
    "text": "Help your peers!\n\nAre the arguments / sentences easy to understand? Does the report flow?\nIs the same information included in multiple places?\nDo the plots have nice axis labels?\nCan you easily find the regression equations? Do the equations make sense?\nDo the interpretations / conclusions from the equations make sense?\nDo they justify who they can infer their results onto?\nDo they justify why they can / cannot use causal language?"
  },
  {
    "objectID": "slides/week6-day2.html#formatting-guidelines",
    "href": "slides/week6-day2.html#formatting-guidelines",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Formatting Guidelines",
    "text": "Formatting Guidelines\n\nDon’t write in bullet points\nAvoid redundancy as much as possible\nMake contents easy to navigate\n\nUse subheaders and / or boldface to indicate important content\nPlace your regression equations on their own line"
  },
  {
    "objectID": "slides/week6-day2.html#section",
    "href": "slides/week6-day2.html#section",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "",
    "text": "Better Practices for Data Visualization"
  },
  {
    "objectID": "slides/week6-day2.html#some-guidelines",
    "href": "slides/week6-day2.html#some-guidelines",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Some Guidelines",
    "text": "Some Guidelines\n\n\n\nTitles should be top left aligned (usually)\nDon’t make people tilt their head\nAlignment should create clean lines and symmetry\nRemove and lighten grids as much as possible\n\n\n\n\n\nLegends suck\nWhitespace is like garlic; take the amount you need and then triple it\nFonts matter\nColor is hard"
  },
  {
    "objectID": "slides/week6-day2.html#some-guidelines-1",
    "href": "slides/week6-day2.html#some-guidelines-1",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Some Guidelines",
    "text": "Some Guidelines\n\n\n\nTitles should be top left aligned (usually)\nDon’t make people tilt their head\nAlignment should create clean lines and symmetry\nRemove and lighten grids as much as possible\n\n\n\n\n\nLegends suck\nWhitespace is like garlic; take the amount you need and then triple it\nFonts matter\nColor is hard"
  },
  {
    "objectID": "slides/week6-day2.html#dont-make-people-tilt-their-head",
    "href": "slides/week6-day2.html#dont-make-people-tilt-their-head",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Don’t make people tilt their head",
    "text": "Don’t make people tilt their head"
  },
  {
    "objectID": "slides/week6-day2.html#tailoring-colors",
    "href": "slides/week6-day2.html#tailoring-colors",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Tailoring Colors",
    "text": "Tailoring Colors\nDifferent color palettes are available through a variety of R packages. Popular options include:\n\n\n\nRColorBrewer\n\n\n\n\n\n\nviridis\n\n\n\n\n\nThis website provides an exhaustive list of color themes."
  },
  {
    "objectID": "slides/week6-day2.html#grids-remove-and-lighten-as-much-as-possible",
    "href": "slides/week6-day2.html#grids-remove-and-lighten-as-much-as-possible",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Grids: remove and lighten as much as possible",
    "text": "Grids: remove and lighten as much as possible"
  },
  {
    "objectID": "slides/week6-day2.html#a-reasonable-option-theme_minimal",
    "href": "slides/week6-day2.html#a-reasonable-option-theme_minimal",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "A Reasonable Option – theme_minimal()",
    "text": "A Reasonable Option – theme_minimal()"
  },
  {
    "objectID": "slides/week6-day2.html#an-example-for-categorical",
    "href": "slides/week6-day2.html#an-example-for-categorical",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "An Example for Categorical",
    "text": "An Example for Categorical\n\nand_vertebrates %&gt;% \n  ggplot(mapping = aes(x = length_1_mm, \n                     y = weight_g, \n                     color = species)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Length (mm)\", \n      y = \"\", \n      color = \"Species\",\n      title = \"Comparing Weight (g) versus Length (mm) \\nfor Vertebrates in Mack Creek\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\")"
  },
  {
    "objectID": "slides/week6-day2.html#an-example-for-categorical-output",
    "href": "slides/week6-day2.html#an-example-for-categorical-output",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "An Example for Categorical",
    "text": "An Example for Categorical"
  },
  {
    "objectID": "slides/week6-day2.html#an-example-for-continuous",
    "href": "slides/week6-day2.html#an-example-for-continuous",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "An Example for Continuous",
    "text": "An Example for Continuous\n\npie_crab %&gt;% \n  ggplot(mapping = aes(x = latitude, \n                     y = size, \n                     color = water_temp)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Latitude\", \n      y = \"\", \n      color = \"Water Temperature (C)\",\n      title = \"Size of Fiddler Crabs (mm) Across the Atlantic Coast\") +\n  theme_minimal() +\n  scale_color_viridis_c(option = \"viridis\")"
  },
  {
    "objectID": "slides/week6-day2.html#an-example-for-continuous-output",
    "href": "slides/week6-day2.html#an-example-for-continuous-output",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "An Example for Continuous",
    "text": "An Example for Continuous"
  },
  {
    "objectID": "slides/week6-day2.html#section-1",
    "href": "slides/week6-day2.html#section-1",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "",
    "text": "Data is Power"
  },
  {
    "objectID": "slides/week6-day2.html#section-2",
    "href": "slides/week6-day2.html#section-2",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "",
    "text": "“Data is the new oil.”\n\n\nThe Economist, Intel CEO, Reliance Industrices CEO, UAE Minister of Artifical Intelligence, Google execs, etc., mainly elite white men\n\n\n\n“Data is the same old oppression.”\n\n\nBIWOC, white women, Indigenous people, immigrant communities, LGBTQ+ individuals, + more"
  },
  {
    "objectID": "slides/week6-day2.html#does-your-phone-recognize-your-face",
    "href": "slides/week6-day2.html#does-your-phone-recognize-your-face",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Does your phone recognize your face?",
    "text": "Does your phone recognize your face?\n\nJoy Buolamwini found that she had to put on a white mask for the facial detection program to “see” her face."
  },
  {
    "objectID": "slides/week6-day2.html#how-does-google-label-your-images",
    "href": "slides/week6-day2.html#how-does-google-label-your-images",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "How does Google label your images?",
    "text": "How does Google label your images?"
  },
  {
    "objectID": "slides/week6-day2.html#should-the-cash-bail-system-change",
    "href": "slides/week6-day2.html#should-the-cash-bail-system-change",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Should the cash bail system change?",
    "text": "Should the cash bail system change?"
  },
  {
    "objectID": "slides/week6-day2.html#will-your-car-be-able-to-drive-you",
    "href": "slides/week6-day2.html#will-your-car-be-able-to-drive-you",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Will your car be able to drive you?",
    "text": "Will your car be able to drive you?"
  },
  {
    "objectID": "slides/week6-day2.html#forward-selection-by-hand",
    "href": "slides/week6-day2.html#forward-selection-by-hand",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "Forward Selection (by Hand)",
    "text": "Forward Selection (by Hand)\n\nStart with the most basic model (one mean)\nDecide which one variable to add (based on adjusted \\(R^2\\))\nDecide if you should add another variable\n\n\\(\\vdots\\)\n\nStop adding variables when adjusted \\(R^2\\) stops increasing"
  },
  {
    "objectID": "slides/week6-day2.html#a-more-automated-option",
    "href": "slides/week6-day2.html#a-more-automated-option",
    "title": "Midterm Project Feedback, Visualization Best (Better) Practices & Machine Learning",
    "section": "A More Automated Option",
    "text": "A More Automated Option\n\nevals_train %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals_train)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-ID, \n         -score,\n         -&lt;VARIABLE 1 SELECTED&gt;,\n         -&lt;VARIABLE 2 SELECTED&gt;\n         ) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)"
  },
  {
    "objectID": "slides/data-viz-principles.html#section",
    "href": "slides/data-viz-principles.html#section",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "theme_classic() in ggplot()\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "slides/data-viz-principles.html#section-1",
    "href": "slides/data-viz-principles.html#section-1",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "Gestalt Principles\n\n\n\nGestalt psychology is a theory of perception that believes humans are inclined to understand objects as an entire structure rather than the sum of its parts.\n\n\n\nGestalt Principles for Data Visualization\n\n\nThe Figure and Ground Principle describes the capacity to perceive the relationship between form and surrounding space to create meaning. A sense of wholeness or unity depends on how you perceive the relationship between an object and the area in which it is contained. The ‘figure’ is the focus element, while the ‘ground’ is the figure’s background."
  },
  {
    "objectID": "slides/data-viz-principles.html#section-2",
    "href": "slides/data-viz-principles.html#section-2",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "theme_bw() in ggplot()\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\nList of all ggplot() themes"
  },
  {
    "objectID": "slides/data-viz-principles.html#section-3",
    "href": "slides/data-viz-principles.html#section-3",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "An aura objectivity\n\n\n\n\n\n\n\n\n\n\n\n\n“We focus on four conventions which imbue visualizations with a sense of objectivity, transparency and facticity. These include: (a) two-dimensional viewpoints, (b) clean layouts, (c) geometric shapes and lines, (d) the inclusion of data sources.”\nThe work that visualization communications do"
  },
  {
    "objectID": "slides/data-viz-principles.html#section-4",
    "href": "slides/data-viz-principles.html#section-4",
    "title": "Data Visualization Principles",
    "section": "",
    "text": "Elevate emotion\n\nhttps://guns.periscopic.com/"
  },
  {
    "objectID": "slides/week1-day2.html#section",
    "href": "slides/week1-day2.html#section",
    "title": "Working with Data",
    "section": "",
    "text": "There’s Data\n\n\nGender stereotypes in 5-7 year old children\n\n\n\n\n\n\n\n\nsubject\nsex\nage\ntrait\ntarget\nstereotype\nhigh_achieve_caution\n\n\n\n\n116\nmale\n5\nsmart\nadults\n0.00\n0.50\n\n\n139\nmale\n7\nsmart\nchildren\n0.75\n0.75\n\n\n12\nfemale\n6\nsmart\nadults\n0.25\n0.75\n\n\n140\nmale\n5\nnice\nchildren\n0.50\n0.25\n\n\n76\nfemale\n5\nnice\nadults\n1.00\n1.00\n\n\n10\nmale\n5\nsmart\nadults\n1.00\n0.50"
  },
  {
    "objectID": "slides/week1-day2.html#section-1",
    "href": "slides/week1-day2.html#section-1",
    "title": "Working with Data",
    "section": "",
    "text": "Lots of Data\n\nBody girth measurements and skeletal diameter measurements for 247 men and 260 women.\n\n\n\n\n\n\nage\nwgt\nhgt\nsex\nsho_gi\nwai_gi\nnav_gi\nhip_gi\n\n\n\n\n46\n84.5\n181.6\n1\n111.5\n91.6\n102.1\n106.7\n\n\n20\n56.4\n163.2\n0\n105.1\n66.1\n73.2\n91.8\n\n\n37\n75.5\n177.8\n1\n116.7\n75.9\n77.0\n93.4\n\n\n29\n81.8\n182.9\n0\n108.9\n83.6\n108.6\n108.3\n\n\n62\n64.6\n167.0\n1\n104.0\n76.0\n83.0\n93.0\n\n\n43\n87.3\n188.0\n1\n119.0\n97.8\n93.6\n102.5"
  },
  {
    "objectID": "slides/week1-day2.html#section-2",
    "href": "slides/week1-day2.html#section-2",
    "title": "Working with Data",
    "section": "",
    "text": "In Every Context\n\n\nNBA player of the week from 1985 to 2016\n\n\n\n\n\n\n\n\nAge\nDate\nDraft Year\nHeight\nPlayer\nPosition\n\n\n\n\n22\nJan 25, 2004\n2002\n6-9\nCarlos Boozer\nPF\n\n\n27\nDec 7, 1986\n1981\n6-10\nTom Chambers\nPF\n\n\n27\nFeb 8, 2004\n1997\n6-11\nTim Duncan\nFC\n\n\n33\nDec 20, 2010\n1998\n6-7\nPaul Pierce\nSF\n\n\n27\nDec 3, 2006\n2000\n6-6\nMichael Redd\nG\n\n\n27\nJan 1, 2006\n1999\n6-7\nShawn Marion\nF"
  },
  {
    "objectID": "slides/week1-day2.html#section-3",
    "href": "slides/week1-day2.html#section-3",
    "title": "Working with Data",
    "section": "",
    "text": "You Can Imagine\n\n\nFish Sampled on Blackfoot River\n\n\n\n\n\n\n\n\ntrip\nmark\nlength\nweight\nyear\nsection\nspecies\n\n\n\n\n1\n0\n186\n80\n2004\nScottyBrown\nRBT\n\n\n1\n0\n142\n25\n2004\nScottyBrown\nRBT\n\n\n1\n0\n203\n80\n1996\nJohnsrud\nRBT\n\n\n2\n0\n113\n25\n1990\nScottyBrown\nRBT\n\n\n2\n1\n369\nNA\n2002\nJohnsrud\nBrown\n\n\n1\n0\n192\n60\n1993\nJohnsrud\nRBT"
  },
  {
    "objectID": "slides/week1-day2.html#section-4",
    "href": "slides/week1-day2.html#section-4",
    "title": "Working with Data",
    "section": "",
    "text": "Your Turn\n\n\nEvery year, the US releases to the public a large data set containing information on births recorded in the country.\nA total of 13 variables were collected on every birth, including information about:\n\nthe birth (baby weight, sex of baby, premie status)\nthe pregnancy (hospital visits, length of gestation, )\nthe mother’s attributes (age, smoking status, marital status, race)\nthe father’s age\n\n\n\n\nHow would you expect this dataframe to look?"
  },
  {
    "objectID": "slides/week1-day2.html#section-5",
    "href": "slides/week1-day2.html#section-5",
    "title": "Working with Data",
    "section": "",
    "text": "Types of Variables\n\n\n\nDiagram of types of variables we will analyze!"
  },
  {
    "objectID": "slides/week1-day2.html#section-6",
    "href": "slides/week1-day2.html#section-6",
    "title": "Working with Data",
    "section": "",
    "text": "Examples\n\n\nA person’s height (usually) would be a continuous, numerical variable\nThe number of classes someone takes would be a discrete, numerical variable\nA course letter grade would be a ordinal, categorical variable\nThe color of someone’s hair would be a regular, categorical variable"
  },
  {
    "objectID": "slides/week1-day2.html#section-7",
    "href": "slides/week1-day2.html#section-7",
    "title": "Working with Data",
    "section": "",
    "text": "Your Turn\n\n\nSuppose researchers have yearly data on Elephant Seal abundance on Pedras Blancas from 2010 - 2014.\n\n\n\nWhat type of variable would year be?"
  },
  {
    "objectID": "slides/week1-day2.html#section-8",
    "href": "slides/week1-day2.html#section-8",
    "title": "Working with Data",
    "section": "",
    "text": "Types of Studies\n\n\n\nExperiment\n\nrandomization\nreplication\ncontrolling\nblocking\n\n\n\n\nObservational Study\n\ncollect data in a way that does not directly interfere with how the data arise"
  },
  {
    "objectID": "slides/week1-day2.html#section-9",
    "href": "slides/week1-day2.html#section-9",
    "title": "Working with Data",
    "section": "",
    "text": "Relationships Between Variables\n\n\nexplanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable\n\n\nIf two variables are not associated, then they are said to be independent.\nIf two variables are associated, then they are said to be dependent."
  },
  {
    "objectID": "slides/week1-day2.html#section-10",
    "href": "slides/week1-day2.html#section-10",
    "title": "Working with Data",
    "section": "",
    "text": "Causal Inference\n\n\nassociation \\(\\neq\\) causation\n\n\nWhat do you need to say that the explanatory variable causes a change in the response variable?"
  },
  {
    "objectID": "slides/week1-day2.html#section-11",
    "href": "slides/week1-day2.html#section-11",
    "title": "Working with Data",
    "section": "",
    "text": "Lab Warm-up"
  },
  {
    "objectID": "slides/week1-day2.html#section-12",
    "href": "slides/week1-day2.html#section-12",
    "title": "Working with Data",
    "section": "",
    "text": "Data Types in R\n\n\n\nglimpse(births_small)\n\nRows: 1,000\nColumns: 10\n$ fage           &lt;int&gt; 34, 36, 37, NA, 32, 32, 37, 29, 30, 29, 30, 34, 28, 28,…\n$ mage           &lt;dbl&gt; 34, 31, 36, 16, 31, 26, 36, 24, 32, 26, 34, 27, 22, 31,…\n$ weeks          &lt;dbl&gt; 37, 41, 37, 38, 36, 39, 36, 40, 39, 39, 42, 40, 40, 39,…\n$ premie         &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"pr…\n$ gained         &lt;dbl&gt; 28, 41, 28, 29, 48, 45, 20, 65, 25, 22, 40, 30, 31, NA,…\n$ weight         &lt;dbl&gt; 6.96, 8.86, 7.51, 6.19, 6.75, 6.69, 6.13, 6.74, 8.94, 9…\n$ lowbirthweight &lt;chr&gt; \"not low\", \"not low\", \"not low\", \"not low\", \"not low\", …\n$ sex            &lt;fct&gt; male, female, female, male, female, female, female, mal…\n$ habit          &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"no…\n$ whitemom       &lt;chr&gt; \"white\", \"white\", \"not white\", \"white\", \"white\", \"white…\n\n\n\n\n\nWhat do you think dbl means?\nHow is that different from int?\n\n\n\nWhat does chr mean?\nHow might it differ from fct?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section",
    "href": "slides/week1-day1-alt.html#section",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "About Me…"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-1",
    "href": "slides/week1-day1-alt.html#section-1",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What is Statistics?\n\n\n\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data.\n\n\nStatistics is the study of how best to collect, analyze, and draw conclusions from data.\n\n\nIntroduction to Modern Statistics"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-2",
    "href": "slides/week1-day1-alt.html#section-2",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What Statistics Is To Me\n\n \n\n\n\n\nThe data science cycle – Wickham & Grolemund"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-3",
    "href": "slides/week1-day1-alt.html#section-3",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What you can expect in STAT 313\n\nThis course will teach you the fundamentals of linear models—simple linear regression, multiple linear regression, and analysis of variance—and experimental design. You will extend the concepts covered in your Stat I course, to:\n\n\nwork with data in a reproducible way (using R)\nvisualize and summarize a variety of datasets (in R)\ncritically evaluate the use of Statistics\nperform statistical analyses to answer research questions (using R)"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-4",
    "href": "slides/week1-day1-alt.html#section-4",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Coding 🙀\n\n\nCoding is a huge part of how doing statistics in the wild looks.\n\n\n\nEveryone is coming from a different background\nDifferent aspects of the course will be difficult to different people\nYou will be given coding resources each week\nUse your peers to support your learning"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-5",
    "href": "slides/week1-day1-alt.html#section-5",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Course Components\n\n\n\n\nBefore Class\n\n\nReading Guides\nConcept Quizzes\nR Tutorials\n\n\n\n\n\nDuring Class\n\n\nGroup Discussion\nHands-on Activities\nLab Assignments\n\n\n\n\n\nOutside of Class\n\n\nStatistical Critiques\nMidterm Project\nFinal Project"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-6",
    "href": "slides/week1-day1-alt.html#section-6",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Specifications Based Grading"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-8",
    "href": "slides/week1-day1-alt.html#section-8",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Tidy Data\n\n\nExpected layout of “tidy” datasets"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-9",
    "href": "slides/week1-day1-alt.html#section-9",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Gender stereotypes in 5-7 year old children\n\n\n\n\n\n\n\n\nsubject\nsex\nage\ntrait\ntarget\nstereotype\nhigh_achieve_caution\n\n\n\n\n49\nmale\n7\nsmart\nchildren\n1.00\n0.25\n\n\n96\nfemale\n6\nnice\nchildren\n1.00\n1.00\n\n\n37\nfemale\n7\nnice\nchildren\n1.00\n1.00\n\n\n37\nfemale\n7\nsmart\nchildren\n0.75\n1.00\n\n\n139\nmale\n7\nsmart\nchildren\n0.75\n0.75\n\n\n135\nmale\n7\nsmart\nadults\n0.75\n0.50"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-10",
    "href": "slides/week1-day1-alt.html#section-10",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Body girth and skeletal diameter measurements\n\n\n\n\n\n\n\nage\nwgt\nhgt\nsex\nsho_gi\nwai_gi\nnav_gi\nhip_gi\n\n\n\n\n19\n70.6\n178.0\n0\n103.0\n68.0\n84.5\n99.5\n\n\n21\n81.6\n184.0\n1\n119.5\n77.5\n81.5\n99.8\n\n\n36\n54.5\n167.6\n0\n96.4\n73.6\n86.9\n94.7\n\n\n40\n56.8\n168.9\n0\n98.2\n62.9\n76.8\n95.5\n\n\n32\n58.4\n173.2\n0\n97.7\n68.6\n83.0\n94.2\n\n\n49\n89.1\n182.9\n1\n115.6\n103.6\n100.7\n100.6"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-11",
    "href": "slides/week1-day1-alt.html#section-11",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "NBA player of the week\n\n\n\n\n\n\n\n\nAge\nDate\nDraft Year\nHeight\nPlayer\nPosition\n\n\n\n\n23\nDec 15, 1985\n1984\n6-3\nAlvin Robertson\nSG\n\n\n20\nJan 9, 2005\n2003\n6-11\nChris Bosh\nPF\n\n\n27\nNov 21, 2016\n2011\n201cm\nJimmy Butler\nGF\n\n\n29\nDec 2, 1984\n1977\n6-11\nJack Sikma\nC\n\n\n24\nJan 4, 2004\n1999\n6-9\nElton Brand\nPF\n\n\n25\nMar 1, 1992\n1988\n6-10\nDanny Manning\nF"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-12",
    "href": "slides/week1-day1-alt.html#section-12",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Your Turn\n\n\nEvery year, the US releases to the public a large data set containing information on births recorded in the country.\nA total of 13 variables were collected on every birth, including information about:\n\nthe birth (baby weight, sex of baby, premie status)\nthe pregnancy (hospital visits, length of gestation, )\nthe birth parent’s attributes (age, smoking status, marital status, race)\nthe partner’s age\n\n\n\n\nHow would you expect this dataframe to look?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-13",
    "href": "slides/week1-day1-alt.html#section-13",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Military Spending\n\n\n\n\n\n\n\nCountry\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n\n\n\n\nAfrica\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nUSA\n4.922642\n4.840174\n4.477401\n4.046679\n3.695891\n3.477846\n3.418941\n3.313385\n3.316244\n3.413107\n\n\nAustralia\n1.856791\n1.757078\n1.670963\n1.639861\n1.772211\n1.950601\n2.081512\n1.997974\n1.894180\n1.879802\n\n\nNorway\n1.515698\n1.451436\n1.402134\n1.413997\n1.472039\n1.507280\n1.626681\n1.622270\n1.628300\n1.684120\n\n\nSweden\n1.188288\n1.104288\n1.133304\n1.116715\n1.129776\n1.069579\n1.052833\n1.024075\n1.031100\n1.120599\n\n\n\n\n\n\n\n\n\n\n\nDo these data satisfy the “tidy” principles?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-14",
    "href": "slides/week1-day1-alt.html#section-14",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Vehicle Efficiency\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nToyota Corolla\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.9\n1\n1\n4\n1\n\n\nPorsche 914-2\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.7\n0\n1\n5\n2\n\n\nVolvo 142E\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.6\n1\n1\n4\n2\n\n\nFord Pantera L\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.5\n0\n1\n5\n4\n\n\n\n\n\n\n\n\n\n\n\nDo these data satisfy the “tidy” principles?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-15",
    "href": "slides/week1-day1-alt.html#section-15",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-16",
    "href": "slides/week1-day1-alt.html#section-16",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Types of Variables\n\n\n\nDiagram of types of variables we will analyze!"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-18",
    "href": "slides/week1-day1-alt.html#section-18",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "{fig-alt=“An image of the three different types of categorical variables, nominal, ordinal, and binary. The ‘nominal’ category has a turtle, snail, and butterfly and represents variables with unordered descriptions. The ‘ordinal’ category has three bumblebees, one saying ‘I am happy,’ one saying ‘I am oka,” and one saying ’I am awesome!!!’. The bumblebees’ emotions represent an ordinal variable. Lastly, the ‘binary’ category references variables that have two mutually exclusive outcomes. The image displayed under ‘binary’ is a t-rex saying ‘I am extinct’ and a shark saying ‘Ha’ (because they are not extinct).”}"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-19",
    "href": "slides/week1-day1-alt.html#section-19",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Your Turn (90-seconds)\n\n\nWrite down one example of:\n\na continuous, numerical variable\na discrete, numerical variable\nan ordinal, categorical variable\na regular, categorical variable\n\n\n\nShare out!"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-21",
    "href": "slides/week1-day1-alt.html#section-21",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Data Types in R\n\n\n\nglimpse(births_small)\n\nRows: 1,000\nColumns: 10\n$ fage           &lt;int&gt; 34, 36, 37, NA, 32, 32, 37, 29, 30, 29, 30, 34, 28, 28,…\n$ mage           &lt;dbl&gt; 34, 31, 36, 16, 31, 26, 36, 24, 32, 26, 34, 27, 22, 31,…\n$ weeks          &lt;dbl&gt; 37, 41, 37, 38, 36, 39, 36, 40, 39, 39, 42, 40, 40, 39,…\n$ premie         &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"pr…\n$ gained         &lt;dbl&gt; 28, 41, 28, 29, 48, 45, 20, 65, 25, 22, 40, 30, 31, NA,…\n$ weight         &lt;dbl&gt; 6.96, 8.86, 7.51, 6.19, 6.75, 6.69, 6.13, 6.74, 8.94, 9…\n$ lowbirthweight &lt;chr&gt; \"not low\", \"not low\", \"not low\", \"not low\", \"not low\", …\n$ sex            &lt;fct&gt; male, female, female, male, female, female, female, mal…\n$ habit          &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"no…\n$ whitemom       &lt;chr&gt; \"white\", \"white\", \"not white\", \"white\", \"white\", \"white…\n\n\n\n\n\nWhat do you think dbl means?\nHow is that different from int?\n\n\n\nWhat does chr mean?\nHow might it differ from fct?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-22",
    "href": "slides/week1-day1-alt.html#section-22",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Types of Studies\n\n\n\nExperiment\n\nrandomization\nreplication\ncontrolling\nblocking\n\n\n\n\nObservational Study\n\ncollect data in a way that does not directly interfere with how the data arise"
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-23",
    "href": "slides/week1-day1-alt.html#section-23",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Relationships Between Variables\n\n\nexplanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable\n\n\nIf two variables are not associated, then they are said to be independent.\nIf two variables are associated, then they are said to be dependent."
  },
  {
    "objectID": "slides/week1-day1-alt.html#section-24",
    "href": "slides/week1-day1-alt.html#section-24",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Causal Inference\n\n\nassociation \\(\\neq\\) causation\n\n\nWhat do you need to say that the explanatory variable causes a change in the response variable?"
  },
  {
    "objectID": "slides/week1-day1-alt.html#joining-the-stat-313-workspace-on-posit-cloud",
    "href": "slides/week1-day1-alt.html#joining-the-stat-313-workspace-on-posit-cloud",
    "title": "Welcome to Stat 313!",
    "section": "Joining the STAT 313 Workspace on Posit Cloud",
    "text": "Joining the STAT 313 Workspace on Posit Cloud\n\nAccess Posit Cloud from link posted on Canvas\nCreate a Cloud Student account (using your Cal Poly email)\n\n\n\n\n\n\n\n$5 / month subscription\n\n\nThe Cloud Student account costs $5 / month. You will only need to pay for three (3) months of access, for a total of $15 for the quarter.\n\n\n\n\nJoin the STAT 313 workspace"
  },
  {
    "objectID": "slides/week1-day1-alt.html#accessing-lab-1",
    "href": "slides/week1-day1-alt.html#accessing-lab-1",
    "title": "Welcome to Stat 313!",
    "section": "Accessing Lab 1",
    "text": "Accessing Lab 1\n\nAccess Lab 1 either through:\n\n\nthe link posted on Canvas\nthe Content tab in the STAT 313 workspace\n\n\nClick on Lab 1 to open the Project"
  },
  {
    "objectID": "slides/week1-day1-alt.html#opening-the-lab-1-document",
    "href": "slides/week1-day1-alt.html#opening-the-lab-1-document",
    "title": "Welcome to Stat 313!",
    "section": "Opening the Lab 1 Document",
    "text": "Opening the Lab 1 Document\n\nClick on the lab-1.qmd file in the lower right hand corner to open the lab assignment\n\n{fig-alt=“A screenshot of the lower right hand panel of Posit Cloud. The image displays the documents listed under the”Files” tab, with lab-1.qmd (a Quarto file) higlighted in a purple box.}\n\nStart working!"
  },
  {
    "objectID": "slides/week4-day2.html#revisions-due-tonight",
    "href": "slides/week4-day2.html#revisions-due-tonight",
    "title": "Week 4, Day 2",
    "section": "Revisions Due Tonight",
    "text": "Revisions Due Tonight\nPlease make sure you submitted reflections with your revisions! If there are not revisions present when I start grading them tomorrow morning, your revisions are not eligible to be regraded."
  },
  {
    "objectID": "slides/week4-day2.html#a-grading-reminder",
    "href": "slides/week4-day2.html#a-grading-reminder",
    "title": "Week 4, Day 2",
    "section": "A Grading Reminder",
    "text": "A Grading Reminder\n\n\n\n\n“Complete” = Satisfactory\n\n\nYour group obtained a “Success” on every question\n\n\n\n\n“Incomplete” = Growing\n\n\nYour group received a “Growing” on at least one question"
  },
  {
    "objectID": "slides/week4-day2.html#common-mistakes",
    "href": "slides/week4-day2.html#common-mistakes",
    "title": "Week 4, Day 2",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\n\n\nCategorical variables in R (Q2)\n\nWhat data types does R use to store categorical variables? Integers? Characters? Doubles? Factors? Dates?\nThe output of glimpse() can help!\n\nComparing distributions between groups (Q9)\n\nWere trout observed in every channel type in both sections of forest?\n\nCalculating group means (Q10)\n\ngroup_by()creates groups based on a categorical variable not based on the dataset\ngroup_by(species) not group_by(trout)"
  },
  {
    "objectID": "slides/week4-day2.html#copying-the-lab-last-weeks-recorder",
    "href": "slides/week4-day2.html#copying-the-lab-last-weeks-recorder",
    "title": "Week 4, Day 2",
    "section": "Copying the Lab – Last Week’s Recorder",
    "text": "Copying the Lab – Last Week’s Recorder\nThe person who typed your lab needs to make their project “public”\n\nOpen Posit Cloud\nGo to the STAT 313 workspace\nClick on “Your Content”\nOpen the settings for your Lab 3 project"
  },
  {
    "objectID": "slides/week4-day2.html#copying-the-lab-last-weeks-recorder-1",
    "href": "slides/week4-day2.html#copying-the-lab-last-weeks-recorder-1",
    "title": "Week 4, Day 2",
    "section": "Copying the Lab – Last Week’s Recorder",
    "text": "Copying the Lab – Last Week’s Recorder\n\nChange the access for your project to “Space Members”"
  },
  {
    "objectID": "slides/week4-day2.html#copying-the-lab-everyone-else",
    "href": "slides/week4-day2.html#copying-the-lab-everyone-else",
    "title": "Week 4, Day 2",
    "section": "Copying the Lab – Everyone Else",
    "text": "Copying the Lab – Everyone Else\n\nFind your group member’s lab (you can use the search bar to search for their name)\n\n\n\nOpen their Lab 3 project\nSelect “Save a Permanent Copy”"
  },
  {
    "objectID": "slides/week4-day2.html#completing-revisions",
    "href": "slides/week4-day2.html#completing-revisions",
    "title": "Week 4, Day 2",
    "section": "Completing Revisions",
    "text": "Completing Revisions\nLab 3 revisions are due by Wednesday, May 1.\n\n\nRead comments on Canvas\nCopy your group’s lab assignment\nComplete your revisions\nRender your revised Lab 3\nDownload your revised HTML\nSubmit your revisions to the original Lab 3 assignment portal\n\n\n\n\n\n\n\n\nReflections\n\n\n\nRevisions are required to be accompanied with reflections on what you learned while completing your revisions. These can be written in your Lab 3 file (next to the problems you revised), in a Word document, or in the comment box on Canvas."
  },
  {
    "objectID": "slides/week4-day2.html#least-squares",
    "href": "slides/week4-day2.html#least-squares",
    "title": "Week 4, Day 2",
    "section": "Least Squares",
    "text": "Least Squares\n\n\nPublished in 1805 by Legendre\n\n\n\n\n\n\n\n\n\n\n\n\nand Gauss in 1809\n\n\n\n\n\n\n\n\n\n\n\n\nUsed to determine, from astronomical observations, the orbits of bodies about the Sun."
  },
  {
    "objectID": "slides/week4-day2.html#regression",
    "href": "slides/week4-day2.html#regression",
    "title": "Week 4, Day 2",
    "section": "“regression”",
    "text": "“regression”\n\n\n\n\n\n\n\n\nCoined by Francis Galton in the 19th century\nDescribed a biological phenomenon\n\nHeights of children of tall parents tend to be tall, but shorter than their parents\n“regression to the mean”"
  },
  {
    "objectID": "slides/week4-day2.html#a-polymath",
    "href": "slides/week4-day2.html#a-polymath",
    "title": "Week 4, Day 2",
    "section": "A “polymath”",
    "text": "A “polymath”\n\n\n\n\n\n\n\nIn Statistics, Galton (1822–1911) is a towering figure.\nHe invented standard deviation, correlation, linear regression, ANOVA\nGalton’s developments and discoveries were fueled in large part by his fascination with the science of heredity."
  },
  {
    "objectID": "slides/week4-day2.html#the-invention-of-eugenics",
    "href": "slides/week4-day2.html#the-invention-of-eugenics",
    "title": "Week 4, Day 2",
    "section": "The Invention of Eugenics",
    "text": "The Invention of Eugenics\n\n\n\nBased on Greek eugenes, meaning “well-born”\nThe science of heredity could help humanity better itself through breeding.\nGalton served as founding president of the British Eugenics Society\n\n\n\n\n\n\n“What nature does blindly, slowly and ruthlessly, man may do providently, quickly, and kindly. As it lies within his power, so it becomes his duty to work in that direction.”\nFrancis Galton"
  },
  {
    "objectID": "slides/week4-day2.html#and-then-it-spread",
    "href": "slides/week4-day2.html#and-then-it-spread",
    "title": "Week 4, Day 2",
    "section": "And then it spread…",
    "text": "And then it spread…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMein Kampf references the ideas of British and American eugenicists\nDeclared non-Aryan races inferior\nBelieved Germans should do everything possible to make sure their gene pool stayed “pure”"
  },
  {
    "objectID": "slides/week4-day2.html#but-wasnt-that-a-long-time-ago",
    "href": "slides/week4-day2.html#but-wasnt-that-a-long-time-ago",
    "title": "Week 4, Day 2",
    "section": "But wasn’t that a long time ago?",
    "text": "But wasn’t that a long time ago?\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetween 1970 and 1976 between 25% and 50% of Native Americans were sterilized, many without consent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn 1927 the US Supreme Court ruled that sterilization of the handicapped did not violate the Constitution.\n\nIn 1957 “conservatorship” was introduced “to avoid the stigma of incompetency”"
  },
  {
    "objectID": "slides/week4-day2.html#would-you-exist",
    "href": "slides/week4-day2.html#would-you-exist",
    "title": "Week 4, Day 2",
    "section": "Would you exist?",
    "text": "Would you exist?\n\n\n\nIs your skin white?\nAre you blonde?\nDo you have blue eyes?\n\n\n\nWere your ancestors poor?\nAre you Muslim, Hindu, Buddhist, Sikh, Tao, or Jewish?\nDo you identify as LGBTQIQ+?"
  },
  {
    "objectID": "slides/week4-day2.html#section",
    "href": "slides/week4-day2.html#section",
    "title": "Week 4, Day 2",
    "section": "",
    "text": "More Information\n\n\nRadiolab Presents: G\n\n“G” is a multi-episode exploration of one of the most dangerous ideas of the past century: the concept of intelligence.\nhttps://radiolab.org/series/radiolab-presents-g\n\nHow Eugenics shaped Statistics\n\nhttps://nautil.us"
  },
  {
    "objectID": "slides/week4-day2.html#todays-data",
    "href": "slides/week4-day2.html#todays-data",
    "title": "Week 4, Day 2",
    "section": "Today’s Data",
    "text": "Today’s Data\n\n\nData includes lake name, dates of freeze-up and thaw, and duration of ice cover of lakes in the Madison, WI area. Ice cover duration is the number of days that a lake is frozen, excluding periods where the lake thaws before refreezing again. Lakes Monona and Wingra are considered to be frozen if they are completely ice covered, while Lake Mendota is considered to be frozen if there is ice from Picnic Point to Maple Bluff and more than 50% of the lake is covered by ice."
  },
  {
    "objectID": "slides/week4-day2.html#research-question",
    "href": "slides/week4-day2.html#research-question",
    "title": "Week 4, Day 2",
    "section": "Research Question",
    "text": "Research Question\n \n\n\nHas the duration of ice cover changed over the last 175 years?"
  },
  {
    "objectID": "slides/week4-day2.html#data-layout",
    "href": "slides/week4-day2.html#data-layout",
    "title": "Week 4, Day 2",
    "section": "Data Layout",
    "text": "Data Layout\n\n\n\n\n\n\n\n\nlakeid\nice_on\nice_off\nice_duration\nyear\n\n\n\n\nLake Mendota\n1880-11-23\n1881-05-03\n161\n1880\n\n\nLake Mendota\n1932-12-10\n1933-04-04\n115\n1932\n\n\nLake Mendota\n2003-01-04\n2003-04-03\n89\n2002\n\n\nLake Mendota\n1953-12-30\n1954-03-25\n85\n1953\n\n\nLake Monona\n1963-12-18\n1964-03-19\n92\n1963\n\n\nLake Monona\n1989-12-07\n1990-03-15\n98\n1989\n\n\nLake Monona\n1985-12-06\n1986-03-29\n113\n1985\n\n\nLake Monona\n1996-12-20\n1997-03-26\n96\n1996\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecording the year of the winter\n\n\nHow is the year variable related to the ice_on and ice_off variables?"
  },
  {
    "objectID": "slides/week4-day1.html#section",
    "href": "slides/week4-day1.html#section",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Visualizing Linear Regression\n\n\n\n\n\n\n\nThe scatterplot has been called the most “generally useful invention in the history of statistical graphics.”\n\n\n\n\n\nIt is a simple two-dimensional plot in which the two coordinates of each dot represent the values of two variables measured on a single observation."
  },
  {
    "objectID": "slides/week4-day1.html#section-1",
    "href": "slides/week4-day1.html#section-1",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Characterizing Relationships\n\n\n\n\nForm (e.g. linear, quadratic, non-linear)\nDirection (e.g. positive, negative)\nStrength (how much scatter/noise?)\nUnusual observations (do points not fit the overall pattern?)"
  },
  {
    "objectID": "slides/week4-day1.html#section-2",
    "href": "slides/week4-day1.html#section-2",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Data for Today\n\nThe ncbirths dataset is a random sample of 1,000 cases taken from a larger dataset collected in North Carolina in 2004.\n\n\nEach case describes the birth of a single child born in North Carolina, along with various characteristics of the child (e.g. birth weight, length of gestation, etc.), the child’s mother (e.g. age, weight gained during pregnancy, smoking habits, etc.) and the child’s father (e.g. age)."
  },
  {
    "objectID": "slides/week4-day1.html#section-3",
    "href": "slides/week4-day1.html#section-3",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Your Turn!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would your characterize this relationship?\n\nform\ndirection\nstrength\nunusual observations"
  },
  {
    "objectID": "slides/week4-day1.html#cleaning-filtering-the-data",
    "href": "slides/week4-day1.html#cleaning-filtering-the-data",
    "title": "Introduction to Linear Regression",
    "section": "Cleaning & Filtering the Data",
    "text": "Cleaning & Filtering the Data\nIt seems like pregnancies with a gestation less than 28 weeks have a non-linear relationship with a baby’s birth weight, so we will filter these observations out of our dataset.\n\n\nbirths_post28 &lt;- ncbirths %&gt;% \n  drop_na(weight, weeks) %&gt;% \n  filter(weeks &gt; 28)\n\n\n\n\n\n\n\n\n\nChange in scope of inference\n\n\nRemoving these observations narrows the population of births we are able to make inferences onto! In this case, what population could we infer our findings onto?"
  },
  {
    "objectID": "slides/week4-day1.html#section-4",
    "href": "slides/week4-day1.html#section-4",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Correlation:\n\n\nstrength and direction of a linear relationship between two quantitative variables\n\n\n\n\n\nCorrelation coefficient between -1 and 1\nSign of the correlations shows direction\nMagnitude of the correlation shows strength"
  },
  {
    "objectID": "slides/week4-day1.html#section-5",
    "href": "slides/week4-day1.html#section-5",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Anscombe Correlations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFour datasets, very different graphical presentations\n\nsame mean and standard deviation in both \\(x\\) and \\(y\\)\nsame correlation\nsame regression line\n\n\n\n\n\n\nFor which of these relationships is correlation a reasonable summary measure?"
  },
  {
    "objectID": "slides/week4-day1.html#calculating-correlation-in-r",
    "href": "slides/week4-day1.html#calculating-correlation-in-r",
    "title": "Introduction to Linear Regression",
    "section": "Calculating Correlation in R",
    "text": "Calculating Correlation in R\n\nget_correlation(births_post28, \n                weeks ~ weight)\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.557\n\n\n\n\nWhat if I ran get_correlation(births_post28, weight ~ weeks) instead? Would I get the same value?"
  },
  {
    "objectID": "slides/week4-day1.html#section-6",
    "href": "slides/week4-day1.html#section-6",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Linear regression:\n\n\nwe assume the the relationship between our response variable (\\(y\\)) and explanatory variable (\\(x\\)) can be modeled with a linear function, plus some random noise\n\n\n\n\n\n\\(response = intercept + slope \\cdot explanatory + noise\\)"
  },
  {
    "objectID": "slides/week4-day1.html#writing-the-regression-equation",
    "href": "slides/week4-day1.html#writing-the-regression-equation",
    "title": "Introduction to Linear Regression",
    "section": "Writing the Regression Equation",
    "text": "Writing the Regression Equation\n\n\n\n\nPopulation Model\n\\(y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon\\)\n\n\\(y\\) = response\n\\(\\beta_0\\) = population intercept\n\\(\\beta_1\\) = population slope\n\\(\\epsilon\\) = errors / residuals\n\n\n\n\n\nSample Model\n\\(\\widehat{y} = b_0 + b_1 \\cdot x\\)\n\n\\(b_0\\) = sample intercept\n\\(b_1\\) = sample slope\n\nWhy does this equation have a hat on \\(y\\)?"
  },
  {
    "objectID": "slides/week4-day1.html#step-1-fit-a-linear-regression",
    "href": "slides/week4-day1.html#step-1-fit-a-linear-regression",
    "title": "Introduction to Linear Regression",
    "section": "Step 1: Fit a linear regression",
    "text": "Step 1: Fit a linear regression\n\nweeks_lm &lt;- lm(weight ~ weeks, \n               data = births_post28)"
  },
  {
    "objectID": "slides/week4-day1.html#step-2-obtain-coefficient-table",
    "href": "slides/week4-day1.html#step-2-obtain-coefficient-table",
    "title": "Introduction to Linear Regression",
    "section": "Step 2: Obtain coefficient table",
    "text": "Step 2: Obtain coefficient table\n\nget_regression_table(weeks_lm)\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n-5.003\n0.582\n-8.603\n0\n-6.144\n-3.862\n\n\nweeks\n0.316\n0.015\n21.010\n0\n0.287\n0.346\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nget_regression_table()\n\n\nThis function lives in the moderndive package, so we will need to load in this package (e.g., library(moderndive) if we want to use the get_regression_table() function."
  },
  {
    "objectID": "slides/week4-day1.html#section-7",
    "href": "slides/week4-day1.html#section-7",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Our focus (for now…)"
  },
  {
    "objectID": "slides/week4-day1.html#section-8",
    "href": "slides/week4-day1.html#section-8",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Estimated regression equation\n\n\\[\\widehat{y} = b_0 + b_1 \\cdot x\\]\n\n\n\n\n\nweeks_lm &lt;- lm(weight ~ weeks, \n               data = births_post28)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n-5.003\n0.582\n-8.603\n0\n-6.144\n-3.862\n\n\nweeks\n0.316\n0.015\n21.010\n0\n0.287\n0.346\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite out the estimated regression equation!"
  },
  {
    "objectID": "slides/week4-day1.html#section-9",
    "href": "slides/week4-day1.html#section-9",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "How do you interpret the intercept value of -5.003?\n\n\n\n\n\n\nHow do you interpret the slope value of 0.316?"
  },
  {
    "objectID": "slides/week4-day1.html#section-10",
    "href": "slides/week4-day1.html#section-10",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Obtaining Residuals\n\n\n\\(\\widehat{weight} = -5.003+0.316 \\cdot weeks\\)\n\n\nWhat would the residual be for a pregnancy that lasted 39 weeks and whose baby weighed 7.63 pounds?"
  },
  {
    "objectID": "slides/week4-day1.html#step-1-finding-distinct-levels",
    "href": "slides/week4-day1.html#step-1-finding-distinct-levels",
    "title": "Introduction to Linear Regression",
    "section": "Step 1: Finding distinct levels",
    "text": "Step 1: Finding distinct levels\n\ndistinct(births_post28, habit)\n\n# A tibble: 2 × 1\n  habit    \n  &lt;fct&gt;    \n1 nonsmoker\n2 smoker"
  },
  {
    "objectID": "slides/week4-day1.html#section-11",
    "href": "slides/week4-day1.html#section-11",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Step 2: Fit a linear regression\n\n\nhabit_lm &lt;- lm(weight ~ habit,\n               data = births_post28)\n\n\n\n\nStep 3: Obtain coefficient table\n\n\nget_regression_table(habit_lm)\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n7.246\n0.046\n158.369\n0.000\n7.157\n7.336\n\n\nhabit: smoker\n-0.418\n0.128\n-3.270\n0.001\n-0.668\n-0.167\n\n\n\n\n\n\n\n\n\n\n\n🤔"
  },
  {
    "objectID": "slides/week4-day1.html#estimated-regression-equation",
    "href": "slides/week4-day1.html#estimated-regression-equation",
    "title": "Introduction to Linear Regression",
    "section": "Estimated Regression Equation",
    "text": "Estimated Regression Equation\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n7.246\n0.046\n158.369\n0.000\n7.157\n7.336\n\n\nhabit: smoker\n-0.418\n0.128\n-3.270\n0.001\n-0.668\n-0.167\n\n\n\n\n\n\n\n\n\n\\[\\widehat{weight} = 7.23 - 0.4 \\cdot Smoker\\]\n\n\n\nBut what does \\(Smoker\\) represent???"
  },
  {
    "objectID": "slides/week4-day1.html#section-12",
    "href": "slides/week4-day1.html#section-12",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Indicator Variables\n\n\n\\[\n  \\widehat{y} = b_0 + b_1 \\cdot x\n\\]\n\n\n\n\n\\(x\\) is a categorical variable with levels:\n\n\"nonsmoker\"\n\"smoker\"\n\n\n\n\nWe need to convert to:\n\na “baseline” group\n“offsets” / adjustments to the baseline"
  },
  {
    "objectID": "slides/week4-day1.html#a-more-intuitive-equation",
    "href": "slides/week4-day1.html#a-more-intuitive-equation",
    "title": "Introduction to Linear Regression",
    "section": "A More Intuitive Equation",
    "text": "A More Intuitive Equation\n\\[\\widehat{weight} = 7.23 - 0.4 \\cdot 1_{smoker}(x)\\]\nwhere\n\n\\(1_{smoker}(x) = 1\\) if the mother was a \"smoker\"\n\\(1_{smoker}(x) = 0\\) if the mother was a \"nonsmoker\""
  },
  {
    "objectID": "slides/week4-day1.html#obtaining-group-means",
    "href": "slides/week4-day1.html#obtaining-group-means",
    "title": "Introduction to Linear Regression",
    "section": "Obtaining Group Means",
    "text": "Obtaining Group Means\n\\[\\widehat{weight} = 7.23 - 0.4 \\cdot 1_{Smoker}(x)\\] \nGiven the equation, what is the estimated mean birth weight for nonsmoking mothers?\n\nFor smoking mothers?"
  },
  {
    "objectID": "slides/week4-day1.html#statistical-critique",
    "href": "slides/week4-day1.html#statistical-critique",
    "title": "Introduction to Linear Regression",
    "section": "Statistical Critique",
    "text": "Statistical Critique\n\n\nGet out the article you picked for Statistics in Your Field (Week 1)\nChoose a visualization from the article\n\n\n\n\n\n\n\n\nUse a table if there are no visualizations\n\n\n\n\n\n\n\n\nWhere are variables being used in the visualization / table? Those are the aesthetics!\nWrite a critique of the good / bad aspects of the plot\n\nRepeat with a “pop” visualization from the New York Times."
  },
  {
    "objectID": "slides/week4-day1.html#midterm-project-proposal",
    "href": "slides/week4-day1.html#midterm-project-proposal",
    "title": "Introduction to Linear Regression",
    "section": "Midterm Project Proposal",
    "text": "Midterm Project Proposal\n\n\nChoose a dataset\nChoose a numerical response variable\nChoose a numerical explanatory variable\nChoose a second explanatory variable, either numerical or categorical\n\n\n\n\n\n\n\n\nChecking values of your numerical variable(s)\n\n\n\nYour numerical variable cannot have a small (e.g., 2 or 3) number of values. You can use the distinct() function to determine the unique values of your variable. For example, by running distinct(hbr_maples, year) I would discover that year only has two values (2003 and 2004), meaning year is not eligible to be a numerical response or explanatory variable. It could, however, be a categorical explanatory variable!\n\n\n\n\n\n\nWrite your Introduction"
  },
  {
    "objectID": "slides/week10-day2.html#findings",
    "href": "slides/week10-day2.html#findings",
    "title": "STAT 313 Last Day",
    "section": "Findings",
    "text": "Findings\nThe results of each hypothesis test go directly below the test.\n\n\n\nTheory-based Methods\nYour decision & conclusion for your hypothesis test go directly below your ANOVA table.\n\n\n\nSimulation-based Methods\nYour decision & conclusion for your hypothesis test go directly below your permutation distribution and p-value."
  },
  {
    "objectID": "slides/week10-day2.html#hypothesis-test-conclusions",
    "href": "slides/week10-day2.html#hypothesis-test-conclusions",
    "title": "STAT 313 Last Day",
    "section": "Hypothesis Test Conclusions",
    "text": "Hypothesis Test Conclusions\n\nConclusions should be written in terms of the alternative hypothesis\n\n\n\n\nDid you reject the null hypothesis?\nThen you have evidence that at least one group has a different mean!\n\n\n\nDid you fail to reject the null hypothesis?\nThen you have insufficient evidence that at least one group has a different mean!"
  },
  {
    "objectID": "slides/week10-day2.html#model-validity",
    "href": "slides/week10-day2.html#model-validity",
    "title": "STAT 313 Last Day",
    "section": "Model Validity",
    "text": "Model Validity\n\nIn this section you discuss the reliability of the p-values you obtained based on the model conditions.\n\n\n\nIndependence\n\nwithin groups\nbetween groups\n\nNormality of the distributions for each group\nEqual variance of the distributions for each group\n\n\n\n\n\n\n\n\nConditions for Each Test\n\n\nEach one-way ANOVA test considers different groups. So, your conditions should be evaluated for each test separately."
  },
  {
    "objectID": "slides/week10-day2.html#conditions-are-never-met",
    "href": "slides/week10-day2.html#conditions-are-never-met",
    "title": "STAT 313 Last Day",
    "section": "Conditions are never met!",
    "text": "Conditions are never met!\n\\(H_0\\): the condition is met\n\\(H_A\\): the condition is violated\n\n\nJust like we never say “I accept the null hypothesis,” we never say a condition is “met.” Instead, we say there is no evidence that the condition is violated."
  },
  {
    "objectID": "slides/week10-day2.html#study-limitations",
    "href": "slides/week10-day2.html#study-limitations",
    "title": "STAT 313 Last Day",
    "section": "Study Limitations",
    "text": "Study Limitations\nThis section summarizes your understanding of the foundational aspects of experimental design.\n\n\n\n\nBased on the sampling method used, what larger population can you infer the results or your analysis onto?\n\n\nWhat were the inclusion criteria of the observations?\nHow does that influence the population you can infer your findings onto?\n\n\n\n\n\n\n\nBased on the design of the study, what type of statements can be made about the relationship between the explanatory and response variables?\n\n\nWere the explanatory variables randomly assigned to control for confounding variables?\n\nHow does that influence what you can and cannot say about the relationships between the variables?"
  },
  {
    "objectID": "slides/week10-day2.html#overall-conclusions",
    "href": "slides/week10-day2.html#overall-conclusions",
    "title": "STAT 313 Last Day",
    "section": "Overall Conclusions",
    "text": "Overall Conclusions\n\n\nBased on the results of your analysis what is your conclusion for the questions of interest? Connect your conclusion(s) to the relationships you saw in the visualizations you made and the results of your hypothesis tests.\n\n\n\n\n\n\n\nDid you distributions look similar but your hypothesis test said at least one group was different?\nThink about how sample size effects p-values!\n\n\n\n\n\n\nDid you reject the null hypothesis for your one-way ANOVA?\nLook back at your visualizations – which group(s) look the most different?\n\n\n\n\n\n\n\nDid you fail to reject the null hypothesis for your one-way ANOVA?\nLook back at your visualizations – do all of the groups look the same?"
  },
  {
    "objectID": "slides/week10-day2.html#do-you-have-really-skewed-data",
    "href": "slides/week10-day2.html#do-you-have-really-skewed-data",
    "title": "STAT 313 Last Day",
    "section": "Do you have really skewed data?",
    "text": "Do you have really skewed data?"
  },
  {
    "objectID": "slides/week10-day2.html#try-using-a-log-transformation",
    "href": "slides/week10-day2.html#try-using-a-log-transformation",
    "title": "STAT 313 Last Day",
    "section": "Try using a log transformation!",
    "text": "Try using a log transformation!\n\n\nUn-transformed Variances\n\n\n\n\n\n\n\n\nunittype\nvar\n\n\n\n\nC\n84.826493\n\n\nI\n80.783024\n\n\nIP\n5.729663\n\n\nP\n129.138547\n\n\nR\n59.096925\n\n\nS\n49.280157\n\n\nSC\n49.923399\n\n\nNA\n112.284569\n\n\n\n\n\n\n\n\n\nLog Transformed Variances\n\n\n\n\n\n\n\n\nunittype\nvar\n\n\n\n\nC\n1.6978424\n\n\nI\n0.8788463\n\n\nIP\n0.8990321\n\n\nP\n1.5659500\n\n\nR\n1.3621461\n\n\nS\n2.1770549\n\n\nSC\n1.6514917\n\n\nNA\n0.7591942\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think? Did it work?"
  },
  {
    "objectID": "slides/week10-day2.html#presentation-structure",
    "href": "slides/week10-day2.html#presentation-structure",
    "title": "STAT 313 Last Day",
    "section": "Presentation Structure",
    "text": "Presentation Structure\n\nYou will give a 3-minute presentation on one aspect of your final project you found the most interesting. Notice, you need to pick one aspect, since your presentation is so short.\n\n\nHere are some examples of what you could choose:\n\nThe relationships you saw in the visualizations\nThe design of the study\nThe model you found best represents the relationships between variables you selected"
  },
  {
    "objectID": "slides/week10-day2.html#presentation-slides",
    "href": "slides/week10-day2.html#presentation-slides",
    "title": "STAT 313 Last Day",
    "section": "Presentation Slides",
    "text": "Presentation Slides\nFor your presentation you are allowed to make two slides:\n\nA title slide (make it fun!) with your name\nA content slide\n\nYour slides must be submitted as a PDF.\n\n\n\n\n\n\nDeadline for slides\n\n\nSlides are due by 5pm the night before your final exam timeslot. If you do not submit slides by the deadline, you will not be allowed to present."
  },
  {
    "objectID": "slides/week10-day2.html#i-hope-you-leave-this-class-understanding",
    "href": "slides/week10-day2.html#i-hope-you-leave-this-class-understanding",
    "title": "STAT 313 Last Day",
    "section": "I hope you leave this class understanding…",
    "text": "I hope you leave this class understanding…\n\nReproducibility is a foundational aspect to scientific research.\nData visualizations tell you a story, where statistical tests only tell you a summary.\nMultiple regression and ANOVA are powerful tools to explore multivariate relationships.\nA well thought out study is more powerful than any statistical analysis."
  },
  {
    "objectID": "slides/week10-day2.html#the-discipline-of-statistics",
    "href": "slides/week10-day2.html#the-discipline-of-statistics",
    "title": "STAT 313 Last Day",
    "section": "The Discipline of Statistics",
    "text": "The Discipline of Statistics\nThe field of Statistics was developed to evaluate evidence obtained from data. Over the last century, the use of statistics has become embedded as a component of the scientific process for many disciplines.\n\n\n\n“Significance, the new s-word, is overused and underdefined in the realm of connecting statistical results to the underlying science.” (Higgs, 2013)\n\n\n\n\n\n\n“I advocate a simple solution: Replace the s-word with words describing what you actually mean by it.”"
  },
  {
    "objectID": "slides/week10-day2.html#foundational-ideas-taught-in-statistics-courses-were-invented-by",
    "href": "slides/week10-day2.html#foundational-ideas-taught-in-statistics-courses-were-invented-by",
    "title": "STAT 313 Last Day",
    "section": "Foundational ideas taught in statistics courses were invented by:",
    "text": "Foundational ideas taught in statistics courses were invented by:\n\nFrancis Galton\nKarl Pearson\nRonald Fisher"
  },
  {
    "objectID": "slides/week10-day2.html#section",
    "href": "slides/week10-day2.html#section",
    "title": "STAT 313 Last Day",
    "section": "",
    "text": "Remember to give yourself praise!"
  },
  {
    "objectID": "slides/week2-day2.html#revisions",
    "href": "slides/week2-day2.html#revisions",
    "title": "The Flaws of Averages",
    "section": "Revisions",
    "text": "Revisions\n\n\n\n“Complete” = Satisfactory\n\n\nYour images were included in the document\nYou provided responses to every question\n\n\n\n\n\n“Incomplete” = Growing\n\n\nYour images were not included in the document\nYou did not provide responses to every question"
  },
  {
    "objectID": "slides/week2-day2.html#key---code-chunk-options",
    "href": "slides/week2-day2.html#key---code-chunk-options",
    "title": "The Flaws of Averages",
    "section": "Key - Code Chunk Options",
    "text": "Key - Code Chunk Options\nA code chunk option is declared after a #|. Here are some options we may want to use:\n\n\n#| label: packages – creates a label for the code chunk (describing its contents)\n#| echo: false – tells Quarto not to output the code in the rendered HTML (only the output)\n#| include: false – tells Quarto not to include the code or the output in the rendered HTML"
  },
  {
    "objectID": "slides/week2-day2.html#key---previewing-your-data",
    "href": "slides/week2-day2.html#key---previewing-your-data",
    "title": "The Flaws of Averages",
    "section": "Key - Previewing Your Data",
    "text": "Key - Previewing Your Data\nThe glimpse() function is a great tool to preview the dataset you are working with! It gives you:\n\nthe dimensions of the data (rows and columns)\nthe names of the columns\nthe data type of each column (e.g., chr, dbl)\na preview of the first 10 rows of each column"
  },
  {
    "objectID": "slides/week2-day2.html#key---plotting-your-data",
    "href": "slides/week2-day2.html#key---plotting-your-data",
    "title": "The Flaws of Averages",
    "section": "Key - Plotting Your Data",
    "text": "Key - Plotting Your Data\nNow that we’ve practiced making some plots, we know…\n\nmapping = aes(y = manufacturer, x = hwy) declares what variables are plotted on the x- and y-axis.\n\n\n\n\n\n\nTip\n\n\nThe variable names you put insides aes() must be identical to the names of the variables in the dataset!\n\n\n\n\n\n\n\nlabs(x = \"Highway Miles Per Gallon\", y = \"Car Manufacturer\") declares new x- and y-axis labels for the plot.\n\n\n\n\n\n\nTip\n\n\nIncluding nice axis labels (with their units) is a critical part of every visualization we make!"
  },
  {
    "objectID": "slides/week2-day2.html#completing-revisions",
    "href": "slides/week2-day2.html#completing-revisions",
    "title": "The Flaws of Averages",
    "section": "Completing Revisions",
    "text": "Completing Revisions\n\nLab 1 revisions are due by Wednesday, April 17 (at midnight).\n\n\n\nRead comments on Canvas\nGo back into your Lab 1 on Posit Cloud and complete your revisions\nRender your revised Lab 1\nDownload your revised HTML\n\n\n\n\n\n\n\n\n\nReflections\n\n\nRevisions are required to be accompanied with reflections on what you learned while completing your revisions. These can be written in your Lab 1 Quarto file (next to the problems you revised), in a Word document, or in the comment box on Canvas."
  },
  {
    "objectID": "slides/week2-day2.html#section",
    "href": "slides/week2-day2.html#section",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "15-minutes\n\n\n\nReview Lab 1 comments\nAsk questions\nStart revisions"
  },
  {
    "objectID": "slides/week2-day2.html#section-1",
    "href": "slides/week2-day2.html#section-1",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Suppose…\n\n\n“Overall this instructor was educationally effective.”\n\n\n\n\n\n\n\nyear\nquarter\naverage\n\n\n\n\n2021\nFall\n4.53\n\n\n2021\nFall\n4.36\n\n\n2022\nWinter\n4.18\n\n\n2022\nWinter\n4.24\n\n\n2022\nSpring\n4.83\n\n\n2022\nSpring\n4.41\n\n\n2022\nSpring\n4.00"
  },
  {
    "objectID": "slides/week2-day2.html#section-2",
    "href": "slides/week2-day2.html#section-2",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "How were those averages calculated?"
  },
  {
    "objectID": "slides/week2-day2.html#section-3",
    "href": "slides/week2-day2.html#section-3",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "What do these averages mean?"
  },
  {
    "objectID": "slides/week2-day2.html#section-4",
    "href": "slides/week2-day2.html#section-4",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "The Problem\n\n\nIt’s incredibly rare for scientists, including statisticians, to explicitly think about that conditions underlying their models.\n\n\n\n“I’ve had many conversations in very different contexts with scientists about what the average calculated from the data (or mean in a model) could reasonably represent and whether that was really what the scientist was after.” Dr. Megan Higgs"
  },
  {
    "objectID": "slides/week2-day2.html#section-5",
    "href": "slides/week2-day2.html#section-5",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Why so much resistance?\n\n\n\n\nDepartments hold specific expectations of statistics courses\n\n\nThese expectations are conditional on the assumption that means represent the magic quantity of interest.\n\n\n\nI’m then expected to educate you to “play the game” in the scientific culture of averages"
  },
  {
    "objectID": "slides/week2-day2.html#section-6",
    "href": "slides/week2-day2.html#section-6",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Averagarianism\n\n\n\n“The primary research method of averagarianism is aggregate, then analyze: First, combine many people together and look for patterns in the group. Then, use these group patterns (such as averages and other statistics) to analyze and model individuals. The science of the individual instead instructs scientists to analyze, then aggregate: First, look for pattern within each individual. Then, look for ways to combine these individual patterns into collective insight.”\nThe End of Average by Todd Rose"
  },
  {
    "objectID": "slides/week2-day2.html#section-7",
    "href": "slides/week2-day2.html#section-7",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "“We’ve always done it this way”\n\n\nMethods based on averages are available, easy, convenient, and take little creativity — and they are expected in our scientific culture.\n\n\nJustification for using averages is simply not demanded — though justification for use of anything but averages is incredibly difficult to sell."
  },
  {
    "objectID": "slides/week2-day2.html#section-8",
    "href": "slides/week2-day2.html#section-8",
    "title": "The Flaws of Averages",
    "section": "",
    "text": "Some Rules to Play By\n\n\n\n\n\nLook at and understand your raw data before aggregating\nBoxplots don’t count as visualizing the raw data"
  },
  {
    "objectID": "slides/week2-day2.html#departure-delays",
    "href": "slides/week2-day2.html#departure-delays",
    "title": "The Flaws of Averages",
    "section": "Departure Delays",
    "text": "Departure Delays\n\nInspect the nycflights dataset\nVisualize departure delays\nPlay with histogram binwidth\nfilter() data to include only certain flights\ncalculate() summary statistics\nMake decisions based on summary statistics\nCompare summary statistics to a visualization"
  },
  {
    "objectID": "slides/week9-day1.html#final-project-first-draft",
    "href": "slides/week9-day1.html#final-project-first-draft",
    "title": "ANalysis Of VAriance",
    "section": "Final Project First Draft",
    "text": "Final Project First Draft\nStep 1 - Due by Wednesday\n\n\nIntroduction (Data description, research questions)\n\n\n\nStep 2 - Due by Friday\n\n\nMethods (Data visualizations)\nFindings (Data analysis)\n\n\n\n\nStep 3 - Due by Sunday\n\n\nStudy Limitations\nConclusion"
  },
  {
    "objectID": "slides/week9-day1.html#section",
    "href": "slides/week9-day1.html#section",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Before…"
  },
  {
    "objectID": "slides/week9-day1.html#section-1",
    "href": "slides/week9-day1.html#section-1",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Now…"
  },
  {
    "objectID": "slides/week9-day1.html#section-2",
    "href": "slides/week9-day1.html#section-2",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Goal of an ANOVA\n\nAnalysis of variance (ANOVA) compares the means of three of more groups to detect if the means of the groups are different."
  },
  {
    "objectID": "slides/week9-day1.html#section-3",
    "href": "slides/week9-day1.html#section-3",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "How???\n\n\nCompare how different a group of means are\nScale the differences relative to the variability of the groups\nSummarize the differences with one number"
  },
  {
    "objectID": "slides/week9-day1.html#section-4",
    "href": "slides/week9-day1.html#section-4",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Visualizing Group Differences\n\n\nWe want visualizations that allow for us to easily compare:\n\nthe center (mean) of the groups\nthe spread (variability) of the groups"
  },
  {
    "objectID": "slides/week9-day1.html#section-5",
    "href": "slides/week9-day1.html#section-5",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "What can you say about the differences between the age groups?\n\n\n\nWhat can you say about the variability within the age groups?"
  },
  {
    "objectID": "slides/week9-day1.html#section-6",
    "href": "slides/week9-day1.html#section-6",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 1: Compare your groups"
  },
  {
    "objectID": "slides/week9-day1.html#section-7",
    "href": "slides/week9-day1.html#section-7",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 2: Find the overall mean\n\n\n\n\nThis ignores the groups and finds one mean for every observation!"
  },
  {
    "objectID": "slides/week9-day1.html#section-8",
    "href": "slides/week9-day1.html#section-8",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 3: Find the group means"
  },
  {
    "objectID": "slides/week9-day1.html#section-9",
    "href": "slides/week9-day1.html#section-9",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 4: Calculate the sum of squares"
  },
  {
    "objectID": "slides/week9-day1.html#section-10",
    "href": "slides/week9-day1.html#section-10",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 5: Calculate the F-statistic"
  },
  {
    "objectID": "slides/week9-day1.html#section-11",
    "href": "slides/week9-day1.html#section-11",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Step 6: Find the p-value"
  },
  {
    "objectID": "slides/week9-day1.html#section-12",
    "href": "slides/week9-day1.html#section-12",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "F-distribution\n\nAn \\(F\\)-distribution is a variant of the \\(t\\)-distribution, and is also defined by degrees of freedom.\n\n\nThis distribution is defined by two different degrees of freedom:\n\nfrom the numerator (MSG) : \\(k - 1\\)\nfrom the denominator (MSE) : \\(n - k\\)"
  },
  {
    "objectID": "slides/week9-day1.html#section-13",
    "href": "slides/week9-day1.html#section-13",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Two degrees of freedom!\n\n\n\n\nChanging the numerator degrees of freedom\n\n\n\n\nChanging the denominator degrees of freedom"
  },
  {
    "objectID": "slides/week9-day1.html#section-14",
    "href": "slides/week9-day1.html#section-14",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Do you always use an F-distribution to get the p-value?\n\n\n\n\nNO!"
  },
  {
    "objectID": "slides/week9-day1.html#conditions-of-an-anova",
    "href": "slides/week9-day1.html#conditions-of-an-anova",
    "title": "ANalysis Of VAriance",
    "section": "Conditions of an ANOVA",
    "text": "Conditions of an ANOVA\n\nIndependence of observations\n\n\n\nObservations are independent within groups and between groups\n\n\n\n\nNormality of the residuals\n\n\n\nThe distribution of residuals for each group is approximately normal\n\n\n\n\n\nEqual variability of the groups\n\n\n\nThe spread of the distributions are similar across groups"
  },
  {
    "objectID": "slides/week9-day1.html#choosing-a-method",
    "href": "slides/week9-day1.html#choosing-a-method",
    "title": "ANalysis Of VAriance",
    "section": "Choosing a Method",
    "text": "Choosing a Method\n\n\n\n\nWhich condition(s) are required to use “theory-based” methods?\n\n\n\n\n\n\nWhich condition(s) are required to use “simulation-based” methods?"
  },
  {
    "objectID": "slides/week9-day1.html#section-15",
    "href": "slides/week9-day1.html#section-15",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "What do you think?"
  },
  {
    "objectID": "slides/week9-day1.html#step-1-calculating-the-observed-f-statistic",
    "href": "slides/week9-day1.html#step-1-calculating-the-observed-f-statistic",
    "title": "ANalysis Of VAriance",
    "section": "Step 1: Calculating the Observed F-statistic",
    "text": "Step 1: Calculating the Observed F-statistic\n\nobs_F &lt;- evals_small %&gt;% \n  specify(response = min_eval, \n          explanatory = age_cat) %&gt;% \n  calculate(stat = \"F\")\n\n\n\n\nResponse: min_eval (numeric)\nExplanatory: age_cat (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  1.41"
  },
  {
    "objectID": "slides/week9-day1.html#step-2-simulating-what-could-have-happened-under-h_0",
    "href": "slides/week9-day1.html#step-2-simulating-what-could-have-happened-under-h_0",
    "title": "ANalysis Of VAriance",
    "section": "Step 2: Simulating what could have happened under \\(H_0\\)",
    "text": "Step 2: Simulating what could have happened under \\(H_0\\)\n\n\n\nHow could we use cards to simulate what minimum evaluation score a professor would have gotten, if their score was independent from their age?"
  },
  {
    "objectID": "slides/week9-day1.html#another-permutation-distribution",
    "href": "slides/week9-day1.html#another-permutation-distribution",
    "title": "ANalysis Of VAriance",
    "section": "Another Permutation Distribution",
    "text": "Another Permutation Distribution\n\nnull_dist &lt;- evals_small %&gt;% \n  specify(response = min_eval, \n          explanatory = age_cat) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"F\")"
  },
  {
    "objectID": "slides/week9-day1.html#another-permutation-distribution-1",
    "href": "slides/week9-day1.html#another-permutation-distribution-1",
    "title": "ANalysis Of VAriance",
    "section": "Another Permutation Distribution",
    "text": "Another Permutation Distribution\n\n\n\nWhy doesn’t the distribution have negative numbers?"
  },
  {
    "objectID": "slides/week9-day1.html#visualizing-the-p-value",
    "href": "slides/week9-day1.html#visualizing-the-p-value",
    "title": "ANalysis Of VAriance",
    "section": "Visualizing the p-value",
    "text": "Visualizing the p-value\n\nvisualise(null_dist) + \n  shade_p_value(obs_stat = obs_F, \n                direction = \"greater\")"
  },
  {
    "objectID": "slides/week9-day1.html#visualizing-the-p-value-1",
    "href": "slides/week9-day1.html#visualizing-the-p-value-1",
    "title": "ANalysis Of VAriance",
    "section": "Visualizing the p-value",
    "text": "Visualizing the p-value"
  },
  {
    "objectID": "slides/week9-day1.html#making-a-decision-reaching-a-conclusion",
    "href": "slides/week9-day1.html#making-a-decision-reaching-a-conclusion",
    "title": "ANalysis Of VAriance",
    "section": "Making a Decision & Reaching a Conclusion",
    "text": "Making a Decision & Reaching a Conclusion\n\n\nFor a p-value of 0.254, what decision would you reach regarding your hypothesis test?\n\n\n\n\nWhat would you conclude regarding the mean minimum evaluation score for different age groups of faculty?"
  },
  {
    "objectID": "slides/week9-day1.html#theory-based-methods",
    "href": "slides/week9-day1.html#theory-based-methods",
    "title": "ANalysis Of VAriance",
    "section": "Theory-based Methods",
    "text": "Theory-based Methods\n\naov(min_eval ~ age_cat, \n    data = evals_small)  %&gt;% \n  tidy()\n\n\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 age_cat       3  1.24  0.414      1.41   0.244\n2 Residuals    90 26.4   0.293     NA     NA    \n\n\n\n\nHow was the statistic calculated?\n\n\nWhat distribution was used to calculate the p.value?"
  },
  {
    "objectID": "slides/week9-day1.html#making-a-decision-reaching-a-conclusion-1",
    "href": "slides/week9-day1.html#making-a-decision-reaching-a-conclusion-1",
    "title": "ANalysis Of VAriance",
    "section": "Making a Decision & Reaching a Conclusion",
    "text": "Making a Decision & Reaching a Conclusion\n\n\nFor a p-value of 0.244, what decision would you reach regarding your hypothesis test?\n\n\n\n\nWhat would you conclude regarding the mean minimum evaluation score for different age groups of faculty?"
  },
  {
    "objectID": "slides/week9-day1.html#section-16",
    "href": "slides/week9-day1.html#section-16",
    "title": "ANalysis Of VAriance",
    "section": "",
    "text": "Did the two methods yield different results?"
  },
  {
    "objectID": "slides/week9-day1.html#introduction-research-questions",
    "href": "slides/week9-day1.html#introduction-research-questions",
    "title": "ANalysis Of VAriance",
    "section": "Introduction & Research Questions",
    "text": "Introduction & Research Questions\n\n\nIn 4-6 sentences, introduce / describe your data.\n\nWhat is the context of the data?\nFor what purpose were these data collected?\n\n\n\n\n\nOutline the two questions your research seeks to address.\n\nYou will have one question for each one-way ANOVA model\nYour question should be in terms of “group means” not “relationships”"
  },
  {
    "objectID": "slides/week9-day1.html#removing-silencing-code-templates",
    "href": "slides/week9-day1.html#removing-silencing-code-templates",
    "title": "ANalysis Of VAriance",
    "section": "Removing / Silencing Code Templates",
    "text": "Removing / Silencing Code Templates\n\nYour final_project.qmd file contains code templates for evaluating the conditions of your ANOVA model (like you did in Lab 8).\nThese do not need to be used for your Introduction. So, you need to “turn off” these code chunks to turn in your Introduction!\n\n\n\n\nThere are two ways to do this:\n\nIn the YAML (at the top of your document) add eval: false to your execute options\nIn each code chunk (there are two) add eval: false to your code chunk options"
  },
  {
    "objectID": "slides/week9-day1.html#yaml-option",
    "href": "slides/week9-day1.html#yaml-option",
    "title": "ANalysis Of VAriance",
    "section": "YAML Option",
    "text": "YAML Option\n---\ntitle: \"Your Title Goes Here! Make it fun!\"\nauthor: \"Your Name Here!\"\nformat:\n  html:\n    code-tools: true\neditor: visual\nembed-resources: true\nexecute:\n  echo: false\n  message: false\n  warning: false\n  eval: false\n---"
  },
  {
    "objectID": "slides/week9-day1.html#code-chunk-option",
    "href": "slides/week9-day1.html#code-chunk-option",
    "title": "ANalysis Of VAriance",
    "section": "Code Chunk Option",
    "text": "Code Chunk Option\n#| label: distribution-of-residuals\n#| layout-nrow: 1\n#| eval: false\n#| echo: fenced\n\n# Residuals of first one-way ANOVA \nbroom::augment(my_model) %&gt;% \n  ggplot(mapping = aes(x = .resid)) +\n  geom_histogram() +\n  labs(x = \"Residual\")\n\n# Residuals of second one-way ANOVA \nbroom::augment(my_model) %&gt;% \n  ggplot(mapping = aes(x = .resid)) +\n  geom_histogram() +\n  labs(x = \"Residual\")"
  },
  {
    "objectID": "slides/week3-day1.html#section",
    "href": "slides/week3-day1.html#section",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "How would you describe a categorical variable?"
  },
  {
    "objectID": "slides/week3-day1.html#in-r",
    "href": "slides/week3-day1.html#in-r",
    "title": "Incorporating Categorical Variables",
    "section": "In R…",
    "text": "In R…\ncategorical variables can have either character or factor data types\n\n. . .\nfactor – structured & fixed number of levels / options\n\ncan be ordered or unordered\n\n\n. . .\ncharacter – unstructured & variable number of levels\n\nis inherently unordered"
  },
  {
    "objectID": "slides/week3-day1.html#section-1",
    "href": "slides/week3-day1.html#section-1",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "Fill in the associated data types (e.g. character, factor, integer, double) with each type of variable.\n\n\n\n\nVariable\nData Type in R\n\n\n\n\nCategorical variable\n\n\n\nContinuous numerical variable\n\n\n\nDiscrete numerical variable"
  },
  {
    "objectID": "slides/week3-day1.html#dplyr-a-tool-bag-for-data-wrangling",
    "href": "slides/week3-day1.html#dplyr-a-tool-bag-for-data-wrangling",
    "title": "Incorporating Categorical Variables",
    "section": "dplyr – a tool bag for data wrangling",
    "text": "dplyr – a tool bag for data wrangling\n\n\n\n\n\n\nfilter()\nselect()\nmutate()\nsummarize()\narrange()\ngroup_by()"
  },
  {
    "objectID": "slides/week3-day1.html#section-2",
    "href": "slides/week3-day1.html#section-2",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "Brainstorm definitions for each verb\n\n\n\nfilter()\nselect()\nmutate()\ngroup_by()\nsummarize()\narrange()"
  },
  {
    "objectID": "slides/week3-day1.html#section-3",
    "href": "slides/week3-day1.html#section-3",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "The Pipe %&gt;%"
  },
  {
    "objectID": "slides/week3-day1.html#section-4",
    "href": "slides/week3-day1.html#section-4",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "If you wanted means for each level of a categorical variable, what would you do?"
  },
  {
    "objectID": "slides/week3-day1.html#trout-size",
    "href": "slides/week3-day1.html#trout-size",
    "title": "Incorporating Categorical Variables",
    "section": "Trout Size",
    "text": "Trout Size\n\nThe HJ Andrews Experimental Forest houses one of the larges long-term ecological research stations, specifically researching cutthroat trout and salamanders in clear cut or old growth sections of Mack Creek.\n\n\n\ntrout %&gt;% \n  group_by(section) %&gt;% \n  summarize(\n    mean_length = mean(length_1_mm, \n                       na.rm = TRUE)\n            )\n\n# A tibble: 2 × 2\n  section                               mean_length\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 clear cut forest                             85.3\n2 upstream old growth coniferous forest        81.4\n\n\n\n. . .\n\n\n\nWhy na.rm = TRUE?"
  },
  {
    "objectID": "slides/week3-day1.html#classifying-channel-types",
    "href": "slides/week3-day1.html#classifying-channel-types",
    "title": "Incorporating Categorical Variables",
    "section": "Classifying Channel Types",
    "text": "Classifying Channel Types\n\nThe channels of the Mack Creek which were sampled were classified into the following groups:\n\n\n\n\"C\"\n\"I\"\n\"IP\"\n\"P\"\n\"R\"\n\"S\"\n\"SC\"\nNA\n\n\n\ncascade\nriffle\nisolated pool\npool\nrapid\nstep (small falls)\nside channel\nnot sampled by unit"
  },
  {
    "objectID": "slides/week3-day1.html#filter-ing-specific-channel-types",
    "href": "slides/week3-day1.html#filter-ing-specific-channel-types",
    "title": "Incorporating Categorical Variables",
    "section": "filter()-ing Specific Channel Types",
    "text": "filter()-ing Specific Channel Types\n\nThe majority of the Cutthroat trout were captured in cascades (C), pools (P), and side channels (SC). Suppose we want to only retain these levels of the unittype variable.\n\n. . .\n\nWhat would you do?\n\n. . .\n\n\n\ntrout %&gt;% \n  filter(\n    unittype %in% c(\"C\", \"P\", \"SC\")\n         )\n\n\n. . .\n\n\nWhy use %in% instead of ==?"
  },
  {
    "objectID": "slides/week3-day1.html#incorporating-categorical-variables-into-data-visualizations",
    "href": "slides/week3-day1.html#incorporating-categorical-variables-into-data-visualizations",
    "title": "Incorporating Categorical Variables",
    "section": "Incorporating Categorical Variables into Data Visualizations",
    "text": "Incorporating Categorical Variables into Data Visualizations\n\n\nAs a variable on the x- or y-axis\nAs a color / fill\nAs a facet"
  },
  {
    "objectID": "slides/week3-day1.html#salamander-size",
    "href": "slides/week3-day1.html#salamander-size",
    "title": "Incorporating Categorical Variables",
    "section": "Salamander Size",
    "text": "Salamander Size\n\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_2_mm)) + \n  geom_histogram(binwidth = 14) + \n  labs(x = \"Snout to Tail Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would this histogram look if there was no variation in salamander length?\n\n\n\nWhat are possible causes for the variation in salamander length?"
  },
  {
    "objectID": "slides/week3-day1.html#faceted-histograms",
    "href": "slides/week3-day1.html#faceted-histograms",
    "title": "Incorporating Categorical Variables",
    "section": "Faceted Histograms",
    "text": "Faceted Histograms\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_2_mm)) + \n  geom_histogram(binwidth = 14) + \n  facet_wrap(~ section, scales = \"free\") +\n  labs(x = \"Snout to Tail Length (mm)\")"
  },
  {
    "objectID": "slides/week3-day1.html#side-by-side-boxplots",
    "href": "slides/week3-day1.html#side-by-side-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                     y = species)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = salamander, \n       mapping = aes(y = length_1_mm, \n                     x = species)\n         ) + \n  geom_boxplot() + \n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/week3-day1.html#colors-in-boxplots",
    "href": "slides/week3-day1.html#colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Colors in Boxplots",
    "text": "Colors in Boxplots\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = unittype)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"\", \n       color = \"Channel Type\")"
  },
  {
    "objectID": "slides/week3-day1.html#facets-colors-in-boxplots",
    "href": "slides/week3-day1.html#facets-colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = section)\n         ) + \n  geom_boxplot() + \n  facet_wrap(~ unittype) + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"\", \n       color = \"Section in Mack Creek\")"
  },
  {
    "objectID": "slides/week3-day1.html#facets-colors-in-boxplots-output",
    "href": "slides/week3-day1.html#facets-colors-in-boxplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots"
  },
  {
    "objectID": "slides/week3-day1.html#facets-color-in-scatterplots",
    "href": "slides/week3-day1.html#facets-color-in-scatterplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = weight_g, \n                       color = section)\n         ) + \n  geom_point(alpha = 0.25) + \n  facet_wrap(~species) +\n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Weight (g)\", \n       color = \"Section in Mack Creek\")"
  },
  {
    "objectID": "slides/week3-day1.html#facets-color-in-scatterplots-output",
    "href": "slides/week3-day1.html#facets-color-in-scatterplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots"
  },
  {
    "objectID": "slides/week3-day1.html#your-turn-90-seconds",
    "href": "slides/week3-day1.html#your-turn-90-seconds",
    "title": "Incorporating Categorical Variables",
    "section": "Your Turn – 90 seconds",
    "text": "Your Turn – 90 seconds\n\n\n\n\n\n\n\n\n\n. . .\nWhat are the aesthetics included in this plot?\n. . .\nWhat is one aspect of this plot that you believe is well done? What is one aspect of the plot that could be improved?"
  },
  {
    "objectID": "slides/week3-day1.html#your-tasks",
    "href": "slides/week3-day1.html#your-tasks",
    "title": "Incorporating Categorical Variables",
    "section": "Your Tasks",
    "text": "Your Tasks\n\nFind your visualizations\n\n\nVisualization from your Week 1 article (Statistics in Your Field)\nVisualization from the New York Times\n\n\nDiscuss the aesthetics included in the plots\nDiscuss what you believe the plot does well\nDiscuss what improvements could be made to make the plot more clear"
  },
  {
    "objectID": "slides/week8-day2.html#common-mistakes",
    "href": "slides/week8-day2.html#common-mistakes",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "Common Mistakes",
    "text": "Common Mistakes\nQuestion 8 (Interpret Confidence Interval):\n\nDid not include population in interpretation\nStated interval was about crabs not marshes\n\n\nQuestion 11 (Bootstrap Assumptions):\n\nDid not talk about the sample of marshes\nStated population was crabs"
  },
  {
    "objectID": "slides/week8-day2.html#first-steps",
    "href": "slides/week8-day2.html#first-steps",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "First Steps",
    "text": "First Steps\n\nOpen the directions!\nCopy the Statistical Critique template on Posit Cloud\nCopy-and-paste code from your Midterm Project\nCopy-and-paste your justification for why you chose the model you chose"
  },
  {
    "objectID": "slides/week8-day2.html#next-steps",
    "href": "slides/week8-day2.html#next-steps",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "Next Steps",
    "text": "Next Steps\n\nFit the most complex model\nObtain an ANOVA table of your model\nUse the p-values in the ANOVA table to decide what model is best"
  },
  {
    "objectID": "slides/week8-day2.html#what-is-the-most-complex-model",
    "href": "slides/week8-day2.html#what-is-the-most-complex-model",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "What is the “most complex” model?",
    "text": "What is the “most complex” model?\nFor 1 numerical & 1 categorical explanatory variables\n\nFit the different slopes (interaction) model\n\n\nspecies_lm &lt;- lm(bill_length_mm ~ bill_depth_mm * species, \n                  data = penguins)\n\n\n\nFor 2 numerical explanatory variables\n\nFit the model with both variables included:\n\n\nbill_lm &lt;- lm(bill_length_mm ~ body_mass_g + bill_depth_mm, \n                  data = penguins)"
  },
  {
    "objectID": "slides/week8-day2.html#how-do-i-know-what-model-is-best-from-the-anova-table",
    "href": "slides/week8-day2.html#how-do-i-know-what-model-is-best-from-the-anova-table",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "How do I know what model is “best” from the ANOVA table?",
    "text": "How do I know what model is “best” from the ANOVA table?\nFor 1 numerical & 1 categorical explanatory variables\n\nLook at the interaction line (e.g., bill_depth_mm:species), it is testing if the slopes are different!\n\n\nanova(species_lm)\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nbill_depth_mm\n1\n561.5708\n561.570850\n93.96508\n9.384795e-20\n\n\nspecies\n2\n7460.3201\n3730.160057\n624.15060\n7.116342e-114\n\n\nbill_depth_mm:species\n2\n134.2515\n67.125732\n11.23184\n1.897629e-05\n\n\nResiduals\n336\n2008.0631\n5.976378\nNA\nNA"
  },
  {
    "objectID": "slides/week8-day2.html#section",
    "href": "slides/week8-day2.html#section",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "bill_depth_mm:species: With a p-value of 0.0000189, I would conclude there is evidence that the slopes are different!"
  },
  {
    "objectID": "slides/week8-day2.html#how-do-i-know-what-model-is-best-from-the-anova-table-1",
    "href": "slides/week8-day2.html#how-do-i-know-what-model-is-best-from-the-anova-table-1",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "How do I know what model is “best” from the ANOVA table?",
    "text": "How do I know what model is “best” from the ANOVA table?\nFor 2 numerical explanatory variables\n\nLook at the p-value for each variable, it is testing if that variable has a relationship with the response (conditional on the other variable being in the model)!\n\n\nanova(bill_lm)\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nbody_mass_g\n1\n3599.71136\n3599.71136\n186.673943\n3.685224e-34\n\n\nbill_depth_mm\n1\n27.41605\n27.41605\n1.421742\n2.339508e-01\n\n\nResiduals\n339\n6537.07813\n19.28342\nNA\nNA"
  },
  {
    "objectID": "slides/week8-day2.html#section-1",
    "href": "slides/week8-day2.html#section-1",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "body_mass_g: With a p-value of approximately 0, I would conclude there is a relationship between body mass and bill length (after accounting for bill depth)!\n\n\n\nbill_depth_mm: With a p-value of 0.234, I would conclude there is not a relationship between bill depth and bill length (after accounting for body mass)!"
  },
  {
    "objectID": "slides/week8-day2.html#section-2",
    "href": "slides/week8-day2.html#section-2",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "We carried out a hypothesis test!\n\n\n\n\\[H_0: \\beta_1 = 0\\]\n\\[H_A: \\beta_1 \\neq 0\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do these hypotheses mean in words?"
  },
  {
    "objectID": "slides/week8-day2.html#section-3",
    "href": "slides/week8-day2.html#section-3",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "By creating a permutation distribution!\n\n\nnull_dist &lt;- evals %&gt;% \n  specify(response = score, \n          explanatory = bty_avg) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "slides/week8-day2.html#section-4",
    "href": "slides/week8-day2.html#section-4",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "And visualizing where our observed statistic fell on the distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat would you estimate the p-value to be?"
  },
  {
    "objectID": "slides/week8-day2.html#section-5",
    "href": "slides/week8-day2.html#section-5",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "And calculated the p-value\n\n\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n\nWhat would you decide for this hypothesis test? What conclusion would you reach about these hypotheses?"
  },
  {
    "objectID": "slides/week8-day2.html#section-6",
    "href": "slides/week8-day2.html#section-6",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "How would this process have changed if we used theory-based methods instead?"
  },
  {
    "objectID": "slides/week8-day2.html#section-7",
    "href": "slides/week8-day2.html#section-7",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Approximating the permutation distribution\n\n\n\nA \\(t\\)-distribution can be a reasonable approximation for the permutation distribution if certain conditions are not violated."
  },
  {
    "objectID": "slides/week8-day2.html#section-8",
    "href": "slides/week8-day2.html#section-8",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What about the observed statistic?\n\n\nBeforeNow\n\n\n\nobs_slope &lt;- evals %&gt;% \n  specify(response = score, \n          explanatory = bty_avg) %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n   bty_avg \n0.06663704 \n\n\n\n\n\nevals_lm &lt;- lm(score ~ bty_avg,\n               data = evals)\n\nget_regression_table(evals_lm)\n\n\n\n# A tibble: 1 × 1\n  statistic\n      &lt;dbl&gt;\n1      4.09"
  },
  {
    "objectID": "slides/week8-day2.html#section-9",
    "href": "slides/week8-day2.html#section-9",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "How did R calculate the \\(t\\)-statistic?\n\n\nStep 1: SEStep 2: t-statisticProof!\n\n\n\n\n\\(SE_{b_1} = \\frac{\\frac{s_y}{s_x} \\cdot \\sqrt{1 - r^2}}{\\sqrt{n - 2}}\\)\n\n\n\n\n\n[1] 0.01495204\n\n\n\n\n\n\n\\(t = \\frac{b_1}{SE_{b_1}} = \\frac{0.067}{0.014952} = 4.4809947\\)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n3.880\n0.076\n50.961\n0\n3.731\n4.030\n\n\nbty_avg\n0.067\n0.016\n4.090\n0\n0.035\n0.099"
  },
  {
    "objectID": "slides/week8-day2.html#section-10",
    "href": "slides/week8-day2.html#section-10",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "How does R calculate the p-value?"
  },
  {
    "objectID": "slides/week8-day2.html#section-11",
    "href": "slides/week8-day2.html#section-11",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Did we get similar results between these methods?"
  },
  {
    "objectID": "slides/week8-day2.html#section-12",
    "href": "slides/week8-day2.html#section-12",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Why not always use theoretical methods?\n\n\n\n\nTheory-based methods only hold if the sampling distribution is normally shaped.\n\n\n\n\nThe normality of a sampling distribution depends heavily on model conditions."
  },
  {
    "objectID": "slides/week8-day2.html#section-13",
    "href": "slides/week8-day2.html#section-13",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What are these “conditions”?\n\n\n\n\nFor linear regression we are assuming…\n\n\n\nLinear relationship between \\(x\\) and \\(y\\)\n\nIndepdent observations\n\nNormality of residuals\n\nEqual variance of residuals"
  },
  {
    "objectID": "slides/week8-day2.html#section-14",
    "href": "slides/week8-day2.html#section-14",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Linear relationship between \\(x\\) and \\(y\\)\n\n\n\nWhat should we do?"
  },
  {
    "objectID": "slides/week8-day2.html#section-15",
    "href": "slides/week8-day2.html#section-15",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Variable transformation!"
  },
  {
    "objectID": "slides/week8-day2.html#section-16",
    "href": "slides/week8-day2.html#section-16",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Independence of observations\n\n\n\nThe evals dataset contains 463 observations on 94 professors. Meaning, professors have multiple observations.\n\nWhat can we do?\n\n\n\n\nBest – use a random effects model\nReasonable – collapse the multiple scores into a single score"
  },
  {
    "objectID": "slides/week8-day2.html#section-17",
    "href": "slides/week8-day2.html#section-17",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Normality of residuals\n\n\n\nWhat should we do?"
  },
  {
    "objectID": "slides/week8-day2.html#section-18",
    "href": "slides/week8-day2.html#section-18",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Variable transformation!"
  },
  {
    "objectID": "slides/week8-day2.html#section-19",
    "href": "slides/week8-day2.html#section-19",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Equal variance of residuals\n\n\n\nWhat should we do?"
  },
  {
    "objectID": "slides/week8-day2.html#section-20",
    "href": "slides/week8-day2.html#section-20",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Variable transformation!"
  },
  {
    "objectID": "slides/week8-day2.html#section-21",
    "href": "slides/week8-day2.html#section-21",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "Are these conditions required for both methods?\n\n\n\n\n\nSimulation-based Methods\n\n\n\nLinearity of Relationship\nIndependence of Observations\nEqual Variance of Residuals\n\n\n\n\n\n\nTheory-based Methods\n\n\n\nLinearity of Relationship\nIndependence of Observations\n\n\n\n\nNormality of Residuals\n\n\n\n\nEqual Variance of Residuals"
  },
  {
    "objectID": "slides/week8-day2.html#section-22",
    "href": "slides/week8-day2.html#section-22",
    "title": "🔬 Simulation-Based Methods versus Theory-Based Methods",
    "section": "",
    "text": "What happens if the conditions are violated?\n\n\nIn general, when the conditions associated with these methods are violated, the permutation and \\(t\\)-distributions will underestimate the true standard error of the sampling distribution.\n\n\n\nOur p-values will be too small!\nOur confidence intervals will be too narrow!\nWe will make more Type I errors than we expect!"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course Instructor",
    "section": "",
    "text": "Dr. Allison Theobold (she / they) is an Assistant Professor of Statistics at Cal Poly in beautiful San Luis Obispo, California. Allison received her doctorate in Statistics, with an emphasis in Statistics Education, from Montana State University. Allison’s work focuses on innovation in statistics and data science education, with an emphasis on equitable pedagogy and learning trajectories. Allison is also interested in exploring pedagogical approaches for enhancing retention of under-represented students in STEM, including creating inclusive discoursive spaces and equitable group collaborations.",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "",
    "text": "A deep dive into linear models for analyzing multivariate data sets. In this course, you will learn techniques for checking the appropriateness of proposed models, such as residual analyses and case influence diagnostics, and techniques for selecting models. Gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course_resources.html",
    "href": "course_resources.html",
    "title": "R Resources",
    "section": "",
    "text": "Tip\n\n\n\nClick on the link related to a specific week to explore R skills learned that week!",
    "crumbs": [
      "R Resources",
      "R Resources"
    ]
  },
  {
    "objectID": "course_resources.html#bonus",
    "href": "course_resources.html#bonus",
    "title": "R Resources",
    "section": "Bonus",
    "text": "Bonus\n\nggplot Customizations\n\n\nPivoted Summary Tables",
    "crumbs": [
      "R Resources",
      "R Resources"
    ]
  },
  {
    "objectID": "activity/week-5-sample-selection.html",
    "href": "activity/week-5-sample-selection.html",
    "title": "Sample Selection & Scope of Inference",
    "section": "",
    "text": "This activity is designed to help you critically evaluate how the data you are using in your Midterm Project were collected. You only need to complete one section of the activity, the section related to your dataset!"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#inclusion-criteria",
    "href": "activity/week-5-sample-selection.html#inclusion-criteria",
    "title": "Sample Selection & Scope of Inference",
    "section": "Inclusion Criteria",
    "text": "Inclusion Criteria\nThis activity will lead you in thinking about inclusion criteria–criteria which needed to be met in order for an observation to be included in a sample. Inclusion criteria determine which members of a population can or cannot be included as a possible observation in a study.\nInclusion criteria are used for experiments and observational studies! For example, a clinical trial for a new treatment for individuals with chronic heart failure might require potential subjects to:\n\nbe between 18 to 80 years of age\nhave been diagnosed with chronic heart failure within at least 6 months\nbe currently taking stable doses of heart failure therapies\nbe willing to return for required follow-up (post-test) visits\n\nAnyone meeting the inclusion criteria would be eligible to participate in the study. This does not mean that everyone meeting these criteria would participate in the study! The researchers would then decide how to sample from these eligible participants to obtain the sample for their study.\n\nAlright, let’s give this a go with the dataset you chose for your Midterm Project!"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#and_vertebrates",
    "href": "activity/week-5-sample-selection.html#and_vertebrates",
    "title": "Sample Selection & Scope of Inference",
    "section": "and_vertebrates",
    "text": "and_vertebrates\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#ntl_icecover",
    "href": "activity/week-5-sample-selection.html#ntl_icecover",
    "title": "Sample Selection & Scope of Inference",
    "section": "ntl_icecover",
    "text": "ntl_icecover\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#hbr_maples",
    "href": "activity/week-5-sample-selection.html#hbr_maples",
    "title": "Sample Selection & Scope of Inference",
    "section": "hbr_maples",
    "text": "hbr_maples\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#pie_crab",
    "href": "activity/week-5-sample-selection.html#pie_crab",
    "title": "Sample Selection & Scope of Inference",
    "section": "pie_crab",
    "text": "pie_crab\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#births14",
    "href": "activity/week-5-sample-selection.html#births14",
    "title": "Sample Selection & Scope of Inference",
    "section": "births14",
    "text": "births14\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#possum",
    "href": "activity/week-5-sample-selection.html#possum",
    "title": "Sample Selection & Scope of Inference",
    "section": "possum",
    "text": "possum\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#evals",
    "href": "activity/week-5-sample-selection.html#evals",
    "title": "Sample Selection & Scope of Inference",
    "section": "evals",
    "text": "evals\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-5-sample-selection.html#gapminder",
    "href": "activity/week-5-sample-selection.html#gapminder",
    "title": "Sample Selection & Scope of Inference",
    "section": "gapminder",
    "text": "gapminder\n\nWhat are the observations / rows in this dataset?\nFrom what population was the sample drawn?\nFor an observation to be included in the dataset, what inclusion criteria needed to be met?\nHow were the observations who satisfied the inclusion criteria sampled from the population?\nBased on the inclusion criteria and sampling methods, to what population can the findings of the study be generalized?"
  },
  {
    "objectID": "activity/week-2-code-along.html#adding-colors-to-scatterplots",
    "href": "activity/week-2-code-along.html#adding-colors-to-scatterplots",
    "title": "Week 2 Lecture - Practicing Your Visualizations",
    "section": "Adding Colors to Scatterplots",
    "text": "Adding Colors to Scatterplots\nIn the code chunk below:\n\ncopy and paste your scatterplot code\ncolor the points based on the body mass of the penguin\n\n\n\n\n\n\nWhat happens when you color the points based on the penguin’s species instead?"
  },
  {
    "objectID": "activity/week-7-sampling-activity.html",
    "href": "activity/week-7-sampling-activity.html",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "",
    "text": "What is the central tenant of random sampling? i.e. How are observations selected from the study population?\n\n\n\nComment on the “random-ness” (not representative-ness) of the following study:\n\n\nA Cal Poly administrator wants to know the average income of all graduates in the last 10 years. So they get the records of fifty randomly chosen graduates, contact them, and obtain their answers."
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#random-sampling",
    "href": "activity/week-7-sampling-activity.html#random-sampling",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "",
    "text": "What is the central tenant of random sampling? i.e. How are observations selected from the study population?\n\n\n\nComment on the “random-ness” (not representative-ness) of the following study:\n\n\nA Cal Poly administrator wants to know the average income of all graduates in the last 10 years. So they get the records of fifty randomly chosen graduates, contact them, and obtain their answers."
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#sampling-randomly",
    "href": "activity/week-7-sampling-activity.html#sampling-randomly",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "Sampling Randomly",
    "text": "Sampling Randomly\nSuppose we have a database of every professor at UT Austin, and are interested in studying the relationship between a professor’s teaching evaluations and their age.\n\nHow would we go about randomly sampling observations from the database?\n\n\n\nWould we expect that our sample look like the population of professors at UT Austin?"
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#representative-sampling",
    "href": "activity/week-7-sampling-activity.html#representative-sampling",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "Representative Sampling",
    "text": "Representative Sampling\n\nWhat is the central tenant of representative sampling? i.e. How are observations selected from the study population?\n\n\n\nComment on the “representative-ness” of the following studies:\n\n\nThe Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force).\n\n\n\nYou want to know the average number of people living in houses in the 1-mile radius surrounding Cal Poly. You randomly pick 25 houses and record data by knocking on the door of each house."
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#sampling-representatively",
    "href": "activity/week-7-sampling-activity.html#sampling-representatively",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "Sampling Representatively",
    "text": "Sampling Representatively\nSuppose we want to ensure that we have a representative proportion of faculty who are women and faculty of different ranks (e.g., teaching, tenure track, tenured). The table below summarizes the demographics of women and men faculty of different ranks:\n\n\nWomen make up approximately 40% of all UT Austin faculty\n\nWomen teaching = 58%\nWomen tenure track = 30%\nWomen tenured = 12%\n\n\n\n\nMen make up approximately 60% of all UT Austin faculty\n\nMen teaching = 20%\nMen tenure track = 42%\nMen tenured = 38%\n\n\n\n\nSuppose we can reasonably collect data on 200 individuals. How could we go about collecting a representative sample that accounts for these demographic characteristics?"
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#sampling-issues",
    "href": "activity/week-7-sampling-activity.html#sampling-issues",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "Sampling Issues",
    "text": "Sampling Issues\nIn the evals data, the courses included in the dataset were only taught by 94 unique professors. If we were interested in making inferences about the relationship between eval scores and age for professors at UT Austin, a simpler approach would be to have one observation per professor!\n\n\n\n\n\n\n\n\n\n\nProfessor ID\nNumber of Times Sampled\n\n\n\n\n34\n13\n\n\n49\n13\n\n\n82\n11\n\n\n10\n10\n\n\n20\n10\n\n\n58\n10\n\n\n71\n10\n\n\n19\n9\n\n\n70\n9\n\n\n4\n8\n\n\n\n\n\n\n\n\n\n\n\nHow could you use sampling to remedy this situation?"
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#population-parameter",
    "href": "activity/week-7-sampling-activity.html#population-parameter",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "Population Parameter",
    "text": "Population Parameter\nTo investigate the relationship between course evaluation score and the professor’s age, we would carry out a simple linear regression.\n\nWhat would be the population parameter we are interested in? Do we know its value?\n\n\n\nWhat would be our point estimate?"
  },
  {
    "objectID": "activity/week-7-sampling-activity.html#repeated-samples",
    "href": "activity/week-7-sampling-activity.html#repeated-samples",
    "title": "Exploring Sampling Concepts (30-minutes)",
    "section": "Repeated Samples",
    "text": "Repeated Samples\nRepeated samples are necessary for us to create a sampling distribution—a distribution of statistics from lots of different samples—like the one below.\n\nWhat are we assuming about how the samples were collected when we plot every statistic on the same distribution?"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8 – Hypothesis Tests for Slope Coefficients & Conditions for Inference",
    "section": "",
    "text": "1 Textbook Reading\nRequired Reading: Hypothesis Test for Slope & Inference Conditions\n\n\n\n\n\n\nReading Guide – Due Monday by the start of class\n\n\n\nDownload the Word Document\n\n\n\n\n2 Concept Quiz – Due Monday by the start of class\n1. Which of the following are always true for hypothesis statements?\n\nThey are about sample statistics.\nThey are about relationships between variables.\nThey are about population parameters.\nThey are about differences in groups.\n\n2. Which of the following are true about a null distribution? (select all that apply)\n\nIt is a distribution of statistics.\nThe values on the distribution represent what might have happened if the null hypothesis was true.\nThe values on the distribution represent what might have happened if the alternative hypothesis was true.\nIt is a distribution of sample observations.\n\n3. Which of the following are true about a p-value? (select all that apply)\n\nIt is calculated assuming the null hypothesis is true.\nIt is the probability the null hypothesis is true.\nIt quantifies how “surprising” our data are.\nIt compares the observed statistic to a distribution of values that could have happened if the null was true.\nIt is calculated assuming the alternative hypothesis is true.\nIt is a probability.\n\n4. Which of the following is true about a small p-value?\n\nThe sample statistic is unlikely to have happened by chance.\nThe sample size was large.\nThe sample statistic is unlikely to have happened if the null hypothesis was true.\nThe sample statistic was large.\n\n5. If you obtain a large p-value, what can you conclude about your hypotheses?\n\nWe cannot say the alternative hypothesis is false.\nWe cannot say the null hypothesis is false.\nThe null hypothesis is true.\nThe alternative hypothesis is true.\n\n6. Match each procedure to the question it addresses.\n\n\nconfidence intervals\nhypothesis tests\n\n\n\n\nWhat are plausible values for the population parameter?\nWhat are plausible values for the sample statistic?\nIs the population parameter different from 0?\nIs the value of the parameter different from a specified quantity?\n\n\n\n7. If the probability of a Type I error goes down, what can you say about the probability of a Type II error?\n\nThe probability of a Type II error goes down.\nThe probability of a Type II error stays the same.\nThe probability of a Type II error goes up.\n\n8. If you obtained a small p-value (e.g., 0.02), what could you say about what you would expect if you constructed a 95% confidence interval?\n\nIt would contain the null hypothesized value.\nIt would not contain the null hypothesized value.\nIt would contain the sample statistic.\nIt would contain the true population parameter.\n\n9. In a regression table, what is the “statistic” value associated with the slope?\n\na bootstrap statistic\na z-statistic\nthe sample slope statistic\na t-statistic\n\n10. In a regression table, how is the p-value calculated?\n\nUsing a permutation distribution with 1000 resamples\nUsing a bootstrap distribution with 1000 samples\nUsing a Normal distribution\nUsing a t-distribution\n\n11. What are the required conditions for linear regression? (select all that apply)\n\na random sample was taken\nequal variance of residuals\na large sample was collected\nlinear relationship between x and y\nindependence of observations\nindependence of variables\nnormality of residuals\nnormality of observations\n\n12. Which of the following would violate the condition of independence? (select all that apply)\n\ncollecting a non-random sample\nobservations related geographically (spatially)\nobservations that are related in time (temporally)\nrepeated observations on the same person\nobservations related biologically\n\n\n\n3 R Tutorial – Due Wednesday by the start of class\nRequired Tutorial: Randomization test for the slope\nRequired Tutorial: Evaluating the technical conditions in linear regression",
    "crumbs": [
      "Weekly materials",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10: Two-Way ANOVA",
    "section": "",
    "text": "Welcome!\nThis week we explore one final area of ANOVA, a situation where we have multiple explanatory variables. This is called a two-way ANOVA. A two-way ANOVA extends the one-way ANOVA to situations with two categorical explanatory variables. This new methods allows researchers to simultaneously study two variables that might explain variability in the responses and explore whether the impacts of one explanatory variable change depending on the level of the other explanatory variable.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#textbook-reading",
    "href": "weeks/week-10.html#textbook-reading",
    "title": "Week 10: Two-Way ANOVA",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\nTo be honest, I didn’t have the energy this week to write another chapter on two-way ANOVA. So, instead we will be reading a couple of sections out of another statistics textbook. I’ve uploaded the PDFs of these sections to OneDrive, so you should be able to read them online or download them.\n\nSection 1 – Introduction to Two-Way ANOVA\n\n\n\n\n\n\nRead through “Step 6” on page 222\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3.1 – Problematic Undertones\n\n\n\nI want to point out that the context of example 3.1 in this section has some major problematic undertones. The example focuses on “the effects of corporate and endorser credibility” on how likely someone is to buy shoes. For no clear reason, the example only considers “female” college students, which implicitly sends two problematic messages, (1) sex and gender are the same, and (2) women are more easily swayed by advertising.\nIn addition, the example uses Roseanne Barr as one of the celebrity endorsers—an actress who has written deplorable remarks about transgender people.\n\n\n\n\nSection 2 – Two-Way ANOVA Interaction Models\n\n\n\n\n\n\nRead through “Follow-up Confidence Intervals” on page 239\n\n\n\n\n\n\n\n\nReading Guide – Due Monday by the start of Class\nDownload the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#concept-quiz-due-monday-by-the-start-of-class",
    "href": "weeks/week-10.html#concept-quiz-due-monday-by-the-start-of-class",
    "title": "Week 10: Two-Way ANOVA",
    "section": "1.2 Concept Quiz – Due Monday by the start of class",
    "text": "1.2 Concept Quiz – Due Monday by the start of class\n1. Match each two-way ANOVA model to its correct description.\n\n\n\nadditive model\n  \ninteraction model\n\nThe relationship between one factor and the response changes depending on the level of another factor.\n\nThe relationship between one factor and the response remains the same across all levels of another factor.\n\nThere is a significant relationship between each factor and the response.\n\n\n\n2. Match each two-way ANOVA model to its multiple linear regression counterpart.\n\n\nadditive model\ninteraction model\n\n\n\ndifferent slopes\nparallel slopes\n\n\n\n\n\n\n\n\n\n3. Which of the following are conditions of a two-way ANOVA model? (Select all that apply)\n\nindependence of observations between the groups\nindependence of observations within each group\nindependence of variables\nnormally distributed responses within each group\nnormally distributed residuals\nlinear relationship between the explanatory and response variables\nequal variance of residuals\nequal variance of responses within each group\n\n\n4. Which of the following is important to consider when assessing independence? (Select all that apply)\n\nIf the same observational unit (e.g., person, penguin, tree) could be sampled multiple times\nIf observations could be biologically related (e.g., parents, siblings, etc.)\nIf observations could be spatially related (e.g., countries that border each other)\nIf there is a relationship between the observations and the response variable\n\n\n\n\n5. [True or False] When an additive model is fit, the interpretation of the p-value associated with each explanatory variable is [conditional / independent] on the other explanatory variable(s) in the model.\n\n6. If the null hypothesis is rejected in an additive two-way ANOVA model, what model should be fit instead?\n\nseparate one-way ANOVA models\na one-way ANOVA model containing the variable with the smallest p-value (from the additive two-way ANOVA)\na mean only model\nan interaction two-way ANOVA model\na simple linear regression",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-10.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-10.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "title": "Week 10: Two-Way ANOVA",
    "section": "1.3 R Tutorial – Due Wednesday by the start of class",
    "text": "1.3 R Tutorial – Due Wednesday by the start of class\nThis week’s R tutorial is a short guide stepping you through model selection starting with a two-way ANOVA interaction model.\nClick here to access this week’s R tutorial\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit a screenshot of the completion page at the end of the tutorial.",
    "crumbs": [
      "Weekly materials",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to dive deeper into exploring data, by incorporating categorical variables. Specifically, we will explore how we can incorporate categorical variables into our visualizations through the use of colors and facets. We will also explore how we can obtain summary statistics for different groups of a categorical variable, making our summaries more specific.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#learning-outcomes",
    "href": "weeks/week-3.html#learning-outcomes",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe what a categorical variable is and how these types of variables are stored in R\ndetermine whether a variable in a dataset is categorical\nincorporate categorical variables into visualizations of two and three variables\nobtain summary statistics for each level of a categorical variable",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#reading-guide-due-monday-by-the-start-of-class",
    "href": "weeks/week-3.html#reading-guide-due-monday-by-the-start-of-class",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.1 Reading Guide – Due Monday by the start of class",
    "text": "1.1 Reading Guide – Due Monday by the start of class\nDownload the Word Document\n\n\n\n\n\n\nAnswers\n\n\n\nPlease include your answers as a different color! You can pick whatever color you like, but please use a color other than black.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#textbook-reading-part-1",
    "href": "weeks/week-3.html#textbook-reading-part-1",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.2 Textbook Reading – Part 1",
    "text": "1.2 Textbook Reading – Part 1\n\n\n\n\n\n\n\n\n\n\n\nRequired Reading: Incorporating Categorical Variables into Visualizations\n\n\n\n\n\n\nOnly read Section 4.6!",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#textbook-reading-part-2",
    "href": "weeks/week-3.html#textbook-reading-part-2",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.3 Textbook Reading – Part 2",
    "text": "1.3 Textbook Reading – Part 2\nRequired Reading: Incorporating Categorical Variables into Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRead Sections 3.3 - 3.6!",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#concept-quiz-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-3.html#concept-quiz-due-wednesday-by-the-start-of-class",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.4 Concept Quiz – Due Wednesday by the start of class",
    "text": "1.4 Concept Quiz – Due Wednesday by the start of class\n1. Suppose you’ve run the following code to make side-by-side boxplots of life expectancy:\n\nggplot(data = gapminder_small, \n       mapping = aes(x = country, y = lifeExp)) +\n  geom_boxplot() +\n  labs(x = \"\", \n       y = \"\", \n       title = \"Life Expectancy (in Years) by Country\",\n       fill = \"Country\"\n       )\n\nYour resulting plot looks like this:\n\n\n\n\n\n\n\n\n\nWhat should you do?\n\nMove country to the y-axis.\nRemove some countries so the names don’t overlap.\nNothing, it looks great!\n\n2. If you didn’t want to stack your boxplots side-by-side, how else could you separate the groups?\n\n\n\n\n\n\nTip\n\n\n\nThis does not go inside the aes() function!\n\n\n3. What functions are necessary to calculate summary statistics for different groups of a categorical variable?\n\nsummarize()\nmutate()\narrange()\ngroup_by()\nfilter()\n\n4. A categorical variable could be correctly stored as what data types?\n\ninteger\ndouble\ncharacter\nfactor\n\n5. In Lab 2, you worked with the nycflights dataset, which contained information on flights departing from NYC airports. If we wanted to know the average departure delay for every airline in the dataset (e.g., Delta, Allegiant, United), what steps would we need to use? Match the code to its corresponding step.\n\n\nStep 1 %&gt;%\nStep 2 %&gt;%\nStep 3\n\n\n\nnycflights\ngroup_by(airline)\nfilter(dest == \"SFO\")\nsummarize(mean_dd = mean(dep_delay))\n\n\n6. What function did you learn that is used for converting a numerical variable to a categorical variable (a process called discretizing)?\n\nmutate()\nfilter()\nif_else()\ngroup_by()\n\n7. The arrange() function sorts or reorder a data frame’s rows according to the values of the specified variable. By default, what ordering does it use for the rows?\n\nascending (smallest to largest)\ndescending (largest to smallest)",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-3.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "title": "Week Three: Incorporating Categorical Variables",
    "section": "1.5 R Tutorial – Due Wednesday by the start of class",
    "text": "1.5 R Tutorial – Due Wednesday by the start of class\nIt has come to my attention that Posit Cloud has removed their tutorials in favor of what they are calling “recipes”—short tutorials on how to do specific things in R.\nSo, I’m pivoting to a different tutorial that uses group_by()—the tool you are learning this week—to explore different sampling strategies.\nRequired Tutorial: Sampling Strategies & Experimental Design\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit a screenshot of the completion page for each tutorial to the Canvas assignment portal!",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-0.html",
    "href": "weeks/week-0.html",
    "title": "Course Setup and Structure",
    "section": "",
    "text": "Welcome!\nIn this coursework, you’ll get set up with the Class Discord, learn about what is expected of you each week, and hear some tips from me about how to succeed.",
    "crumbs": [
      "Weekly materials",
      "Week 0"
    ]
  },
  {
    "objectID": "weeks/week-0.html#set-up-your-account",
    "href": "weeks/week-0.html#set-up-your-account",
    "title": "Course Setup and Structure",
    "section": "1.1 Set up your account",
    "text": "1.1 Set up your account\n\nVerify your email\nTo use this Discord server, you must have a verified email.\nNobody (including your professors) will be able to see this email, and it does not have to be your Cal Poly email. This is simply to keep the server from being overrun by temporary accounts.\n\n\nCreate your identity\nThe first thing you should do is decide what name and picture you would like to use.\n\nI would like to strongly encourage you to use your real name and picture, so that everyone can get to know you. However, if you prefer to remain anonymous, you are free to do so.\n\n(Please do not be like Regina and use the name of another student, however!\nThis kind of impersonation will result in a permanent ban from the server.)\n\n\nDecide about privacy and notifications\nThe default settings on the channel are probably just fine for you.\nFeel free to make any changes that work for you, though.\nYou can change your message notifications:\n\nYou can edit your privacy settings, although most things are already private:\n\n\n\nConnect other apps\nYou can connect other apps to Discord, either for productivity or just for fun.",
    "crumbs": [
      "Weekly materials",
      "Week 0"
    ]
  },
  {
    "objectID": "weeks/week-0.html#using-the-shannels",
    "href": "weeks/week-0.html#using-the-shannels",
    "title": "Course Setup and Structure",
    "section": "1.2 Using the shannels",
    "text": "1.2 Using the shannels\nThe server is made up of many channels. Some are text chatrooms, while some are “Voice Channels” that connect you via audio to everyone else in the channel.\n\nText shannels\nUse the #general channel for anything and everything:\n\nIf your question is about course logistics, rather than the material itself, consider using the #class-logistics channel:\n\nYou can use the specific weekly channels to ask questions about the material…\n\n… or the specific lab assignment.\n\nNotice that you can use tick marks (```), like in Quarto, to make your code appear in a formatted code box.\n\n\nVoice shannels\nTo join a voice channel, simply click it! Make sure you are careful about when you are muted or unmuted.\n\nThe extra “Side Chat” channels are limited to 4 or 8 people, if you would like to start an impromptu study conversation without being heard by me and / or the rest of the class. (I’ll only drop in if you invite us!)\nVoice channels can also be used for people to “Go Live”, and share their screen with everyone else.\n\nWhile this will usually be something professors use to demonstrate code, you can go live, too! But you may need to download the desktop version of Discord to do so.\n\n\nPrivate messages\nIt is also easy to send private messages, to your professor or to each other. These private messages can also easily be used to launch a private video chat and / or screen sharing.",
    "crumbs": [
      "Weekly materials",
      "Week 0"
    ]
  },
  {
    "objectID": "weeks/week-0.html#see-you-at-the-party",
    "href": "weeks/week-0.html#see-you-at-the-party",
    "title": "Course Setup and Structure",
    "section": "1.3 See you at the Party!",
    "text": "1.3 See you at the Party!",
    "crumbs": [
      "Weekly materials",
      "Week 0"
    ]
  },
  {
    "objectID": "weeks/week-0.html#how-your-typical-week-will-look",
    "href": "weeks/week-0.html#how-your-typical-week-will-look",
    "title": "Course Setup and Structure",
    "section": "2.1 How your typical week will look",
    "text": "2.1 How your typical week will look\n\nReadings and videos (every week)\nI favor a “flipped classroom,” as it gives you more time to clarify and solidify statistical concepts through hands-on exercises. Each week, you will read the required chapter(s), completing a required reading guide to walk you through the central concepts for each week.\nDue Monday by the start of class.\n\n\nConcept quizzes (every week)\nEach week there will be a short (~10 questions) quiz over the reading and videos from the week. These quizzes are intended to ensure that you grasped the key concepts from the week’s readings. The quizzes are not timed, so you can feel free to check your answers with the textbook and / or videos if you so wish. The quizzes are marked on completion as complete or incomplete.\nDue Monday by the start of class.\n\n\nTutorials (every week)\nYou can think of the tutorials as an “interactive” textbook, as they interweave statistical ideas alongside examples of how to work with data in R and hands-on exercises writing the R code necessary to complete a given task. Each exercise has hints available if you get stuck!\nThe tutorials are work at your own pace, so you can complete them all at once or slowly throughout the week. The lab assignments will require for you to put the skills you learned in the tutorials to work, so you are required to complete each week’s tutorial before Wednesday’s lab.\nThe tutorials are marked on completion as complete or incomplete. You will submit a screenshot of the completion page at the end of the tutorial to confirm that you completed the tutorial for the week.\nDue Wednesday by the start of class.\n\n\nStatistical critiques (every 4-weeks)\nThese assignments are case studies in which you will evaluate a data visualization or statistical analysis, determining how well-performed and presented the analysis was and making recommendations for improving or using the analysis. Critiques are due roughly 1-week after they are assigned and should take 1-2 hours. You will receive feedback and a mark of Success or Growing (elaborated more on later), and you will be able to revise based on that feedback. There will be two total critiques.\n\n\nLab Assignments (every week)\nLabs will be assigned on Wednesday every week, providing the opportunity to explore the course concepts in the context of real data. Lab assignments will require for you to work through the tutorial for the week, thus the tutorials are due before the start of class on Wednesday.\nYou will complete the lab assignments in the same teams you collaborate with in class. You will access the lab assignment through Posit Cloud, which you will be walked through during the first lab. Your group will be expected to submit your completed lab on Canvas. You will need to submit only the HTML document.\nDue Mondays at 5pm.\n\n\nMidterm & Final Projects (Week 6 & Week 10)\nThere will be two projects throughout the quarter, where you will be asked to synthesize the statistical concepts you have learned in a formal statistical report. Your critiques will help guide you toward how you do / don’t want your report to look. Each project will be done independently, and requires you to submit a project proposal and draft report before the final deadline. You are encouraged to use the feedback received on these assignments to improve your final report. The final reports will be graded as Excellent, Satisfactory, Progressing, or No Credit based on a rubric that will be shared with the initial assignment.",
    "crumbs": [
      "Weekly materials",
      "Week 0"
    ]
  },
  {
    "objectID": "weeks/week-0.html#week-1-concept-quiz-course-set-up",
    "href": "weeks/week-0.html#week-1-concept-quiz-course-set-up",
    "title": "Course Setup and Structure",
    "section": "2.2 ✅ Week 1 Concept Quiz: Course Set-up",
    "text": "2.2 ✅ Week 1 Concept Quiz: Course Set-up\nQuestion 1: Where are student hours held?\n\nIn person\nDiscord\nZoom by appointment\nCanvas\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the student hours section of the course syllabus!\n\n\nQuestion 2: What materials and technology are required for this course? Select all that apply!\n\nIntroduction to Modern Statistics\nRStudio Cloud – the application for working in R\nCanvas\nDiscord\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the required materials section of the course syllabus!\n\n\nQuestion 3: In this course, Reading Guides are due by ____, Concept Quizzes are due by ____, and Tutorials are due by ____.\nQuestion 4: If you have a question about the course content, what is the best first step?\n\nPost your question on Discord in the appropriate channel\nSend Dr. Theobold an email\nGo to Dr. Theobold’s office hours on Monday\nGoogle it\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the getting help section of the course syllabus!\n\n\nQuestion 5: You are permitted to submit up to ___ assignments late.\n\n\n\n\n\n\nTip\n\n\n\nLook at the late work policy section of the course syllabus!\n\n\nQuestion 6: If you need to submit an assignment late, you must do which of the following?\n\nemail Dr. Theobold to request a deadline extension\nsubmit a deadline extension request to the Google Form linked in the syllabus (and on Canvas)\nrequest an extension before the deadline\ncomplete the assignment no later than 3-days after the original deadline\n\nQuestion 7: If you receive a “Growing” on a Lab Assignment or a Critique, what should you do?\n\nread the comments and complete revisions on the problems receiving a “Growing”\nread the comments and complete revisions on the entire assignment\ncomplete revisions on the problems receiving a “Growing”\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the lab section of the course syllabus!\n\n\nQuestion 8: For revisions on Lab Assignments and Critiques to be considered that week they need to be turned in with:\n\nreflections on how your learning progressed from your initial attempt\nyour new answers\nyour original answers\nnothing\n\n\n\n\n\n\n\nTip\n\n\n\nLook at the revisions section of the course syllabus!\n\n\nQuestion 9: You and a friend have been working on Critique 1 together. You finish up and want to go to bed, but they are still a little confused. You email them your file, and say, “Don’t copy this, just look how I did it so you can figure it out.” Have you violated Academic Honesty policies?\nQuestion 10: You have been working on making a visualization for your Midterm Project for what feels like forever and it seems like you are making little to no progress. You find an example analysis on the internet with the visualization you were hoping to make. You copy-and-paste the code used to make the visualization into your Midterm Project and do not reference that you used an outside source. You have violated the Academic Honesty policy.\nQuestion 11: Upload a picture of you introducing yourself in the “Introductions” channel of the STAT 313 Discord Server.",
    "crumbs": [
      "Weekly materials",
      "Week 0"
    ]
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week Five: Multiple Linear Regression",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to build off the concepts we learned last week and delve deeper into linear regression. We are going to explore multiple linear regression, a statistical model where we have multiple explanatory variables and a single numerical response. We are going to refresh how to visualize these types of models, practice fitting these types of models in R, and the learn how to interpret these types of models.",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#learning-outcomes",
    "href": "weeks/week-5.html#learning-outcomes",
    "title": "Week Five: Multiple Linear Regression",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe to someone what a multiple linear regression is\noutline how a categorical explanatory variable can be included in a simple linear regression\nvisualize multiple linear regression models with one numerical and one categorical explanatory variable\ncalculate the simple linear regression line for each group of a categorical variable\noutline how a second numerical explanatory variable can be included in a simple linear regression\nvisualize multiple linear regression models with two numerical explanatory variables\ninterpret the coefficient of each explanatory variable included in the regression model\nrecite different methods that can be used to decide what multiple linear regression model is “best”\ndescribe the benefits and costs of using “model selection” for deciding on a multiple linear regression model",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#textbook-reading",
    "href": "weeks/week-5.html#textbook-reading",
    "title": "Week Five: Multiple Linear Regression",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Chapter 6 (https://moderndive.com/6-multiple-regression.html)\n\n\n\nReading Guide – Due Monday by the start of class\nDownload the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#concept-quiz-due-monday-by-the-start-of-class",
    "href": "weeks/week-5.html#concept-quiz-due-monday-by-the-start-of-class",
    "title": "Week Five: Multiple Linear Regression",
    "section": "1.2 Concept Quiz – Due Monday by the start of class",
    "text": "1.2 Concept Quiz – Due Monday by the start of class\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1 – Based on the visualizations above, I believe the [interaction / parallel slopes] model is more appropriate because the slopes between the groups are [very different / very similar].\nQuestion 2 – What type of model does the following code obtain?\n\nminority_lm &lt;- lm(score ~ age * ethnicity, data = evals)\n\n\ninteraction model\nparallel slopes model\nsimple linear regression model\n\n\nThe following is the output from the above regression model (from Question 2):\n\n\n# A tibble: 4 × 7\n  term                    estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept                  2.61      0.518      5.04   0        1.59     3.63 \n2 age                        0.032     0.011      2.84   0.005    0.01     0.054\n3 ethnicity: not minority    2.00      0.534      3.74   0        0.945    3.04 \n4 age:ethnicitynot minor…   -0.04      0.012     -3.51   0       -0.063   -0.018\n\n\n\nQuestion 3 – The intercept line represents the [evaluation score / mean evaluation score] for [male / female / minority / non-minority] faculty.\n\nQuestion 4 – The age line represents the relationship between age and evaluation scores for [male / female / minority / non-minority] faculty.\n\nQuestion 5 – The ethnicity:not minority line represents the [mean / adjustment to the mean] evaluation score for [male / female / minority / non-minority] faculty.\n\nQuestion 6 – The age:ethnicitynot minority line represents the [slope / adjustment to the slope] for the relationship between age and evaluation scores for [male / female / minority / non-minority] faculty.\n\nQuestion 7 – The value of the age:ethnicitynot minority line (-0.004) [does / does not ] match the decision I made in Question 1 as there is [little difference / substantial difference] in the slopes between the minority and non-minority faculty.\n\nQuestion 8 – What type of model does the following code obtain?\n\nbty_lm &lt;- lm(score ~ age + bty_avg, data = evals)\n\n\nmultiple linear regression with two numerical predictors\ninteraction model\nparallel slopes\n\n\nThe following is the output from the above regression model (from Question 8):\n\n\n# A tibble: 3 × 7\n  term      estimate std_error statistic p_value lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 intercept    4.06      0.17      23.9    0        3.72     4.39 \n2 age         -0.003     0.003     -1.15   0.251   -0.008    0.002\n3 bty_avg      0.061     0.017      3.55   0        0.027    0.094\n\n\n\nQuestion 9 – The intercept represents the [course evaluation score / mean course evaluation score] for professors whose age is __ and who have a average beauty score of ___.\n\nQuestion 10 – We interpret the value of -0.003 by age as:\nFor every [1 day / 1 year / 1 evaluation] increase in professor’s [evaluation score / age / average beauty] we expect the [course evaluation score / mean course evaluation score] to [increase / decrease] by __, after accounting for [ethnicity / gender / average beauty scores].",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-5.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "title": "Week Five: Multiple Linear Regression",
    "section": "1.3 R Tutorial – Due Wednesday by the start of class",
    "text": "1.3 R Tutorial – Due Wednesday by the start of class\n\nRegression modeling: Parallel Slopes\nRegression modeling: Evaluating and extending parallel slopes model",
    "crumbs": [
      "Weekly materials",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Model Selection",
    "section": "",
    "text": "Download the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#reading-guide-due-monday-by-the-start-of-class",
    "href": "weeks/week-6.html#reading-guide-due-monday-by-the-start-of-class",
    "title": "Model Selection",
    "section": "",
    "text": "Download the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#model-selection",
    "href": "weeks/week-6.html#model-selection",
    "title": "Model Selection",
    "section": "1.1 Model Selection",
    "text": "1.1 Model Selection\nThe best model is not always the most complicated. Sometimes including variables that are not evidently important can actually reduce the accuracy of predictions. This week, we will learn about model selection strategies – strategies which can help us eliminate variables from the model that are found to be less important. It’s common (and hip, at least in the statistical world) to refer to models that have undergone such variable pruning as parsimonious.\nIn practice, the model that includes all available predictors is often referred to as the full model. The full model may not be the best model, and if it isn’t, we want to identify a smaller model that is preferable.\nIn Section 6.3.1 of ModernDive, you used visualizations to compare an interaction model (different slopes) with a parallel slopes model. In the figures below, both the parallel slope and the interaction model attempt to explain \\(y =\\) the average math SAT score for various high schools in Massachusetts.\nWhen comparing the left and right-hand plots below, we observed that the three lines corresponding to small, medium, and large high schools were not that different. Given this similarity, we stated it could be argued that the “simpler” parallel slopes model should be favored.\n# Interaction model\nggplot(MA_schools, \n       aes(x = perc_disadvan, y = average_sat_math, color = size)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Percent economically disadvantaged\", y = \"Math SAT Score\", \n       color = \"School size\", title = \"Interaction model\")\n\n# Parallel slopes model\nggplot(MA_schools, \n       aes(x = perc_disadvan, y = average_sat_math, color = size)) +\n  geom_point(alpha = 0.25) +\n  geom_parallel_slopes(se = FALSE) +\n  labs(x = \"Percent economically disadvantaged\", y = \"Math SAT Score\", \n       color = \"School size\", title = \"Parallel slopes model\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Table 1 and Table 2, we observed that the interaction model was “more complex” in that the regression table had six (6) rows versus the four (4) rows of the parallel slopes model.\n\nsat_interaction &lt;- lm(average_sat_math ~ perc_disadvan * size, \n                          data = MA_schools)\nget_regression_table(sat_interaction)\n\n\n\n\n\nTable 1: Interaction model coefficient estimates\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n594.327\n13.288\n44.726\n0.000\n568.186\n620.469\n\n\nperc_disadvan\n-2.932\n0.294\n-9.961\n0.000\n-3.511\n-2.353\n\n\nsize: medium\n-17.764\n15.827\n-1.122\n0.263\n-48.899\n13.371\n\n\nsize: large\n-13.293\n13.813\n-0.962\n0.337\n-40.466\n13.880\n\n\nperc_disadvan:sizemedium\n0.146\n0.371\n0.393\n0.694\n-0.585\n0.877\n\n\nperc_disadvan:sizelarge\n0.189\n0.323\n0.586\n0.559\n-0.446\n0.824\n\n\n\n\n\n\n\n\n\n\n\nsat_parallel_slopes &lt;- lm(average_sat_math ~ perc_disadvan + size, \n                              data = MA_schools)\nget_regression_table(sat_parallel_slopes)\n\n\n\n\n\nTable 2: Parallel slopes model coefficient estimates\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n588.19\n7.607\n77.325\n0.000\n573.23\n603.15\n\n\nperc_disadvan\n-2.78\n0.106\n-26.120\n0.000\n-2.99\n-2.57\n\n\nsize: medium\n-11.91\n7.535\n-1.581\n0.115\n-26.74\n2.91\n\n\nsize: large\n-6.36\n6.923\n-0.919\n0.359\n-19.98\n7.26\n\n\n\n\n\n\n\n\n\n\nIn this reading, we’ll mimic the model selection we just performed using the qualitative “eyeball test,” but this time using a numerical and quantitative approach. Specifically, we’ll use the \\(R^2\\) summary statistic (pronounced “R-squared”), also called the “coefficient of determination.” But first, we must introduce one new concept: the variance of a numerical variable.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#r-squared-r2",
    "href": "weeks/week-6.html#r-squared-r2",
    "title": "Model Selection",
    "section": "1.2 R-squared (\\(R^2\\))",
    "text": "1.2 R-squared (\\(R^2\\))\nWe’ve previously studied two summary statistics of the spread (or variation) of a numerical variable: the standard deviation and the interquartile range (IQR). We now introduce a third summary statistic of spread: the variance. The variance is merely the standard deviation squared and it can be computed in R using the var() summary function within summarize()1.\nLet’s investigate the variability of the SAT math scores, using the MA_schools dataset. In the first plot, we visualize the overall spread / variability in these scores, using a histogram. In the second plot, we investigate the relationship between SAT math scores and the percent of students who are economically disadvantaged.\nggplot(MA_schools, \n       aes(x = average_sat_math)) +\n  geom_histogram(binwidth = 30) +\n  labs(x = \"Math SAT Score\")\n\nggplot(MA_schools, \n       aes(x = perc_disadvan, y = average_sat_math)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Percent economically disadvantaged\", y = \"Math SAT Score\")\n\n\n\n\n\n\nExploring the variability of SAT math scores\n\n\n\n\n\n\n\nExploring the variability of SAT math scores\n\n\n\n\n\nAs evidenced by the plot, there appears to be a moderately strong relationship between SAT math scores and the percent of students who are economically disadvantaged. Next, let’s fit the linear regression for modeling the relationship between SAT math scores and the percent of students who are economically disadvantaged.\nRecall, we can use the get_regression_points() function2 to our saved linear model (fit with the lm() function) to get:\n\nthe observed values (\\(y\\))\nthe fitted values (\\(\\hat{y}\\))\nthe residuals (\\(y - \\hat{y}\\))\n\n\nget_regression_points(sat_simple) \n#&gt; # A tibble: 332 × 5\n#&gt;      ID average_sat_math perc_disadvan average_sat_math_hat residual\n#&gt;   &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     1              516          21.5                 522.    -5.52\n#&gt; 2     2              514          22.7                 518.    -4.18\n#&gt; 3     3              534          14.6                 541.    -6.70\n#&gt; 4     4              581           6.3                 564.    17.2 \n#&gt; 5     5              592          10.3                 553.    39.4 \n#&gt; 6     6              576          10.3                 553.    23.4 \n#&gt; # ℹ 326 more rows\n\nLet’s now use the var() summary function within a summarize() to compute the variance of these three terms:\n\nget_regression_points(sat_simple) %&gt;% \n  summarize(var_y = var(average_sat_math), \n                      var_y_hat = var(average_sat_math_hat), \n                      var_residual = var(residual))\n#&gt; # A tibble: 1 × 3\n#&gt;   var_y var_y_hat var_residual\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 3691.     2570.        1121.\n\nObserve that the variance of \\(y\\) is equal to the variance of \\(\\hat{y}\\) plus the variance of the residuals. But what do these three terms tell us individually?\nFirst, the variance of \\(y\\) (denoted as \\(\\text{var}(y)\\)) tells us how much do Massachusetts high schools differ in average math SAT scores. The goal of regression modeling is to fit a model that hopefully explains this variation. In other words, we want to understand what factors explain why certain schools have high math SAT scores, while others have low scores. This is independent of the model; this is just data. In other words, whether we fit an interaction or parallel slopes model, \\(\\text{var}(y)\\) remains the same.\nSecond, the variance of \\(\\hat{y}\\) (denoted as \\(\\text{var}(\\hat{y})\\)) tells us how much the fitted values from our interaction model vary. That is to say, after accounting for (1) the percentage of the study body that is socioeconomically disadvantaged and (2) school size in an interaction model, how much do our model’s explanations of average math SAT scores vary?\nThird, the variance of the residuals tells us how much do “the left-overs” from the model vary. Observe how the points in the interaction plot above (on the left) scatter around the three lines. Say instead all the points fell exactly on one of the three lines. Then all residuals would be zero and hence the variance of the residuals would be zero.\nWe’re now ready to introduce \\(R^2\\)!\n\\[R^2 = 1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\]\n\\(R^2\\) is the proportion of the spread / variation of the outcome variable \\(y\\) that is explained by our model, where our model’s explanatory power is embedded in the fitted values \\(\\hat{y}\\). Furthermore, since it can be mathematically proven that \\(0 \\leq \\text{var}(y) \\leq \\text{var}(\\hat{y})\\) (a fact we leave for an advanced class on regression), we are guaranteed that:\n\\[0 \\leq R^2 \\leq 1\\]\nThus, \\(R^2\\) can be interpreted as follows:\n\n\\(R^2\\) values of 0 tell us that our model explains 0% of the variation in \\(y\\). Say we fit a model to the Massachusetts high school data and obtained \\(R^2 = 0\\). This would be telling us that the combination of explanatory variables (\\(x\\)) we used and model form we chose (interaction or parallel slopes) explains 0% of the variation in the SAT math scores. Or, in other words, the combination of explanatory variables (\\(x\\)) we used and model form we chose tell us nothing about average math SAT scores. The model is a poor fit.\n\\(R^2\\) values of 1 tell us that our model explains 100% of the variation in \\(y\\). Say we fit a model to the Massachusetts high school data and obtained \\(R^2 = 1\\). This would be telling us that the combination of explanatory variables (\\(x\\)) we used and model form we chose (interaction or parallel slopes) explains 0% of the variation in the SAT math scores. Or, in other words, the combination of explanatory variables (\\(x\\)) we used and model form we chose tell us everything we need to know about average math SAT scores.\n\nIn practice however, \\(R^2\\) values of 1 almost never occur. Think about it in the context of Massachusetts high schools. There are an infinite number of factors that influence why certain high schools perform well on SAT’s on average while others don’t perform well. The idea that a human-designed statistical model can capture all the heterogeneity of all high school students in Massachusetts is bordering on hubris. However, even if such models are not perfect, they may still prove useful in determining educational policy. A general principle of modeling we should keep in mind is a famous quote by eminent statistician George Box: “All models are wrong, but some are useful.”\n\nModel selection using R-squared\nWhen we learned about simple / basic regression, we discussed how, given a set of basic regressions, you could choose the “best” basic regression as the one where the explanatory variable has the strongest relationship with the response. We discussed this in terms of correlation, but for simple linear regression, \\(R^2\\) is the square of the correlation.\nThe ncbirths dataset contains a variety of variables on the habits and practices of expectant mothers and the birth of their children. Suppose, we were interested in choosing a simple linear regression with the variable that has the strongest relationship with weight (the birth weight of the baby). For explanatory variables, we have:\n\nweeks – length of pregnancy\nmage – mother’s age\nvisits – number of hospital visits during pregnancy\ngained – weight gained during pregnancy\n\nLet’s explore each of these basic regression models!\n\n\n\n\nTable 3: Comparison of four different simple regressions on birth weight of baby\n\n\n\n\n\n\n\ncor\nvar_y\nvar_y_hat\nvar_residual\nr_sq\n\n\n\n\nWeeks\n0.670\n2.27\n1.019\n1.25\n0.449\n\n\nGained\n0.154\n2.22\n0.053\n2.17\n0.024\n\n\nVisits\n0.135\n2.21\n0.040\n2.17\n0.018\n\n\nMother's Age\n0.055\n2.28\n0.007\n2.27\n0.003\n\n\n\n\n\n\n\n\n\n\nFrom Table 3, we can see that weeks has by far the strongest relationship with the birth weight of the baby, with a correlation of 0.670. Notice how we could get the \\(R^2\\) for this regression model (weight ~ weeks) two ways:\n\nWe could square the correlation: \\(0.670^2 =\\) 0.449\nWe could find \\(1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)} =\\) 0.449\n\nBoth methods produce an \\(R^2\\) of 0.449. This value tells us that the simple linear regression with weeks as the explanatory variable is able to explain 44.9% of the variability in the birth weight of a baby.\n\n\nAdding more variables\nYou might say, well why not add more than one variable into the model. That makes sense, given what we’ve learned about multiple linear regression. However, when we have a regression with more than one explanatory variable, \\(R^2\\) is no longer a good summary measure of how much variability in the response we’ve explained with our model. The problem lies in the formula for \\(R^2\\)!\nThis strategy for estimating \\(R^2\\) is acceptable when there is just a single explanatory variable. However, it becomes less helpful when there are many variables. The regular \\(R^2\\) is a biased estimate of the amount of variability explained by the model when applied to model with more than one predictor. This is because any variable when added to the model will decrease the residuals, even just by a little bit.\nDecreasing the residuals leads to:\n\na smaller value of \\(\\text{var}(\\text{residuals})\\)\na smaller fraction of \\(\\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\)\na larger number of \\(1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\)\n\nThus, adding any variable will always result in a larger value of \\(R^2\\).\nFor example, I’ve added a new column to the ncbirths dataset, one that is entirely simulated noise and has no relationship with the birth weight of the baby. Here is a plot of my variable, just to confirm!\n\n\n\n\n\n\n\n\n\nAlright, now I’m including this noise variable as an explanatory variable in the multiple linear regression, where weeks is also an explanatory variable.\n\nnoise_lm &lt;- lm(weight ~ weeks + noise, data = ncbirths_noise)\n\nget_regression_points(noise_lm) |&gt; \n    summarize(var_y = var(weight), \n              var_y_hat = var(weight_hat), \n              var_residual = var(residual), \n              r_sq = 1 - (var_residual / var_y)\n              )\n#&gt; # A tibble: 1 × 4\n#&gt;   var_y var_y_hat var_residual  r_sq\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  2.27      1.02         1.25 0.450\n\nFrom the table above, we can see that the \\(R^2\\) value has increased. Not by much, but still! We went from an \\(R^2\\) of 0.449 when weeks was the only explanatory variable, to an \\(R^2\\) of 0.45 when noise was added as an explanatory variable. This may seem like a small change, but it may not necessarily always be small! Thus, this behavior was concerning to Statisticians, and they invented an alternative.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#adjusted-r2",
    "href": "weeks/week-6.html#adjusted-r2",
    "title": "Model Selection",
    "section": "1.3 Adjusted \\(R^2\\)",
    "text": "1.3 Adjusted \\(R^2\\)\nTo get a better estimate, we use the adjusted \\(R^2\\)!\n\n\n\n\n\n\nAdjusted R-squared as a tool for model assessment.\n\n\n\nThe adjusted R-squared is computed as:\n\\[R^2_{adj} = 1 - \\frac{\\text{var(residuals)} / (n - k - 1)}{\\text{var}(y) / (n - 1)}\\] \\[R^2_{adj} = 1 - \\frac{\\text{var(residuals)}}{\\text{var}(y)} \\times \\frac{(n - 1)}{(n - k - 1)}\\] where \\(n\\) is the number of observations used to fit the model and \\(k\\) is the number of predictor variables in the model. Remember that a categorical predictor with \\(p\\) levels will contribute \\(p - 1\\) to the number of variables in the model because we need \\(p - 1\\) offsets!\n\n\nBecause \\(k\\) is never negative, the adjusted \\(R^2\\) will be smaller – often times just a little smaller – than the unadjusted \\(R^2\\). The reasoning behind the adjusted \\(R^2\\) lies in the degrees of freedom associated with each variance, which is equal to \\(n - k - 1\\) in the multiple regression context. If we were to make predictions for new data using our current model, we would find that the unadjusted \\(R^2\\) would tend to be slightly overly optimistic, while the adjusted \\(R^2\\) formula helps correct this bias.\n\n\n\n\n\n\nCaution\n\n\n\nAdjusted \\(R^2\\) could also have been used instead of unadjusted \\(R^2\\) for basic regressions. However, when there is only \\(k = 1\\) explanatory variables, adjusted \\(R^2\\) is very close to regular \\(R^2\\), so this nuance isn’t typically important when the model has only one predictor.\n\n\nComing back to the multiple linear regression where I included a second noise variable, we can compare the unadjusted \\(R^2\\) to the adjusted \\(R^2\\). Rather than calculating all of these summary statistics “by hand”, we can use the get_regression_summaries() function to obtain interesting summary statistics about the model:\n\nget_regression_summaries(noise_lm)\n\n\n\n\n\nTable 4: Summary statistics from multiple linear regression with noise and weeks as predictor variables\n\n\n\n\n\n\nr_squared\nadj_r_squared\nmse\nrmse\n\n\n\n\n0.45\n0.449\n1.25\n1.12\n\n\n\n\n\n\n\n\n\n\nNotice, Table 4 shows that adjusted \\(R^2\\) is lower than unadjusted \\(R^2\\). This is exactly what we want! We want our \\(R^2\\) to increase only if the extra variable we added was “worth it.” Here, the adjusted \\(R^2\\) suggests that adding a second explanatory variable (noise) is not worth the additional model complexity.\n\nModel selection using adjusted \\(R^2\\)\nAlright, let’s apply the concept of adjusted \\(R^2\\) to a multiple linear regression we are actually interested in. At the beginning of this reading, you were asked to recall the “eyeball” selection method we used to determine that the interaction model and parallel slopes model were unnecessarily more complicated than the simple linear regression model.\nIn Table 5 below, I’ve calculated the adjusted \\(R^2\\) for both the interaction model and parallel slopes model, and compared them to the simple linear regression model:\n\n\n\n\nTable 5: Comparing R-squared values for three different models for SAT math scores\n\n\n\n\n\n\n\nr_squared\nadj_r_squared\nmse\nrmse\n\n\n\n\nInteraction Model\n0.699\n0.694\n1107\n33.3\n\n\nParallel Slopes Model\n0.699\n0.696\n1109\n33.3\n\n\nSimple Linear Regression Model\n0.696\n0.695\n1118\n33.4\n\n\n\n\n\n\n\n\n\n\nObserve how the adjusted \\(R^2\\) values for the interaction model and parallel slopes model are nearly identical to the unadjusted \\(R^2\\) value from the simple linear regression model. These summaries suggest, there is no justification for added model complexity. The simple linear regression model with percent of students economically disadvantaged is able to explain the same amount of variance in the SAT math scores as the more complex models which include the size of the school.\n\nExtending to evals data\nNow let’s repeat this \\(R^2\\) comparison between interaction and parallel slopes model for our models of \\(y\\) = teaching score for UT Austin professors. In the ModernDive textbook, you visually compared using parallel slopes or different slopes to model the relationship between age and teaching score for male and female professors.\n\n\n\n\nTable 6: Comparing R-squared values from interaction and parallel slope models for UT Austin evaluation scores data\n\n\n\n\n\n\n\nr_squared\nadj_r_squared\nmse\nrmse\n\n\n\n\nInteraction Model\n0.051\n0.045\n0.280\n0.529\n\n\nParallel Slopes Model\n0.039\n0.035\n0.284\n0.533\n\n\n\n\n\n\n\n\n\n\nObserve how the adjusted \\(R^2\\) values in Table 6 are now very different! The adjusted \\(R^2\\) for the interaction model is 0.045 (or 4.5%) and the adjusted \\(R^2\\) for the parallel slopes model is 0.035 (or 3.5%). This may seem like a small change (of 1%), but if we calculate the percent change (\\(\\frac{0.045 - 0.035}{0.035}\\)), we see that by allowing the slopes to be different, we improved our adjusted \\(R^2\\) by almost 30%! Thus, it could be argued that the additional complexity is warranted.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#step-wise-selection",
    "href": "weeks/week-6.html#step-wise-selection",
    "title": "Model Selection",
    "section": "1.4 Step-wise Selection",
    "text": "1.4 Step-wise Selection\nAnother method for model selection is called “stepwise selection.” This method considers every possible explanatory variable in the dataset and decides if each variable should be included in the model, using a stepwise approach.\n\nData Context\nFor our investigation of this method, we will consider data about loans from the peer-to-peer lender, Lending Club. The loan data includes terms of the loan as well as information about the borrower. The outcome variable we would like to better understand is the interest rate assigned to the loan. For instance, all other characteristics held constant, does it matter how much debt someone already has? Does it matter if their income has been verified? Multiple regression will help us answer these and other questions.\nThe dataset includes results from 10,000 loans, and we’ll be looking at a subset of the available variables. The first six observations in the dataset are shown in Table 7 below, and descriptions for each variable are provided in Table 8. Notice that the past bankruptcy variable (bankruptcy) is an indicator variable, where it takes the value 1 if the borrower had a past bankruptcy in their record and 0 if not. Using an indicator variable in place of a category name allows for these variables to be directly used in regression. Two of the other variables are categorical (verified_income and issue_month), each of which can take one of a few different non-numerical values.\n\nThe loans_full_schema data can be found in the openintro R package. Based on the data in this dataset we have created two new variables: credit_util which is calculated as the total credit utilized divided by the total credit limit and bankruptcy which turns the number of bankruptcies to an indicator variable (0 for no bankruptcies and 1 for at least 1 bankruptcy). We will refer to this modified dataset as loans.\n\n\n\n\n\nTable 7: First six rows of the loans dataset.\n\n\n\n\n\n\ninterest_rate\nverified_income\ndebt_to_income\ncredit_util\nbankruptcy\nterm\ncredit_checks\nissue_month\n\n\n\n\n14.07\nVerified\n18.01\n0.548\n0\n60\n6\nMar-2018\n\n\n12.61\nNot Verified\n5.04\n0.150\n1\n36\n1\nFeb-2018\n\n\n17.09\nSource Verified\n21.15\n0.661\n0\n36\n4\nFeb-2018\n\n\n6.72\nNot Verified\n10.16\n0.197\n0\n36\n0\nJan-2018\n\n\n14.07\nVerified\n57.96\n0.755\n0\n36\n7\nMar-2018\n\n\n6.72\nNot Verified\n6.46\n0.093\n0\n36\n6\nJan-2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Variables and their descriptions for the loans dataset.\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ninterest_rate\nInterest rate on the loan, in an annual percentage.\n\n\nverified_income\nCategorical variable describing whether the borrower's income source and amount have been verified, with levels `Verified`, `Source Verified`, and `Not Verified`.\n\n\ndebt_to_income\nDebt-to-income ratio, which is the percentage of total debt of the borrower divided by their total income.\n\n\ncredit_util\nOf all the credit available to the borrower, what fraction are they utilizing. For example, the credit utilization on a credit card would be the card's balance divided by the card's credit limit.\n\n\nbankruptcy\nAn indicator variable for whether the borrower has a past bankruptcy in their record. This variable takes a value of `1` if the answer is *yes* and `0` if the answer is *no*.\n\n\nterm\nThe length of the loan, in months.\n\n\nissue_month\nThe month and year the loan was issued, which for these loans is always during the first quarter of 2018.\n\n\ncredit_checks\nNumber of credit checks in the last 12 months. For example, when filing an application for a credit card, it is common for the company receiving the application to run a credit check.\n\n\n\n\n\n\n\n\n\n\n\n\nStepwise selection\nTwo common strategies for adding or removing variables in a multiple regression model are called backward elimination and forward selection. These techniques are often referred to as stepwise selection strategies, because they add or delete one variable at a time as they “step” through the candidate predictors.\nBackward elimination starts with the full model (the model that includes every potential predictor variable). Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.\nForward selection is the reverse of the backward elimination technique. Instead, of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.\nAn important consideration in implementing either of these stepwise selection strategies is the criterion used to decide whether to eliminate or add a variable. One commonly used decision criterion is adjusted \\(R^2\\). When using adjusted \\(R^2\\) as the decision criterion, we seek to eliminate or add variables depending on whether they lead to the largest improvement in adjusted \\(R^2\\) and we stop when adding or elimination of another variable does not lead to further improvement in adjusted \\(R^2\\).\n\nBackward Selection\nLet’s consider two models, which are shown in Table 9 and Table 10 below. The first table summarizes the full model since it includes all predictors, while the second does not include the issue_month variable.\n\n\n\n\nTable 9: The fit for the full regression model, including the adjusted R-squared.\n\n\n\n\nThe fit for the full regression model, including the adjusted $R^2$.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.89\n0.21\n9.01\n&lt;0.0001\n\n\nverified_incomeSource Verified\n1.00\n0.10\n10.06\n&lt;0.0001\n\n\nverified_incomeVerified\n2.56\n0.12\n21.87\n&lt;0.0001\n\n\ndebt_to_income\n0.02\n0.00\n7.43\n&lt;0.0001\n\n\ncredit_util\n4.90\n0.16\n30.25\n&lt;0.0001\n\n\nbankruptcy1\n0.39\n0.13\n2.96\n0.0031\n\n\nterm\n0.15\n0.00\n38.89\n&lt;0.0001\n\n\ncredit_checks\n0.23\n0.02\n12.52\n&lt;0.0001\n\n\nissue_monthJan-2018\n0.05\n0.11\n0.42\n0.6736\n\n\nissue_monthMar-2018\n-0.04\n0.11\n-0.39\n0.696\n\n\n\n\n\nAdjusted R-sq = 0.2597\n\n\n\n\n\n\ndf = 9964\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10: he fit for the regression model after dropping issue month, including the adjusted R-squared.\n\n\n\n\nThe fit for the regression model after dropping issue month, including the adjusted $R^2$.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.90\n0.20\n9.56\n&lt;0.0001\n\n\nverified_incomeSource Verified\n1.00\n0.10\n10.05\n&lt;0.0001\n\n\nverified_incomeVerified\n2.56\n0.12\n21.86\n&lt;0.0001\n\n\ndebt_to_income\n0.02\n0.00\n7.44\n&lt;0.0001\n\n\ncredit_util\n4.90\n0.16\n30.25\n&lt;0.0001\n\n\nbankruptcy1\n0.39\n0.13\n2.96\n0.0031\n\n\nterm\n0.15\n0.00\n38.89\n&lt;0.0001\n\n\ncredit_checks\n0.23\n0.02\n12.52\n&lt;0.0001\n\n\n\n\n\nAdjusted R-sq = 0.2598\n\n\n\n\n\n\ndf = 9966\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhich of the two models is better?\nWe compare the adjusted \\(R^2\\) of each model to determine which to choose. Since the second model has a higher \\(R^2_{adj}\\) compared to the first model, we prefer the second model to the first.\n\n\nWill the model without issue_month be better than the model with issue_month? We cannot know for sure, but based on the adjusted \\(R^2\\), this is our best assessment.\nResults corresponding to the full model for the loans data are shown in Table 9. How should we proceed under the backward elimination strategy?\nOur baseline adjusted \\(R^2\\) from the full model is 0.2597, and we need to determine whether dropping a predictor will improve the adjusted \\(R^2\\). To check, we fit models that each drop a different predictor, and we record the adjusted \\(R^2\\) for each model:\n\nExcluding verified_income: 0.2238\nExcluding debt_to_income: 0.2557\nExcluding credit_util: 0.1916\nExcluding bankruptcy: 0.2589\nExcluding term: 0.1468\nExcluding credit_checks: 0.2484\nExcluding issue_month: 0.2598\n\nThe model without issue_month has the highest adjusted \\(R^2\\) of 0.2598, higher than the adjusted \\(R^2\\) for the full model. Because eliminating issue_month leads to a model with a higher adjusted \\(R^2\\), we drop issue_month from the model.\nSince we eliminated a predictor from the model in the first step, we see whether we should eliminate any additional predictors. Our baseline adjusted \\(R^2\\) is now \\(R^2_{adj} = 0.2598\\) – the adjusted \\(R^2\\) from the model without issue_month. We now fit new models, which consider eliminating issue_month and each of the remaining predictors:\n\nExcluding issue_month and verified_income: 0.22395\nExcluding issue_month and debt_to_income: 0.25579\nExcluding issue_month and credit_util: 0.19174\nExcluding issue_month and bankruptcy: 0.25898\nExcluding issue_month and term: 0.14692\nExcluding issue_month and credit_checks: 0.24801\n\nNone of these models lead to an improvement in adjusted \\(R^2\\), so we do not eliminate any of the remaining predictors. That is, after backward elimination, we are left with the model that keeps all predictors except issue_month, which we can summarize using the coefficients from Table 10.\n\n\n\n\n\n\n\nForward Selection\nNext, let’s use forward selection to construct a model for predicting interest_rate from the loans data.\nWe start with the model that includes no predictors. Then we fit each of the possible models with just one predictor. Then we examine the \\(R^2\\) for each of these models3:\n\nIncluding verified_income: 0.05926\nIncluding debt_to_income: 0.01946\nIncluding credit_util: 0.06452\nIncluding bankruptcy: 0.00222\nIncluding term: 0.12855\nIncluding credit_checks: -0.0001\nIncluding issue_month: 0.01711\n\nIn this first step, we compare the unadjusted \\(R^2\\) against a baseline model that has no predictors. The no-predictors model always has \\(R^2 = 0\\). The model with one predictor that has the largest \\(R^2\\) is the model with the term predictor, and because this \\(R^2\\) is larger than the \\(R^2\\) from the model with no predictors (\\(R^2 = 0\\)), we will add this variable to our model.\nWe repeat the process again, this time considering the adjusted \\(R^2\\)4 for all 2-predictor models where one of the predictors is term and with a new baseline of \\(R^2 = 0.12855:\\)\n\nIncluding term and verified_income: 0.16851\nIncluding term and debt_to_income: 0.14368\nIncluding term and credit_util: 0.20046\nIncluding term and bankruptcy: 0.13070\nIncluding term and credit_checks: 0.12840\nIncluding term and issue_month: 0.14294\n\nAdding credit_util yields the highest increase in adjusted \\(R^2\\) and has a higher adjusted \\(R^2\\) (0.20046) than the baseline (0.12855). Thus, we will also add credit_util to the model as a predictor.\nSince we have again added a predictor to the model, we again have a new baseline adjusted \\(R^2\\) of 0.20046. We can continue on and see whether it would be beneficial to add a third predictor:\n\nIncluding term, credit_util, and verified_income: 0.24183\nIncluding term, credit_util, and debt_to_income: 0.20810\nIncluding term, credit_util, and bankruptcy: 0.20169\nIncluding term, credit_util, and credit_checks: 0.20031\nIncluding term, credit_util, and issue_month: 0.21629\n\nThe model including verified_income has the largest increase in adjusted \\(R^2\\) (0.24183) from the baseline (0.20046), so we add verified_income to the model as a predictor as well.\nWe continue on in this way, next adding debt_to_income, then credit_checks, and bankruptcy. At this point, we come again to the issue_month variable: adding this as a predictor leads to \\(R_{adj}^2 = 0.25843\\), while keeping all the other predictors but excluding issue_month has a higher \\(R_{adj}^2 = 0.25854\\). This means we do not add issue_month to the model as a predictor. In this example, we have arrived at the same model that we identified from backward elimination.\n\n\n\nA recap\nBackward elimination begins with the model having the largest number of predictors and eliminates variables one-by-one until we are satisfied that all remaining variables are important to the model. Forward selection starts with no variables included in the model, then it adds in variables according to their importance until no other important variables are found. Notice that, for both methods, we have always chosen to retain the model with the largest adjusted \\(R^2\\) value, even if the difference is less than half a percent (e.g., 0.2597 versus 0.2598). One could argue that the difference between these two models is negligible, as they both explain nearly the same amount of variability in the interest_rate. These negligible differences are an important aspect to model selection. It is highly advised that before you begin the model selection process, you decide what a “meaningful” difference in adjusted \\(R^2\\) is for the context of your data. Maybe this difference is 1% or maybe it is 5%. This “threshold” is what you will then use to decide if one model is “better” than another model. Using meaningful thresholds in model selection requires more critical thinking about what the adjusted \\(R^2\\) values mean.\nAdditionally, backward elimination and forward selection sometimes arrive at different final models. This is because the decision for whether to include a given variable or not depends on the other variables that are already in the model. With forward selection, you start with a model that includes no variables and add variables one at a time. In backward elimination, you start with a model that includes all of the variables and remove variables one at a time. How much a given variable changes the percentage of the variability in the outcome that is explained by the model depends on what other variables are in the model. This is especially important if the predictor variables are correlated with each other.\nThere is no “one size fits all” model selection strategy, which is why there are so many different model selection methods. We hope you walk away from this exploration understanding how stepwise selection is carried out and the considerations that should be made when using stepwise selection with regression models.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#other-model-selection-strategies",
    "href": "weeks/week-6.html#other-model-selection-strategies",
    "title": "Model Selection",
    "section": "1.5 Other model selection strategies",
    "text": "1.5 Other model selection strategies\nStepwise selection using adjusted \\(R^2\\) as the decision criteria is one of many commonly used model selection strategies. Stepwise selection can also be carried out with decision criteria other than adjusted \\(R^2\\), such as p-values, which you’ll learn about in later weeks, or AIC (Akaike information criterion) or BIC (Bayesian information criterion), which you might learn about in more advanced statistics courses or your disciplinary courses.\nAlternatively, one could choose to include or exclude variables from a model based on expert opinion or due to research focus. In fact, many statisticians discourage the use of stepwise regression alone for model selection and advocate, instead, for a more thoughtful approach that carefully considers the research focus and features of the data.",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#footnotes",
    "href": "weeks/week-6.html#footnotes",
    "title": "Model Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe formula for variance is the square of the standard deviation, or \\(\\text{variance} = \\text{sd}^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}\\)↩︎\nTechnically, the get_regression_points() function does the same thing as the augment() function you learned in the R tutorials last week!↩︎\nRemember, for a basic regression with only one variable, we use (unadjusted) \\(R^2\\)!↩︎\nWe’ve moved to adjusted \\(R^2\\) because we now have more than one predictor variable!↩︎",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html",
    "href": "weeks/chapters/week7-reading2.html",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "",
    "text": "This week’s reading is a compilation of Chapter 8 from ModernDive (Kim et al. 2020), Chapter 24 from Introduction to Modern Statistics (Çetinkaya-Rundel and Hardin 2023), with a smattering of my own ideas."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#baby-birth-weights",
    "href": "weeks/chapters/week7-reading2.html#baby-birth-weights",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "1 Baby Birth Weights",
    "text": "1 Baby Birth Weights\nMedical researchers may be interested in the relationship between a baby’s birth weight and the age of the mother, so as to provide medical interventions for specific age groups if it is found that they are associated with lower birth weights.\nEvery year, the US releases to the public a large data set containing information on births recorded in the country. The births14 dataset is a random sample of 1,000 cases from one such dataset released in 2014.\n\n1.1 Observed data\nFigure 1 visualizes the relationship between mage and weight for this sample of 1,000 birth records.\n\nggplot(data = births14, \n       mapping = aes(x = mage, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight of Baby (lbs)\")\n\n\n\n\n\n\n\nFigure 1: Weight of baby at birth (in lbs) as explained by mother’s age.\n\n\n\n\n\nTable 1 displays the estimated regression coefficients for modeling the relationship between mage and weight for this sample of 1,000 birth records.\n\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n\nget_regression_table(births_lm)\n\n\n\n\n\nTable 1: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n6.793\n0.208\n32.651\n0.000\n6.385\n7.201\n\n\nmage\n0.014\n0.007\n1.987\n0.047\n0.000\n0.028\n\n\n\n\n\n\n\n\n\n\nBased on these coefficients, the estimated regression equation is:\n\\[ \\widehat{\\text{birth weight}} = -6.793 + 0.014 \\times \\text{mother's age}\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will let \\(\\beta_1\\) represent the slope of the relationship between baby’s birth weight and mother’s age for every baby born in the US in 2014. We will estimate \\(\\beta_1\\) using the births14 dataset, labeling the estimate \\(b_1\\) (just as we did in Week 4).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nA parameter is the value of the statistic of interest for the entire population.\nWe typically estimate the parameter using a “point estimate” from a sample of data. The point estimate is also referred to as the statistic.\n\n\n\n\n1.2 Variability of the statistic\nThis sample of 1,000 births is only one of possibly tens of thousands of possible samples that could have been taken from the large dataset released in 2014. So, then we might wonder how different our regression equation would be if we had a different sample. There is no reason to believe that \\(\\beta_1\\) is 0.279, but there is also no reason to believe that \\(\\beta_1\\) is particularly far away from \\(b_1 =\\) 0.279.\nJust this week you read about how estimates, such as \\(b_1\\), are prone to sampling variation – the variability from sample to sample. For example, if we took a different sample of 1,000 births, would we obtain a slope of exactly 0.279? No, that seems fairly unlikely. We might obtain a slope of 0.29 or 0.26, or even 0.35!\nWhen we studied the effects of sampling variation, we took many samples, something that was easily done with a shovel and a bowl of red and white balls. In this case, however, how would we obtain another sample? Well, we would need to go to the source of the data—the large public dataset released in 2014—and take another random sample of 1,000 observations. Maybe we don’t have access to that original dataset of 2014 births, how could we study the effects of sampling variation using our single sample? We will do so using a technique known as bootstrap resampling with replacement, which we now illustrate.\n\n\n1.3 Resampling once\n\n\n\n\n\n\nFigure 2: Step 1: Write out mother’s ages and baby’s birth weights on 1,000 slips of paper representing one of the 1,000 births included in the original sample.\n\n\n\nStep 1: Print out 1,000 identically sized slips of paper (or post-it notes) representing the sample of 1,000 babies in our sample. On each piece of paper, write the mother’s age and the birth weight of the baby. Figure 2 displays six of these such papers.\nStep 2: Put the 1,000 slips of paper into a hat as seen in Figure 3.\n\n\n\n\n\n\nFigure 3: Step 2: Putting 1,000 slips of paper (post-its) in a hat.\n\n\n\nStep 3: Mix the hat’s contents and draw one slip of paper at random, as seen in Figure 4. Record the mother’s age and baby’s birth weight, as printed on the paper.\n\n\n\n\n\n\nFigure 4: Step 3: Drawing one slip of paper at random.\n\n\n\nStep 4: Put the slip of paper back in the hat! In other words, replace it as seen in Figure 5.\n\n\n\n\n\n\nFigure 5: Step 4: Replacing slip of paper.\n\n\n\nStep 5: Repeat Steps 3 and 4 a total of 999 more times, resulting in 1,000 recorded mother’s ages and baby birth weights.\nWhat we just performed was a resampling of the original sample of 1,000 birth records. We are not sampling 1,000 birth records from the population of all 2014 US births as we did for our original sample. Instead, we are mimicking this process by resampling 1,000 birth records from our original sample.\nNow ask yourselves, why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would end up with the same 1,000 birth records! In other words, replacing the slips of paper induces sampling variation.\nBeing more precise with our terminology, we just performed a resampling with replacement from the original sample of 1,000 birth records. Had we left the slip of paper out of the hat each time we performed Step 4, this would be resampling without replacement.\nLet’s study our sample of 1,000 resampled birth records. First, I’ve made a table of the number of times each observation (birth record) was resampled. Table 2 displays the birth records that were resampled the most often. Based on the table, it appears that record IDs 71 and 534 were resampled six times.\n\n\n\n\nTable 2: Frequencies of how often a given birth record (ID) was resampled.\n\n\n\n\n\n\n71\n534\n170\n561\n19\n142\n305\n309\n495\n573\n726\n727\n835\n838\n840\n864\n919\n924\n929\n953\n\n\n\n\n6\n6\n5\n5\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember\n\n\n\nWhen sampling with replacement, not every observation is guaranteed to be sampled. So, some observations from the original sample may never appear in the resample, whereas others may appear multiple times.\n\n\nFigure 6 compares the relationship between mother’s age and baby’s birth weight from our resample with the relationship in our original sample.\n\nggplot(data = births14, \n       mapping = aes(x = mage, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight (lbs)\")\n\nggplot(data = resample1, \n       mapping = aes(x = mage, y = weight)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight (lbs)\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Sample of 1,000 Birth Records\n\n\n\n\n\n\n\n\n\n\n\n(b) Resample of 1,000 Birth Records\n\n\n\n\n\n\n\nFigure 6: Comparing relationship between mage and weight in the resampled birth records compared to the relationship seen in the original sample of birth records.\n\n\n\n\nObserved in Figure 6 that while the general shape of both relationships are similar, they are not identical.\nRecall, from the previous section that the sample slope (\\(b_1\\)) from the original sample was . What about for our resample? Based on the scatterplot, what would your guess be? Larger than before? Smaller than before?\nLet’s look at the coefficient estimates for the resampled dataset. Table 3 displays the estimated regression coefficients for modeling the relationship between mage and weight for the resample of 1,000 birth records.\n\nresample_lm &lt;- lm(weight ~ mage, data = resample1)\n\nget_regression_table(resample_lm)\n\n\n\n\n\nTable 3: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age, for the resample of 1,000 birth records.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n6.981\n0.220\n31.738\n0.000\n6.549\n7.413\n\n\nmage\n0.005\n0.008\n0.684\n0.494\n-0.010\n0.020\n\n\n\n\n\n\n\n\n\n\nFor the resampled dataset, the relationship between mother’s age and baby birth weight is much weaker than in the original sample, with an estimated slope of \\(b_1 =\\) 0.005.\nWhat if we repeated this resampling exercise many times? Would we obtain the same slope each time? In other words, would our guess at the slope for the relationship between mother’s age and baby’s birth weight for all births in the US in 2014 exactly 0.005 every time?"
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#computer-simulation-and-resampling",
    "href": "weeks/chapters/week7-reading2.html#computer-simulation-and-resampling",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "2 Computer simulation and resampling",
    "text": "2 Computer simulation and resampling\nIt should be very clear that tactile resampling with a dataset with 1,000 observations would be extremely time consuming, nothing we would want to ask our friends to do. A computer, however, would be happy to do this process for us!\n\n2.1 Virtually resampling once\nFirst, let’s perform the virtual analog of resampling once. Recall that the births14 dataset included in the openintro package contains the 1,000 birth records from the original study. Furthermore, recall in the last chapter that we used the rep_sample_n() function as a virtual shovel to sample balls from our virtual bowl of 2400 balls as follows:\nLet’s modify this code to perform the resampling with replacement from the 1,000 birth records in the original sample:\n\nvirtual_resample &lt;- rep_sample_n(births14, \n                                 size = 1000,\n                                 replace = TRUE, \n                                 reps = 1)\n\nObserve how we explicitly set the replace argument to TRUE in order to tell rep_sample_n() that we would like to sample birth records with replacement. Had we not kept replace = FALSE, we would have done resampling without replacement. Additionally, we changed the sample size, as we need to create a sample with the same size as the original sample which had 1,000 observations.\nLet’s look at only the first 10 out of 1,000 rows of virtual_resample:\n\nvirtual_resample\n\n# A tibble: 1,000 × 15\n# Groups:   replicate [1]\n   replicate    ID  mage weight  fage mature      weeks premie    visits gained\n       &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1         1   333    29   7.4     31 younger mom    39 full term     13     50\n 2         1   595    29   0.75    NA younger mom    21 premie         4     NA\n 3         1   338    31   6.94    NA younger mom    39 full term      9     20\n 4         1   531    33   6.94    37 younger mom    39 full term     14     20\n 5         1   289    35   7.98    35 mature mom     38 full term     10     40\n 6         1   461    32   7.15    33 younger mom    39 full term     13     NA\n 7         1   137    26   7.69    36 younger mom    38 full term     NA     25\n 8         1   828    36   4.54    65 mature mom     36 premie        11     10\n 9         1   118    26   8.31    24 younger mom    37 full term     12     25\n10         1    95    38   6.04    37 mature mom     36 premie        10     32\n# ℹ 990 more rows\n# ℹ 5 more variables: lowbirthweight &lt;chr&gt;, sex &lt;chr&gt;, habit &lt;chr&gt;,\n#   marital &lt;chr&gt;, whitemom &lt;chr&gt;\n\n\nThe replicate variable only takes on the value of 1 corresponding to us only having reps = 1, the ID variable indicates which of the 1,000 birth records was resampled, and mage and weight denote mother’s age and the baby’s birth weight, respectively.\nLet’s now compute the slope for the relationship between mother’s age and baby’s birth weight for this resample:\n\nvirtual_resample_lm &lt;- lm(weight ~ mage, data = virtual_resample)\n\nget_regression_table(virtual_resample_lm)\n\n\n\n\n\nTable 4: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age, for the virtual resample of 1,000 birth records.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n7.156\n0.224\n31.974\n0.000\n6.716\n7.595\n\n\nmage\n0.002\n0.008\n0.210\n0.834\n-0.014\n0.017\n\n\n\n\n\n\n\n\n\n\nAs we saw when we did our tactile resampling exercise, Table 4 shows that the estimated slope is different from the slope of our original sample of 0.005.\n\n\n2.2 infer package workflow\nUnfortunately, our process of virtual resampling relies on us fitting a linear regression for each replicate of our virtual_resample dataset. This gets a bit tricky coding wise, as we would need to fit 35 different linear regressions if we had 35 different resamples of our data.\nEnter, infer, an R package for statistical inference. infer makes efficient use of the %&gt;% pipe operator we learned in Week 3 to spell out the sequence of steps necessary to perform statistical inference in a “tidy” and transparent fashion. Furthermore, just as the dplyr package provides functions with verb-like names to perform data wrangling, the infer package provides functions with intuitive verb-like names to perform statistical inference.\nLet’s go back to our original slope. Previously, we computed the value of the sample slope \\(b_1\\) using the lm() function:\n\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n\nget_regression_table(births_lm)\n\nWe’ll see that we can also do this using infer functions specify() and calculate():\n\nbirths14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  calculate(stat = \"slope\")\n\nYou might be asking yourself: “Isn’t the infer code longer? Why would I use that code?”. While not immediately apparent, you’ll see that there are three chief benefits to the infer workflow as opposed to the lm() function workflow we had previously.\nFirst, the infer verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests. We’ll see flowchart diagrams of this framework in the upcoming Figure 12.\nSecond, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent next week, when we also use infer to conduct hypothesis tests for the slope statistic.\nThird, the infer workflow is much simpler for conducting inference when you have more than one variable, meaning we can extend what we’ve learned for simple linear regression to multiple linear regression models.\nLet’s now illustrate the sequence of verbs necessary to construct a confidence interval for \\(\\b_1\\), the slope of the relationship between mother’s age and baby’s birth weight for all US births in 2014.\n\n1. specify() variables\n\n\n\n\n\n\nFigure 7: “Diagram of specify()ing variables.”\n\n\n\nAs shown in Figure 7, the specify() function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by specifying the response argument. For example, in our births14 data frame, the response variable of interest is weight and the explanatory variable is mage:\n\nbirths14 %&gt;% \n  specify(response = weight, \n          explanatory = mage)\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 1,000 × 2\n   weight  mage\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   6.96    34\n 2   8.86    31\n 3   7.51    36\n 4   6.19    16\n 5   6.75    31\n 6   6.69    26\n 7   6.13    36\n 8   6.74    24\n 9   8.94    32\n10   9.12    26\n# ℹ 990 more rows\n\n\nNotice how the dataset got smaller, now there are only two columns where before there were 14. You should also notice the messages above the dataset (Response: weight (numeric) and Explanatory: mage). These are meta-data about the grouping structure of the dataset, declaring which variable has been assigned to the explanatory / response.\n\n\n\n\n\n\nNote\n\n\n\nThis is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Week 3.\n\n\n\n\n2. generate() replicates\n\n\n\n\n\n\nFigure 8: “Diagram of generate() replicates.”\n\n\n\nAfter we specify() the variables of interest, we pipe the results into the generate() function to generate replicates. Figure 8 shows how this is combined with specify() to start the pipeline. In other words, repeat the resampling process a large number of times, similar to how we collected 35 different samples of balls from the bowl.\nThe generate() function’s first argument is reps, which sets the number of replicates we would like to generate. Suppose we were interested in obtaining 50 different resamples (each of 1,000 birth records). Then, we would we set reps = 50, telling infer that we are interested in obtaining 50 different resamples, each with 1,000 observations.\nThe second argument type determines the type of computer simulation we’d like to perform. We set this to type = \"bootstrap\" indicating that we want to perform bootstrap resampling, meaning the resampling should be done with replacement. You’ll see different options for type when we learn about hypothesis testing next week.\n\nbirths14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 50, \n           type = \"bootstrap\")\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 50,000 × 3\n# Groups:   replicate [50]\n   replicate weight  mage\n       &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1         1  10.6     31\n 2         1   4.31    31\n 3         1   4.94    33\n 4         1   5.65    36\n 5         1   8       23\n 6         1   7.63    37\n 7         1   8       19\n 8         1   3.22    30\n 9         1   8.56    32\n10         1   4.32    21\n# ℹ 49,990 more rows\n\n\nObserve that the resulting data frame has 50,000 rows. This is because we performed resampling of 1000 birth records with replacement 50 times and 1000 \\(\\times\\) 50 = 50,000.\nThe variable replicate indicates which resample each row belongs to. So it has the value 1 1000 times, the value 2 1000 times, all the way through to the value 50 1000 times.\nComparing with original workflow: Note that the steps of the infer workflow so far produce the same results as the original workflow using the rep_sample_n() function we saw earlier. In other words, the following two code chunks produce similar results:\n\n\ninfer workflow\n\nbirths14 %&gt;%\n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 50, \n           type = \"bootstrap\") \n\n\n\n\nOriginal workflow\n\nrep_sample_n(births14, \n             size = 1000, \n             replace = TRUE,\n             reps = 50)\n\n\n\n\n\n3. calculate() summary statistics\n\n\n\n\n\n\nFigure 9: Diagram of calculate()d summary statistics.\n\n\n\nAfter we generate() many replicates of bootstrap resampling with replacement, we next want to summarize each of the 50 resamples of size 1000 to a single sample statistic value. As seen in Figure 9, the calculate() function does this.\nIn our case, we want to calculate the slope between mother’s age and baby’s birth weight for each bootstrap resample of size 1000. To do so, we set the stat argument to \"slope\".\n\n\n\n\n\n\nTip\n\n\n\nYou can also set the stat argument to a variety of other common summary statistics, like \"median\", \"sum\", \"sd\" (standard deviation), and \"prop\" (proportion). To see a list of all possible summary statistics you can use, type ?calculate and read the help file.\n\n\nLet’s save the result in a data frame called bootstrap_distribution and explore its contents:\n\nbootstrap_distribution &lt;- births14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 50) %&gt;% \n  calculate(stat = \"slope\")\n\nbootstrap_distribution\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 50 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1 0.0328 \n 2         2 0.0181 \n 3         3 0.0123 \n 4         4 0.0106 \n 5         5 0.0183 \n 6         6 0.0190 \n 7         7 0.0145 \n 8         8 0.00824\n 9         9 0.0155 \n10        10 0.0160 \n# ℹ 40 more rows\n\n\nObserve that the resulting data frame has 50 rows and two columns. The replicate column corresponds to the 50 replicates and the stat column corresponds to the estimated slope for each resample.\n\n\n4. visualize() the results\n\n\n\n\n\n\nFigure 10: Diagram of closing the entire process by visualize()ing the results.\n\n\n\nThe visualize()verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical stat variable’s values. The pipeline of the main infer verbs used for exploring bootstrap distribution results is shown in Figure 11.\n\nvisualize(bootstrap_distribution)\n\n\n\n\n\n\n\n\n\nFigure 11: Bootstrap distribution of slope statistics from 50 bootstrap resamples.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe visualize() function can take many other arguments which we’ll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values.\n\n\nComparing with original workflow: In fact, visualize() is a wrapper function for the ggplot() function that uses a geom_histogram() layer. That’s a lot of fancy language which means that the visualize() function does the same thing as we did previously with ggplot(), just with fewer steps.\n\n\ninfer workflow\n\nvisualize(bootstrap_distribution)\n\n\n\n\nOriginal workflow\n\nggplot(data = bootstrap_distribution,\n       mapping = aes(x = stat)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCareful! It might sound tempting to ditch the ggplot() code altogether now that you know of a simpler approach. The visualize() function only works for a specific case – a data frame containing a distribution of calculate()d statistics.\n\n\nLet’s recap the steps of the infer workflow for constructing a bootstrap distribution and then visualizing it in Figure 12.\n\n\n\n\n\n\nFigure 12: infer package workflow for resampling\n\n\n\n\n\n\n2.3 Virtually resampling 1,000 times\nTo change from resampling 50 times to resampling 1,000 times, only one line of code needs to change—the number of reps. In the code process below, we’ve chained together the entire infer pipeline, connecting the specify, generate, calculate, and visualize steps.\n\nbootstrap_1000 &lt;- births14 |&gt; \n  specify(response = weight, \n          explanatory = mage) |&gt; \n  generate(reps = 1000, \n           type = \"bootstrap\") |&gt; \n  calculate(stat = \"slope\") \n\nvisualize(bootstrap_1000) + \n  labs(x = \"Bootstrap Slope Statistic\", \n       y = \"Bootstrap Distribution of 1,000 Resamples\")\n\n\n\n\n\n\n\nFigure 13: Bootstrap distribution with 1,000 replicates.\n\n\n\n\n\n\n\n\n\n\n\nChanging axis labels\n\n\n\nNotice you can still use the labs() function to change your axis labels. Notice that you still need to connect the axis labels with the plot using a + sign.\n\n\nWhat do you notice about this distribution? What distribution does it resemble? Why is that the case?\n\n\n2.4 Connection to sampling distributions\nThe histogram of sample slopes in Figure 13 is called the bootstrap distribution. The bootstrap distribution is an approximation to the sampling distribution of sample slopes.\nIf you recall back to the last chapter, a sampling distribution was a distribution of statistics from repeatedly sampling from the population. However, a bootstrap distribution is a distribution of statistics from repeatedly resampling from the sample. This may seem rather strange, how can a distribution based entirely on the sample approximate a distribution based on the population? That’s a great question!\nThe bootstrapping process hinges on the belief that the sample is representative of the population. Meaning, there are not systematic differences between the sample and the population. For example, if there were no young mothers in the sample of 1,000 birth records in the births14 dataset. If we have a random sample, however, then on average our sample should look very similar to our population. Meaning, the individuals in the sample can “stand in” for individuals in the population with similar characteristics. So, we can think of repeatedly resampling from our sample as a similar process as sampling from the population.\n\n\n\n\n\n\nNo guaranteed representation\n\n\n\n\n\nThe process of randomly sampling does not guarantee that smaller groups of observations will always appear in the sample. Rather, by randomly sampling from the population, on average the representation of individuals in the sample should reflect the proportion of individuals in the population.\nTo me, this feels rather uncomfortable as a random sample assumes that you can generalize from the sample onto the population from which it was drawn. This means you are generalizing the observations of a few individuals onto the entire population of similar individuals. For example, if you were to collect a random sample of Cal Poly students, it is likely your sample would include very few Black students (as Cal Poly is a predominantly white institution). But, if your sample was random, statistically you could infer from your small sample of Black students onto the entire population of Black students at Cal Poly. That seems kind of iffy to me."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#sec-understanding",
    "href": "weeks/chapters/week7-reading2.html#sec-understanding",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "3 Understanding confidence intervals",
    "text": "3 Understanding confidence intervals\nLet’s start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish!\nIn the births14 investigation, we are trying to estimate the population slope for the relationship between mother’s age and baby’s birth weight (\\(\\beta_1\\)) for all babies born in the US in 2014. Think of the value of \\(\\beta_1\\) as a fish.\nOn the one hand, we could use the appropriate point estimate/sample statistic to estimate \\(\\beta_1\\), which we saw in Table 1 is the sample slope \\(b_1\\). Based on our sample of 1000 birth records, the sample slope was 0.014. Think of using this value as “fishing with a spear.”\nWhat would “fishing with a net” correspond to? Look at the bootstrap distribution in Figure 13 once more. Between which values would you say that “most” sample slopes lie? While this question is somewhat subjective, saying that most sample slopes lie between 0 and 0.03 would not be unreasonable. Think of this interval as the “net.”\nWhat we’ve just illustrated is the concept of a confidence interval, which I’ll abbreviate with “CI” throughout this chapter. As opposed to a point estimate / sample statistic that estimates the value of an unknown population parameter with a single value, a confidence interval gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates / sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets.\n\n\n\n\n\n\nFigure 14: Analogy of difference between point estimates and confidence intervals.\n\n\n\nOur proposed interval of 0 to 0.03 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the percentile method and the standard error method.\nBoth methods for confidence interval construction share some commonalities. First, they are both constructed from a bootstrap distribution, as you constructed in the previous section and visualized in Figure 13.\nSecond, they both require you to specify the confidence level. Commonly used confidence levels include 90%, 95%, and 99%. All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals.\n\n3.1 Percentile method\nOne method to construct a 95% confidence interval is to use the middle 95% of values of the bootstrap distribution. We can do this by computing the 2.5th and 97.5th percentiles, which are 0.0002 and 0.0281, respectively. This is known as the percentile method for constructing confidence intervals.\nFor now, let’s focus only on the concepts behind a percentile method constructed confidence interval; we’ll show you the code that computes these values in the next section.\nLet’s mark these percentiles on the bootstrap distribution with vertical lines in Figure 15. About 95% of the slope variable values in bootstrap_1000 fall between 0.0002 and 0.0281, with 2.5% to the left of the leftmost line and 2.5% to the right of the rightmost line.\n\n\n\n\n\n\n\n\nFigure 15: Percentile method 95% confidence interval. Interval endpoints marked by vertical lines.\n\n\n\n\n\n\n\n3.2 Standard error method\nRecall in the last chapter we saw that if a numerical variable follows a normal distribution, or, in other words, the histogram of this variable is bell-shaped, then roughly 95% of values fall between \\(\\pm\\) 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure 13 is normally shaped, let’s use this fact about normal distributions to construct a confidence interval in a different way.\nFirst, recall the bootstrap distribution has a mean equal to 0.014. This value almost coincides exactly with the value of the sample slope \\(b_1\\) of our original 1000 birth records 0.014. Second, let’s compute the standard deviation of the bootstrap distribution using the bootstrap slope statistics stored in the stat column of the bootstrap_1000 data frame:\n\nbootstrap_1000 %&gt;% \n  summarize(SE = sd(stat))\n\n# A tibble: 1 × 1\n       SE\n    &lt;dbl&gt;\n1 0.00722\n\n\nWhat is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution. Thus, the variability of the sampling distribution may be approximated by the variability of the resampling distribution. Recall also that the standard deviation of a sampling distribution has a special name: the standard error. Putting these two facts together, we can say that 0.00722 is an approximation of the standard error of \\(b_1\\).\nTraditional theory-based methodologies for inference also have formulas for standard errors, assuming some conditions are not violated. In this method, we are not using a formula to get our standard error, but using the standard error of the bootstrap distribution. Thus, using our 95% rule of thumb about normal distributions, we can use the following formula to determine the lower and upper endpoints of a 95% confidence interval for \\(\\beta_1\\):\n\\[\nb_1 \\pm 1.96 \\times SE = (b_1 - 1.96 \\times SE, b_1 + 1.96 \\times SE)\n\\]\n\\[\n= (0.014 - 1.96 \\times 0.007, 0.014 + 1.96 \\times 0.007)\n\\]\n\\[\n= (0.00028, 0.02772)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nTo use the bootstrap standard error in this formula the bootstrap distribution must be bell shaped and symmetric. This is often the case with bootstrap distributions, especially those in which the original distribution of the sample is not highly skewed, however it is not always the case. Next week, we’ll go deeper into the explorations of model conditions.\n\n\nLet’s now add the SE method confidence interval with dashed lines in Figure 16.\n\n\n\n\n\n\n\n\nFigure 16: Comparing two 95% confidence interval methods.\n\n\n\n\n\nWe see that both methods produce nearly identical 95% confidence intervals for \\(\\beta_1\\) with the percentile method yielding \\((0.0002, 0.0281)\\) while the standard error method produces \\((0.0001, 0.0284)\\).\nNow that we’ve introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let’s explore the code that allows us to construct them."
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#constructing-confidence-intervals",
    "href": "weeks/chapters/week7-reading2.html#constructing-confidence-intervals",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "4 Constructing confidence intervals",
    "text": "4 Constructing confidence intervals\nNow that we’ve covered two methods for obtaining confidence intervals, let’s now check out the infer package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the infer package!\n\nPercentile method with infer\nRecall the percentile method for constructing 95% confidence intervals we introduced in Section 3.1. This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95% of the values of the sample mean in the bootstrap distribution.\nWe can compute the 95% confidence interval using the get_confidence_interval() function function from the infer package. This function takes three arguments (inputs):\n\na data frame containing calculate()d statistics (e.g., slope statistics)\na confidence level (e.g., 0.95 or 0.99)\nthe type of interval to be made – \"percentile\" or \"se\"\n\n\npercentile_ci &lt;- get_confidence_interval(bootstrap_distribution, \n                                         level = 0.95, \n                                         type = \"percentile\")\npercentile_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.00185   0.0285\n\n\nAlternatively, we can visualize this interval by adding a shade_confidence_interval() layer to the visualize() step. This step requires the endpoints of the interval to be specified, so the previous step must come before the following step.\n\nvisualize(bootstrap_1000) + \n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\nFigure 17: Percentile method 95% confidence interval shaded corresponding to potential values.\n\n\n\n\n\nObserve in Figure 17 that 95% of the sample slopes stored in the stat variable in bootstrap_1000 fall between the two endpoints marked with the darker lines, with 2.5% of the sample means to the left of the shaded area and 2.5% of the sample means to the right.\n\n\n\n\n\n\nTip\n\n\n\nThe default colors for shade_confidence_interval() are color = \"mediumaquamarine\" and fill = \"turquoise\". You can, however, change these colors if you wish!\n\nvisualize(bootstrap_distribution) + \n  shade_ci(endpoints = percentile_ci, color = \"hotpink\", fill = \"khaki\")\n\n\n\n\n\nStandard error method with infer\nRecall the standard error method for constructing 95% confidence intervals we introduced in Section 3.2. From properties of the Normal Distribution, we know that for any distribution that is bell shaped and symmetric, roughly 95% of the values lie within two standard deviations of the mean. Recall that in the case of the sampling distribution, the standard deviation has a special name: the standard error. Moreover, remember that the bootstrap distribution provides us with a robust estimate of the standard error of the sampling distribution.\nSo in our case, 95% of values of the bootstrap distribution will lie within \\(\\pm 1.96\\) standard errors of \\(b_1\\). Thus, a 95% confidence interval is\n\\[\nb_a \\pm 1.96 \\times SE = (b_1 - 1.96 \\times SE, b_1 + 1.96 \\times SE)\n\\]\nComputation of the 95% confidence interval can once again be done by inputting the the bootstrap_1000 data frame we created into the get_confidence_interval() function, using roughly the same code as before. There are two components, however, that must change:\n\nwe need to set the type argument to be \"se\"\nwe need to specify where the confidence interval should be centered – the point_estimate\n\nTo calculate the point estimate, we need one additional step before calculating our confidence interval. Rather than using the output from the lm() function like we did in Table 1, let’s obtain our observed slope statistic using the infer pipeline! This process should look familiar, in fact the only part that has changed is we no longer have a generate() step. We are not generating any resamples, so that step is no longer needed. All we need to do is specify() our variables and calculate() the \"slope\" statistic.\n\nobs_slope &lt;- births14 |&gt; \n  specify(response = weight, \n          explanatory = mage) |&gt; \n  calculate(stat = \"slope\")\n\nstandard_error_ci &lt;- get_confidence_interval(bootstrap_1000, \n                                             type = \"se\", \n                                             point_estimate = obs_slope, \n                                             level = 0.95)\nstandard_error_ci\n\n# A tibble: 1 × 2\n   lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;\n1 0.0000947   0.0284\n\n\nIf we would like to visualize this interval, we can once again add a shade_confidence_interval() layer to our plot. However, when visualizing an interval constructed using the SE method, we need to declare the endpoints of the interval, as seen in Figure 18.\n\nvisualize(bootstrap_1000) + \n  shade_confidence_interval(endpoints = standard_error_ci)\n\n\n\n\n\n\n\n\n\nFigure 18: Standard-error-method 95% confidence interval.\n\n\n\n\n\nAs noted in Section 3.2, both methods produce similar confidence intervals:\n\n\n\n\nTable 5: Comparison of 95% confidence intervals between percentile and SE methods.\n\n\n\n\n\n\ntype\n2.5%\n97.5%\n\n\n\n\nPercentile\n-0.0018499\n0.0285359\n\n\nSE\n0.0000947\n0.0283909"
  },
  {
    "objectID": "weeks/chapters/week7-reading2.html#interpreting-confidence-intervals",
    "href": "weeks/chapters/week7-reading2.html#interpreting-confidence-intervals",
    "title": "Week 7 – Confidence Intervals for the Slope",
    "section": "5 Interpreting confidence intervals",
    "text": "5 Interpreting confidence intervals\nNow that we’ve shown you how to construct confidence intervals using a sample drawn from a population, let’s now focus on how to interpret their effectiveness. The effectiveness of a confidence interval is judged by whether or not it contains the true value of the population parameter. Going back to our fishing analogy in Section 3, this is like asking, “Did our net capture the fish?”.\nSo, for example, does our percentile-based confidence interval of (-0.0018, 0.0285) “capture” the slope of the relationship between mother’s age and baby’s birth weight (\\(\\beta_1\\)) for all babies born in the US in 2014? Alas, we’ll never know, because we don’t know what the true value of \\(\\beta_1\\) is. After all, we’re sampling to estimate it!\nIn order to interpret a confidence interval’s effectiveness, we need to know what the value of the population parameter is. That way we can say whether or not a confidence interval “captured” this value. In the case of births14 dataset, we have the ability to go find the original dataset, housing the birth records of every baby born in the US in 2014. I did so, and found that the slope for the relationship between mother’s age and baby’s birth weight (\\(\\beta_1\\)) for all babies born in the US in 2014 was \\(\\beta_1 = 0.0007\\).\n\n5.1 Did our interval(s) capture \\(\\beta_1\\)?\nNow that we know the value of \\(\\beta_1\\), we can see if the intervals we constructed previously do in fact contain this parameter. The percentile interval of (-0.0018, 0.0285) contains \\(\\beta_1 = 0.0007\\) and the SE interval of 0.0001, 0.0284) also contains \\(\\beta_1 = 0.0007\\). Will our 95% confidence intervals always contain \\(\\beta_1\\)? Let’s see!\n\n\n5.2 Constructing more intervals\nLet’s construct 50 more confidence intervals for \\(\\beta_1\\) using the infer workflow we learned before. We can then compare how many of these confidence intervals “captured” the true value of \\(\\beta_1\\), which we know to be 0.0007. That is to say, “Did the net capture the fish?”.\nIn the code below, we create a second bootstrap distribution, also with 1,000 bootstrap resamples. Then, using this second bootstrap distribution, we use the percentile method to calculate a 95% confidence interval.\n\n\n\n\n\n\nChoice of method\n\n\n\nI’ve chosen to use the percentile method as we saw that the intervals had similar results and the percentile method has fewer steps. So, I’m choosing to be efficient! However, since we saw the bootstrap distribution was bell shaped and symmetric, either method would work.\n\n\n\nbootstrap_1000_v2 &lt;- births14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 1000, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n  \npercentile_ci_2 &lt;- get_confidence_interval(bootstrap_1000_v2, \n                                         level = 0.95)\npercentile_ci_2\n\n# A tibble: 1 × 2\n    lower_ci upper_ci\n       &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.0000326   0.0285\n\n\nNotice this interval, also contains \\(\\beta_1\\). Let’s do this again…\n\nbootstrap_1000_v3 &lt;- births14 %&gt;% \n  specify(response = weight, \n          explanatory = mage) %&gt;% \n  generate(reps = 1000, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n  \npercentile_ci_3 &lt;- get_confidence_interval(bootstrap_1000_v3, \n                                         level = 0.95)\npercentile_ci_3\n\n# A tibble: 1 × 2\n   lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.000133   0.0285\n\n\nAgain, our interval contains \\(\\beta_1\\). Let’s scale this up and look at 50 different intervals constructed using this method. Figure 19 visualizes these 50 intervals, where:\n\nthe true value of \\(\\beta_1 = 0.0007\\) is marked with a red, vertical line\neach of the intervals is marked with a horizontal line\n\nIf the interval contained \\(\\beta_1 = 0.0007\\), the line is colored grey. If the interval did not contain \\(\\beta_1 = 0.0007\\), the line is colored black.\n\n\n\n\n\n\n\n\nFigure 19: 50 percentile-based 95% confidence intervals for the true slope for the relationship between mother’s age and baby’s birth weight, for all babies born in the US in 2014.\n\n\n\n\n\nOf the 50 95% confidence intervals, 46 of them captured the true value of \\(\\beta_1 = 0.0007\\), whereas 4 of them didn’t. In other words, 46 of our nets caught the fish, and 4 of our nets didn’t.\nThis is where the “95% confidence level” comes into play: for every 50 95% confidence intervals constructed, we expect that about 95% of them will capture \\(\\beta_1\\) and 5% of them won’t. That’s a bit tricky for 50, as 95% of 50 is 47.5. Technically, we can only have either 47 or 48 intervals contain \\(\\beta_1\\), but this should reinforce that the process is not exact. We will not always get 95% of the intervals containing \\(\\beta_1\\), but on average we expect that 95% of them should.\n\n\n5.3 Connection to the empiracle rule\nYou might wonder, why do only 5% of the 95% confidence intervals miss the parameter they are estimating. This reasoning goes back to what you read on the empiracle rule for bell-shaped and symmetric curves. Namely, the rule of thumb that 95% of values will lie within approximately 2 standard deviations of the center of the distribution. The center of a sampling distribution is the value of the parameter. For our example, this value is \\(\\beta_1 = 0.0007\\), the value of the slope for the relationship between mother’s age and baby’s birth weight for all babies born in the US in 2014.\nFigure 20 displays the sampling distribution of \\(b_1\\), centered at \\(\\beta_1 = 0.0007\\). Because our sampling distribution is bell-shaped and symmetric, we know that 95% of the slope statistics should fall within 2 standard errors of the center, as noted by the two dashed lines. That means, only 5% of the slope statistics should fall outside this interval. Our sample slope statistic (\\(b_1\\)) is always the center of our confidence interval. So, if our sample slope statistic is greater than 2 standard deviations from the center of the sampling distribution, its confidence interval will not contain \\(\\beta_1\\).\n\n\n\n\n\n\n\n\nFigure 20: Rules of thumb about areas under normal curves.\n\n\n\n\n\n\n\n5.4 Precise and shorthand interpretation\nLet’s return our attention to 95% confidence intervals. The precise and mathematically correct interpretation of a 95% confidence interval is a little long-winded:\n\nPrecise interpretation: If we repeated our sampling procedure a large number of times, we expect about 95% of the resulting confidence intervals to capture the value of the population parameter.\n\nThis is what we observed in Figure 19. Our confidence interval construction procedure is 95% reliable. That is to say, we can expect our confidence intervals to include the true population parameter about 95% of the time.\nA common but incorrect interpretation is: “There is a 95% probability that the confidence interval contains \\(\\beta_1\\).” Looking at Figure 19, each of the confidence intervals either does or doesn’t contain \\(\\beta_1\\). In other words, the probability is either 100% or 0%.\nSo if the 95% confidence level only relates to the reliability of the confidence interval construction procedure and not to a given confidence interval itself, what insight can be derived from a given confidence interval? Loosely speaking, we can think of these intervals as our “best guess” of a plausible range of values for the slope of the relationship between mother’s age and baby’s birth weight \\(\\beta_1\\) of all babies born in the US in 2014. It is typical to use the following shorthand summary of the precise interpretation:\n\nWe are 95% “confident” that a 95% confidence interval captures the value of the population parameter. We use quotation marks around “confident” to emphasize that while 95% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. In other words, it’s our best net.\n\nSo returning to the first confidence interval we calculated, we would interpret this interval as “We are 95% ‘confident’ that the slope of the relationship between mother’s age and baby’s birth weight \\(\\beta_1\\) of all babies born in the US in 2014 is between -0.0018 and 0.0285.\n\n\n5.5 Width of confidence intervals\nNow that we know how to interpret confidence intervals, let’s go over some factors that determine their width.\n\nImpact of confidence level\nOne factor that determines confidence interval widths is the pre-specified confidence level. For example, we used 95% confidence intervals for this reading, but we could have equally chosen 85% intervals. A 95% confidence interval created using the percentile method contains 95% of the values in the bootstrap distribution. An 85% confidence interval would only contain 85% of these values, so it would be a narrower confidence interval. The quantification of the confidence level should match what many expect of the word “confident.” In order to be more confident in our best guess of a range of values, we need to widen the range of values.\nTo elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul’s temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50°F - 95°F (10°C - 35°C). However, if we wanted a temperature range we were absolutely confident about, we would need to widen it.\nWe need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32°F - 110°F (0°C - 43°C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70°F - 85°F (21°C - 30°C).\nSo in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to be more confident, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level.\n\nThe moral of the story is: Higher confidence levels tend to produce wider confidence intervals.\n\n\n\nImpact of sample size\nPulling from what we learned about the Central Limit Theorem at the end of the last reading, we know that variability decreases when sample size increases. We quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the standard error. So as the sample size increases, the standard error decreases.\nConnecting to what you learned in this reading, if we have a large sample size, the spread of the bootstrap distribution will be smaller, which in turn leads to smaller estimated standard errors, and narrower confidence intervals. In Section 3.2 you saw directly how the standard error is used when calculating a confidence interval."
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html",
    "href": "weeks/chapters/week-8-reading-mlr.html",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "",
    "text": "Recall, the goal of a hypothesis test is to compare what we observed in our data to what we would have expected to occur if the null hypothesis was true. In the case of simple / basic regression, our null hypothesis assumes that there is no linear relationship between our explanatory variable and our response variable. In your first reading, you learned methods for generating statistics that could have happened if the null hypothesis was true, which we can compare our observed statistic with.\nWhy did we generate these statistics that could have happened if the null hypothesis was true? Well, this comes back to the idea of sampling distributions. If you remember, a sampling distribution is a distribution of statistics from taking repeated samples from the population. When we first learned about sampling distributions in Week 7, our sampling distribution made no assumptions about the samples that were being drawn (other than being representative). However, this week our sampling distributions take on a slightly different flavor, they assume the null hypothesis is true. So, our repeated samples are from an alternative universe where the null hypothesis is true."
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html#hypothesis-testing-refresher",
    "href": "weeks/chapters/week-8-reading-mlr.html#hypothesis-testing-refresher",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "",
    "text": "Recall, the goal of a hypothesis test is to compare what we observed in our data to what we would have expected to occur if the null hypothesis was true. In the case of simple / basic regression, our null hypothesis assumes that there is no linear relationship between our explanatory variable and our response variable. In your first reading, you learned methods for generating statistics that could have happened if the null hypothesis was true, which we can compare our observed statistic with.\nWhy did we generate these statistics that could have happened if the null hypothesis was true? Well, this comes back to the idea of sampling distributions. If you remember, a sampling distribution is a distribution of statistics from taking repeated samples from the population. When we first learned about sampling distributions in Week 7, our sampling distribution made no assumptions about the samples that were being drawn (other than being representative). However, this week our sampling distributions take on a slightly different flavor, they assume the null hypothesis is true. So, our repeated samples are from an alternative universe where the null hypothesis is true."
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html#how-does-the-relationship-between-bill-length-and-body-mass-change-based-on-a-penguins-sex",
    "href": "weeks/chapters/week-8-reading-mlr.html#how-does-the-relationship-between-bill-length-and-body-mass-change-based-on-a-penguins-sex",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "1.1 How does the relationship between bill length and body mass change based on a penguin’s sex?",
    "text": "1.1 How does the relationship between bill length and body mass change based on a penguin’s sex?\nThis research question involves one numerical explanatory variable (body mass) and one categorical explanatory variable (penguin sex). Notice the research question asks how the relationship between body mass and bill length changes for male versus female penguins, which is addressed with a different slopes (interaction) multiple linear regression.\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nBased on the scatterplot above, it appears the slopes for female penguins and male penguins are different. Let’s investigate this with a hypothesis test!"
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html#simulation-based-methods-permutation-distribution",
    "href": "weeks/chapters/week-8-reading-mlr.html#simulation-based-methods-permutation-distribution",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "1.2 Simulation-based Methods (Permutation Distribution)",
    "text": "1.2 Simulation-based Methods (Permutation Distribution)\nFor this question, I am interested in testing if the relationship between body mass and bill length (slope) is different for male versus female penguins. If I were to write my hypotheses down, they would be:\n\\(H_0\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is the same regardless of their sex\n\\(H_A\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is different for male and female penguins\n\nUsing Cards\nWhen generating new datasets that could have happened if the null hypothesis was true, it helps me to think about what actions I am taking with my dataset. As you’ve seen before, I like to think about this using cards.\n\nSuppose I have 333 cards, one card per penguin.\n\nOn each card, I write the penguin’s (bill_length_mm, body_mass_g, sex).\n\nIf I think the relationship between a penguin’s body mass and their bill length is the same regardless of the penguin’s sex (the null hypothesis), then I tear the sex label off each card.\n\nThis leaves me with 333 cards with (bill_length_mm, body_mass_g) measurements and 333 cards with different sex labels. Technically, I would have 165 cards saying female and 168 cards saying male.\n\nI would then randomly draw one card from the (bill_length_mm, body_mass_g) pile and one card from the sex pile and staple them together.\n\nI would continue this process until every (bill_length_mm, body_mass_g) card had a new value for sex.\n\n\nMy resulting dataset would look something like this:\n\n\n\n\n\n\n\n\nbill_length_mm\nbody_mass_g\nsex\nshuffled_sex\n\n\n\n\n36.5\n2850\nfemale\nmale\n\n\n46.4\n4700\nfemale\nfemale\n\n\n37.6\n3750\nmale\nfemale\n\n\n39.0\n3050\nfemale\nmale\n\n\n42.8\n4700\nfemale\nmale\n\n\n52.7\n3725\nmale\nfemale\n\n\n36.6\n3475\nfemale\nmale\n\n\n50.7\n4050\nmale\nmale\n\n\n46.0\n4150\nfemale\nfemale\n\n\n43.5\n4650\nfemale\nmale\n\n\n\n\n\n\n\nNow we can visualize the relationship between body mass and bill length for our shuffled dataset. Notice how the slopes for male and female penguins in the plot below are much more similar than the original plot? Why do you think that is? It’s because we assumed these relationships were the same (the null hypothesis) when we generated this permuted dataset!\n\n\n\n\n\n\n\n\n\n\n\nTranslating into the infer pipeline\nTo generate lots of these permuted datasets, we are going to use the infer package. The process for using these tools for a multiple linear regression are similar, but slightly different from what you saw with a simple linear regression. Namely, we specify() our model in a slightly different way, and we use the fit() function instead of the calculate() function.\n\n\nStep 1: Fitting our Model\nSimilar to a basic regression, in our first step we need to obtain our observed statistic. To do this, we specify() the model we are interested in, and then tell infer to fit() this model. The code below does exactly that!\n\nobserved_fit &lt;- penguins_clean %&gt;%\n  specify(bill_length_mm ~ body_mass_g * sex) %&gt;%\n  fit()\n\n\n\n\n\n\n\nSpecifying Your Model\n\n\n\nWhen we have multiple explanatory variables, we need to use the “tilde” (~) syntax to specify our model. Keep in mind, we are interested in a different slopes multiple regression, so we are using a * to separate our two explanatory variables. If we were interested in a parallel slopes regression, we would us a + instead!\n\n\n\n\nStep 2: Finding our Observed Statistic\nNow that we’ve fit our model, we need to figure out which of these coefficients is our observed statistic. Which do you think it is?\n\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\nintercept\n25.571381438\n\n\nbody_mass_g\n0.004278721\n\n\nsexmale\n5.516061110\n\n\nbody_mass_g:sexmale\n-0.001030075\n\n\n\n\n\n\n\nSince we’re interested in how the slope between body mass and bill length differs between male and female penguins, our statistic is associated with the body_mass_g:sexmale row of this table.\n\n\nStep 3: Generating Permuted Fits\nNow that we have our observed statistic, we need to generate datasets that could have happened if the null hypothesis was true. Similar to basic regression, we obtain these permuted datasets by adding two steps:\n\nwe tell infer our hypothesise() – the relationship between a penguin’s body mass and bill length is \"independent\" of its sex\nwe stipulate how many of these new datasets we want (reps) and the method infer should use when generating these datasets (\"permute\")\n\n\nnull_fits &lt;- penguins_clean %&gt;%\n  specify(bill_length_mm ~ body_mass_g * sex) %&gt;%\n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;%\n  fit()\n\n\n\nStep 4: Visualizing our Null Distribution\nWe’ve used the visualize() function before, to obtain a histogram of our permuted statistics. With a multiple linear regression, the visualize() function generates a histogram for every coefficient. Since we are only interested in the body_mass_g:sexmale coefficient, I’ve pulled that one out:\n\nobs_offset &lt;- observed_fit %&gt;% \n  filter(term == \"body_mass_g:sexmale\") %&gt;% \n  pull(estimate)\n\nnull_fits %&gt;% \n  filter(term == \"body_mass_g:sexmale\") %&gt;% \n  ggplot(mapping = aes(x = estimate)) +\n  geom_histogram(color = \"white\", binwidth = 0.0005) +\n  geom_vline(xintercept = obs_offset, color = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nOnce again, our null distribution is centered at 0. In this context, 0 represents the offset when going from female penguins to male penguins, meaning the relationship between bill length and body mass does not change when going from female penguins (the baseline group) to male penguins.\nOur observed statistic (-0.0010301) is noted in red. The statistic falls somewhat on the edge of the distribution, but not far into the tail. This location suggests our observed statistic could happen somewhat frequently if the null hypothesis was true.\n\n\nStep 4: Calculating our p-value\nOur final coding step is to calculate our p-value. To do this, we need three pieces of information:\n\nstatistics that would have happened if the null was true\nour observed statistic\nthe direction(s) that should be used when calculating the p-value\n\nThe direction of the hypothesis test is determined by the alternative hypothesis. For our research question, we are testing if there is a difference in the relationship between bill length and body mass, which aligns with a \"two-sided\" alternative hypothesis.\n\nget_p_value(null_fits,\n            obs_stat = observed_fit,\n            direction = \"two-sided\") \n\n\n\n\n\n\n\n\n\nterm\np_value\n\n\n\n\nbody_mass_g\n0.000\n\n\nbody_mass_g:sexmale\n0.220\n\n\nintercept\n0.000\n\n\nsexmale\n0.116\n\n\n\n\n\n\n\nOnce again, our statistic of interest is body_mass_g:sexmale. The p-value for this statistic is 0.22, telling us that 220 permuted statistics (out of 1,000) were as large or larger than what we observed in our data.\n\n\nStep 5: Make a Decision & Reaching a Conclusion\nFor an \\(\\alpha\\) of 0.1 (or 0.05 or 0.01), we would decide to fail to reject the null hypothesis (that he relationship between a penguin’s body mass and their bill length is the same regardless of their sex).\nTherefore, our data provided insufficient evidence that the relationship between a penguin’s body mass and their bill length differs based on the sex of the penguin."
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html#theory-based-methods-t-distribution",
    "href": "weeks/chapters/week-8-reading-mlr.html#theory-based-methods-t-distribution",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "1.3 Theory-based Methods (t-distribution)",
    "text": "1.3 Theory-based Methods (t-distribution)\nSimilar to what you saw for a simple linear regression, if our data / model do not violate certain conditions, a \\(t\\)-distribution can be used as a reasonable approximation for the permutation distribution we just found. These conditions are:\n\nLinear relationship between our explanatory and response variable\nIndependence of observations\nNormality of residuals\nEqual variance of residuals\n\nBased on what we saw in Figure 1, it appears that there is a moderate linear relationship between body mass and bill length. From the description of how the data were collected (LTER and Gorman 2020), it doesn’t seem like the same penguin could be captured multiple times. I also know that a penguin could only belong to one of these two groups (male, female). However, it is possible penguins who were captured could be genetically related (from the same family), so there could be possible relationships between the observations.\nConditions three and four involve the residuals of the regression model, visualized in the two plots below. In Figure 2 (a) we see that the distribution of residuals is unimodal and fairly symmetric. While it does appear that there is a slight right skew, I am not super concerned as there are very few observations in the right tail. In Figure 2 (b) we see that the vertical spread of the residuals (on the y-axis) appears similar across the fitted / predicted values of bill length, with most of the residuals falling between +10 and -10.\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatterplot of residuals versus fitted / predicted values\n\n\n\n\n\n\n\nFigure 2: Figures\n\n\n\nAs I did not find that any of these conditions were violated, it seems reasonable to use the \\(t\\)-distribution as an approximation for the permutation distribution.\n\nUsing the \\(t\\)-distribution\nIf you continue along in your R adventures, you will find that the majority of functions built-in to R use parametric / theory-based methods to obtain p-values and confidence intervals. In fact, our familiar friend the get_regression_table() function does just that!\n\nsex_lm &lt;- lm(bill_length_mm ~ body_mass_g * sex, \n             data = penguins_clean)\n\nget_regression_table(sex_lm)\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n25.571\n2.015\n12.693\n0.000\n21.608\n29.534\n\n\nbody_mass_g\n0.004\n0.001\n8.323\n0.000\n0.003\n0.005\n\n\nsex: male\n5.516\n2.830\n1.949\n0.052\n-0.051\n11.083\n\n\nbody_mass_g:sexmale\n-0.001\n0.001\n-1.536\n0.126\n-0.002\n0.000\n\n\n\n\n\n\n\nRemember, for this research question we are interested in the body_mass_g:sexmale coefficient. Looking at the row of the regression table associated with body_mass_g:sexmale, we first see two things we’ve seen before (1) our observed statistic of -0.0010301, and (2) an estimate of the variability of that statistic (standard error). The next two columns are new!\nThe statistic column represents the value of the \\(t\\)-statistic. This statistic is calculated as \\(\\frac{\\text{estimate}}{\\text{SE of the estimate}}\\). In general, you can think of a \\(t\\)-statistic as a way to standardize how “surprising” an estimate is, if the null hypothesis was true. In most cases, values larger than 2 are thought to indicate statistics that would be very “unusual” if the null hypothesis was true.\nFinally, the p_value column finds where the \\(4\\)-statistic falls on the \\(t\\)-distribution and calculates its resulting p-value. By default, the p-value is calculated using both sides (similar to using \"two-sided\" before). The \\(t\\)-distribution returns a p-value of 0.126. While this p-value leads to a similar decision and conclusion as the p-value we obtained from the permutation distribution, it is about 0.1 lower than our previous p-value. Why do you think that is?"
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html#how-does-the-relationship-between-bill-length-and-body-mass-change-based-on-a-penguins-species",
    "href": "weeks/chapters/week-8-reading-mlr.html#how-does-the-relationship-between-bill-length-and-body-mass-change-based-on-a-penguins-species",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "1.4 How does the relationship between bill length and body mass change based on a penguin’s species?",
    "text": "1.4 How does the relationship between bill length and body mass change based on a penguin’s species?\nLet’s change our investigation of the relationship between a penguin’s bill length and body mass to focus on differences between species of penguins.\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nWe’ve outlined two tools we can use to carry out this analysis, (1) a permutation distribution, and (2) a \\(t\\)-distribution. Since it is easier (less coding) to use a \\(t\\)-distribution, I generally start by inspecting my regression conditions to see if this would be a reasonable approach.\nLooking at Figure 3, it appears that all three species have a linear relationship between body mass and bill length. Similar to our previous discussion, without knowing more about the possibility of sampling penguins who are genetically related, we don’t have evidence that the independence condition is violated.\nSo, we turn to our residual analysis. In Figure 4 (a), we see that the distribution of residuals is unimodal and symmetric, so the normality condition is not violated. Finally, Figure 4 (b) suggests the variance in the residuals is fairly similar across the predicted bill lengths, going from about -5 to +5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatterplot of residuals versus fitted / predicted values\n\n\n\n\n\n\n\nFigure 4: Figures\n\n\n\nAs I did not find that any of these conditions were violated, it seems reasonable to use the \\(t\\)-distribution as an approximation for the permutation distribution.\n\nObtaining our p-value\nAlrighty, let’s use the get_regression_table() to get our p-value for this test!\n\nspecies_lm &lt;- lm(bill_length_mm ~ body_mass_g * species, \n             data = penguins_clean)\n\nget_regression_table(species_lm)\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n27.113\n1.632\n16.609\n0.000\n23.902\n30.324\n\n\nbody_mass_g\n0.003\n0.000\n7.228\n0.000\n0.002\n0.004\n\n\nspecies: Chinstrap\n5.061\n3.310\n1.529\n0.127\n-1.451\n11.573\n\n\nspecies: Gentoo\n-0.575\n2.794\n-0.206\n0.837\n-6.072\n4.922\n\n\nbody_mass_g:speciesChinstrap\n0.001\n0.001\n1.475\n0.141\n0.000\n0.003\n\n\nbody_mass_g:speciesGentoo\n0.001\n0.001\n1.558\n0.120\n0.000\n0.002\n\n\n\n\n\n\n\nHmmm…🤔. Last time we had one line that we were interested in (body_mass_g:sexmale), but now we have two lines. What gives?\n\n\nMultiple Hypothesis Tests\nTechnically, each of these lines is testing if that group has a different slope than the baseline group. Meaning, the body_mass_g:speciesChinstrap line is running the following hypothesis test:\n\\(H_0\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is the same for Adelie and Chinstrap penguins\n\\(H_A\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is different for Adelie and Chinstrap penguins\nSimilarly, the body_mass_g:speciesGentoo line is testing:\n\\(H_0\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is the same for Adelie and Gentoo penguins\n\\(H_A\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is different for Adelie and Gentoo penguins\nThis doesn’t quite seem like what I want. I want to test if all of the species have the same relationship!\n\n\nTesting 3+ Groups\nWhen I’m in the scenario where I have three or more groups, a \\(t\\)-distribution isn’t the best tool for the job since it only allows us to test each group relative to the baseline. An ANOVA (analysis of variance), however, can test for differences among three or more groups!\nIf you’ve never encountered an ANOVA before, it has a large number of similarities to what we saw with the \\(t\\)-distribution. There are, however, two main differences. First, an ANOVA has slightly different hypotheses. Since we have three or more groups, our hypotheses are no longer in terms of comparing one group with another. Instead, our hypotheses test for similarities / differences among all of the groups. Specifically, we have the following hypotheses:\n\\(H_0\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is the same regardless of their species\n\\(H_A\\): For penguins in the Palmer Archipelago, the relationship between a penguin’s body mass and their bill length is different for at least one species\n\n\n\n\n\n\nMathematical Negation\n\n\n\nAn alternative hypothesis is always the negation of the null hypothesis. In an ANOVA, the null hypothesis says “every group is the same.” The negation of “every group is the same” is not “every group is different.” The negation of “every group is the same” is “at least one group is different.”\n\n\n\n\nANOVA\nTo carry out an ANOVA, we use the anova() function in R. The output of the anova() function is messy, so I’m piping the result into the tidy() function (from the broom package) to clean it up a bit!\n\nanova(species_lm) %&gt;% \n  tidy()\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nbody_mass_g\n1\n3449.82317\n3449.823171\n591.920719\n0.0000000\n\n\nspecies\n2\n4553.33934\n2276.669668\n390.631021\n0.0000000\n\n\nbody_mass_g:species\n2\n19.92386\n9.961928\n1.709268\n0.1826114\n\n\nResiduals\n327\n1905.81634\n5.828185\nNA\nNA\n\n\n\n\n\n\n\nIn the ANOVA table, we are interested in the body_mass_g:species line. Specifically, we are interested in the statistic and p.value columns. We will learn more about the statistic column when we dive deeper into ANOVA next week, so for right now know that it is a measure of how much additional variation in bill length is being explained by having different slopes for each species.\nBased on the p.value column, it doesn’t seem like there is evidence that the relationship between a penguin’s body mass and their bill length is different for at least one species."
  },
  {
    "objectID": "weeks/chapters/week-8-reading-mlr.html#how-do-body-mass-and-flipper-length-influence-a-penguins-bill-length",
    "href": "weeks/chapters/week-8-reading-mlr.html#how-do-body-mass-and-flipper-length-influence-a-penguins-bill-length",
    "title": "Week 8 – Extending to Multiple Linear Regression",
    "section": "1.5 How do body mass and flipper length influence a penguin’s bill length?",
    "text": "1.5 How do body mass and flipper length influence a penguin’s bill length?\nOur multiple linear regressions take take on a variety of flavors, including models with multiple numerical explanatory variables. An example of this type of model is adding flipper length as a second explanatory variable (in addition to body mass) when explaining the length of a penguin’s bill. This type of investigation is similar to what many of you did for your Midterm Project, where you decided if there was evidence that both explanatory variables have a relationship with the response.\nBased on the scatterplot below, it does seem that both body mass and flipper length have a relationship with a penguin’s bill length. As the color of the points gets darker (longer flippers), a penguin’s bill length also increases.\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\nWhich method?\nAs said before, we have two tools we can use (1) a permutation distribution, and (2) a \\(t\\)-distribution. Let’s see which tool seems like the best option.\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatterplot of residuals versus fitted / predicted values\n\n\n\n\n\n\n\nFigure 6: Figures\n\n\n\nLooking at Figure 6 (a) and Figure 6 (b), it appears that both variables have a linear relationship with bill length. Similar to our previous discussion, without knowing more about the possibility of sampling penguins who are genetically related, we don’t have evidence that the independence condition is violated.\nSo, we turn to our residual analysis. In Figure 7 (a), we see that the distribution of residuals is unimodal and symmetric, so the normality condition is not violated. Finally, Figure 7 (b) suggests the variance in the residuals is fairly similar across the predicted bill lengths, going from about -10 to +10.\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatterplot of residuals versus fitted / predicted values\n\n\n\n\n\n\n\nFigure 7: Figures\n\n\n\nAs I did not find that any of these conditions were violated, it seems reasonable to use theory-based methods as an approximation for the permutation distribution.\n\n\nObtaining our p-value\nUnfortunately, when testing for a relationship between each explanatory variable and the response, the get_regression_table() function does not do what we want. What we want is for the p-value for each variable to be conditional on the other variable(s) in the model, but the p-values for variables output from get_regression_table() are tested in the order they are listed. Meaning, if body_mass_g is the first variable listed in the model, then the p-value output is not conditional on flipper_length_mm also being included in the model.\nBut we have a tool that will give us what we want! An ANOVA!!!\n\n\nMultiple Hypothesis Tests\n\nmlr_anova &lt;- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, \n                   data = penguins_clean)\n\nanova(mlr_lm)\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nbody_mass_g\n1\n3449.8232\n3449.82317\n200.48905\n0\n\n\nflipper_length_mm\n1\n800.7561\n800.75612\n46.53654\n0\n\n\nResiduals\n330\n5678.3234\n17.20704\nNA\nNA\n\n\n\n\n\n\n\nTechnically, each of these lines is testing if there is a relationship between the explanatory variable and the response, controlling for the other variable(s) in the model. Meaning, the flipper_length_mm line is running the following hypothesis test:\n\\(H_0\\): For penguins in the Palmer Archipelago, controlling for a penguin’s body mass there is no relationship between a penguin’s flipper length and their bill length\n\\(H_A\\): For penguins in the Palmer Archipelago, controlling for a penguin’s body mass there is a relationship between a penguin’s flipper length and their bill length\nSimilarly, the body_mass_g line is testing:\n\\(H_0\\): For penguins in the Palmer Archipelago, controlling for a penguin’s flipper length there is no relationship between a penguin’s body mass and their bill length\n\\(H_A\\): For penguins in the Palmer Archipelago, controlling for a penguin’s flipper length there is a relationship between a penguin’s body mass and their bill length\nGiven the p-values output from the table, it appears that after controlling for a penguin’s flipper length, there is a relationship between a penguin’s body mass and bill length (p-value &lt; 0.0001). Similarly, there is also evidence that (after controlling for a penguin’s body mass) there is a relationship between the length of a penguin’s flipper and its bill length (p-value &lt; 0.0001)."
  },
  {
    "objectID": "weeks/tutorial/week-10.html",
    "href": "weeks/tutorial/week-10.html",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "A two-way ANOVA extends the one-way ANOVA to situations with two categorical explanatory variables. This new methods allows researchers to simultaneously study two variables that might explain variability in the responses and explore whether the impacts of one explanatory variable change depending on the level of the other explanatory variable.\nIn a clinical trials context, it is well known that certain factors can change the performance of certain drugs. For example, different dosages of a drug might have different benefits or side-effects on men, versus women or children or even for different age groups in adults. When the impact of one factor on the response changes depending on the level of another factor, we say that the two explanatory variables interact.\nIt is also possible for both explanatory variables to be related to differences in the mean responses and not interact. For example, suppose there are differences in how younger and older subjects respond to a drug, and there are differences in how all individuals respond to different dosages of a drug, but the effect of increasing the dosage is the same for both young and old subjects. This is an example of what is called an additive type of model.\nIn general, the world is more complicated than the single factor models we’ve considered, especially in observational studies, so these models allow us to start to handle more realistic situations."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#two-way-anova",
    "href": "weeks/tutorial/week-10.html#two-way-anova",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "A two-way ANOVA extends the one-way ANOVA to situations with two categorical explanatory variables. This new methods allows researchers to simultaneously study two variables that might explain variability in the responses and explore whether the impacts of one explanatory variable change depending on the level of the other explanatory variable.\nIn a clinical trials context, it is well known that certain factors can change the performance of certain drugs. For example, different dosages of a drug might have different benefits or side-effects on men, versus women or children or even for different age groups in adults. When the impact of one factor on the response changes depending on the level of another factor, we say that the two explanatory variables interact.\nIt is also possible for both explanatory variables to be related to differences in the mean responses and not interact. For example, suppose there are differences in how younger and older subjects respond to a drug, and there are differences in how all individuals respond to different dosages of a drug, but the effect of increasing the dosage is the same for both young and old subjects. This is an example of what is called an additive type of model.\nIn general, the world is more complicated than the single factor models we’ve considered, especially in observational studies, so these models allow us to start to handle more realistic situations."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#visualizing-a-two-way-anova",
    "href": "weeks/tutorial/week-10.html#visualizing-a-two-way-anova",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Visualizing a Two-Way ANOVA",
    "text": "Visualizing a Two-Way ANOVA\nThe visualizations we created for a one-way ANOVA are still relevant here, but we need to figure out how to add a second categorical explanatory variable to our plots.\nSimilar to the multivariate plots we’ve talked about previously, there are two main ways to add a second categorical variable to our plots:\n\ncolors\nfacets\n\nWe’ll explore both below!"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section",
    "href": "weeks/tutorial/week-10.html#section",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Below is a plot of the relationship between body mass and species for penguins in the Palmer Archipelago.\nThe year the data were collected was modified to a categorical variable, named year_cat. Change to code below to fill the density ridges with the year_cat variable.\n\n\npenguins %&gt;% \n  ggplot(aes(y = species, x = body_mass_g, fill = ___)) + \n  geom_density_ridges()\n\n\n\n\n\nggplot(aes(y = species, x = body_mass_g, fill = year_cat))"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-1",
    "href": "weeks/tutorial/week-10.html#section-1",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Now, take the same plot and instead of coloring by year_cat use facets to separate the different sampling years.\n\n\npenguins %&gt;% \n  ggplot(aes(y = species, x = body_mass_g)) + \n  geom_density_ridges() + \n  ___\n\n\n\n\nHint: Add ~ year_cat to facet_wrap() to create year facets.\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = species, x = body_mass_g)) + \n  geom_density_ridges() + \n  facet_wrap(~ year_cat)"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#additive-versus-interactive-models",
    "href": "weeks/tutorial/week-10.html#additive-versus-interactive-models",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Additive versus Interactive Models",
    "text": "Additive versus Interactive Models\nAs was mentioned in the Introduction, there are two different types of two-way ANOVA models. Similar to a multiple linear regression, the two explanatory variables could have their own impact on the response (similar to a parallel slopes regression model). Or, the relationship between one explanatory variable and the response could differ based on another explanatory variable (similar to a different slopes regression model).\nThese two types of models are called an additive two-way ANOVA model or an interaction two-way ANOVA model. Similar to how we decided which model to choose in a multiple linear regression, we will use visualizations to guide us."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-2",
    "href": "weeks/tutorial/week-10.html#section-2",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "When deciding if an interaction model is a good fit for the data, we look to see if the relationship between one categorical variable and the response differs based on the level of the other response variable.\nHere, we look at the relationship between year and body_mass_g and see if it differs based on the species of the penguin. To me, the easiest way to assess if this is the case is to fill the density plots with color for the different years:\n\n\n\n\n\n\n\n\n\nNow that I have the plot, I compare the “profile” of the density ridges (the combo of the pink, green, and blue) between the species. If the relationship between year and body_mass_g changed based on the species, then we would see very different profiles. Note, I’m not paying attention to where the profiles are located along the x-axis, I’m simply looking at the density ridges as individual pictures to be compared.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at the profiles above, removed from the context of the plot, does it seem that these profiles very similar? Or does it seem that these are different pictures? I didn’t think so, since it seems like in all three pictures there is about the same overlap between the three colors. However, we can use a two-way ANOVA interaction model to see if my intuition is right."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#conditions-of-a-two-way-anova-model",
    "href": "weeks/tutorial/week-10.html#conditions-of-a-two-way-anova-model",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Conditions of a Two-Way ANOVA Model",
    "text": "Conditions of a Two-Way ANOVA Model\nThe two-way ANOVA model has the same conditions as its one-way counterpart, however, we now have one more variable to construct our groups from.\nIndependence\nFor both categorical variables:\n\nobservations across groups need to be independent\nobservations within each group need to be independent\n\nEqual Variance: the variability of each group is similar to the others.\n\n\n\n\n\n\nIntersectional groups\n\n\n\nThis is an assumption about the groups at the intersection of each categorical variable (e.g. Gentoo penguins captured in 2007). This assumption is fairly robust, but large differences in variability will cause issues.\n\n\nNormal Distribution: the responses of each group need to be approximately Normal.\n\n\n\n\n\n\nIntersectional distributions\n\n\n\nThis is an assumption about the groups at the intersection of each categorical variable (e.g. Gentoo penguins captured in 2007). This assumption is also fairly robust, but influential outliers and the sample sizes of the groups should be noted when assessing."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-3",
    "href": "weeks/tutorial/week-10.html#section-3",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Your turn!"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#fitting-a-two-way-anova-model",
    "href": "weeks/tutorial/week-10.html#fitting-a-two-way-anova-model",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Fitting a Two-Way ANOVA Model",
    "text": "Fitting a Two-Way ANOVA Model\nThe R code to fit a two-way ANOVA model is very similar to a one-way ANOVA model. We use the aov() (analysis of variance) function, but now we will have two categorical explanatory variables.\nFirst, we’ll fit an interaction two-way ANOVA model to see if the relationship between year and body mass differs based on species of penguins.\n\nSimilar to different slopes model, * is the symbol we use to fit an interaction model.\n\n\naov(body_mass_g ~ species * year_cat, \n    data = penguins) %&gt;% \n  tidy()\n\n# A tibble: 4 × 6\n  term                df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species              2 146864214. 73432107.  340.      3.16e-81\n2 year_cat             2     26661.    13330.    0.0618  9.40e- 1\n3 species:year_cat     4    575055.   143764.    0.666   6.16e- 1\n4 Residuals          333  71841768.   215741.   NA      NA       \n\n\n\nWe notice that the interaction line (species:year_cat) has a small F-statistic 0.666 and a large p-value 0.616. This would lead for us to conclude that there is not an interaction between these two variables, or that the relationship between the mean body mass and year does not differ based on penguin species. As suspected!"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-4",
    "href": "weeks/tutorial/week-10.html#section-4",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "An additive model seems like a better choice. Modify the code from the interaction model to fit an additive model instead.\n\n\naov(body_mass_g ~ species * year_cat, data = penguins) %&gt;% \n  tidy()\n\n\n\n\nHint: We used a + sign to fit an additive model with regression!\n\n\n\naov(body_mass_g ~ species + year_cat, data = penguins) %&gt;% \n  tidy()\n\n\n\nBased on the ANOVA table above, what would you conclude for the relationship between body mass and species and the relationship between body mass and sampling year?"
  },
  {
    "objectID": "weeks/tutorial/week-10.html#model-conclusions",
    "href": "weeks/tutorial/week-10.html#model-conclusions",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "Model Conclusions",
    "text": "Model Conclusions\nIn this context, the ANOVA table allows for use to test two hypotheses:\n\nwhether the mean body mass for every year are equal\nwhether the mean body mass for every species are equal\n\nAgain, similar to a multiple regression, the interpretation of these tests is conditional on the other variable in the model."
  },
  {
    "objectID": "weeks/tutorial/week-10.html#section-5",
    "href": "weeks/tutorial/week-10.html#section-5",
    "title": "Two-Way ANOVA – A Brief Introduction",
    "section": "",
    "text": "Based on the ANOVA table below, with a p-value of 0.94 at an \\(\\alpha\\) of 0.05, we would conclude that, after accounting for the species of penguin, there is insufficient evidence that at least one year of capture has a different mean body mass.\n\n\n# A tibble: 3 × 6\n  term         df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 146864214. 73432107.  342.      8.40e-82\n2 year_cat      2     26661.    13330.    0.0620  9.40e- 1\n3 Residuals   337  72416822.   214887.   NA      NA       \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit a screenshot of the final page of this tutorial to the Week 10 R tutorial assignment!"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html",
    "href": "labs/grading-guides/lab-2-grading-guide.html",
    "title": "Lab 2 - Grading Guide",
    "section": "",
    "text": "How large is the nycflights dataset? (i.e. How many rows and columns does it have?)\nSuccess:\n\nUses glimpse() to obtain the size of the dataset\nSize of 32,735 rows and 16 columns\n\nGrowing:\n\nIf no code is present\nIf they flip the rows and columns\nIf they do not provide size of data\n\nFeedback for no code: For Question 1(a), you needed to write code to explore the size of the data and the types of variables.\nFeedback for not using glimpse(): For this course, we prefer you use the tools being taught in the textbook and course materials. What function have you learned in this class which give a preview of a dataset?\nFeedback for no / incorrect dimensions: For Question 1(a), look again at the output of the glimpse() function, how many rows and columns are included in the nycflights dataset?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#a",
    "href": "labs/grading-guides/lab-2-grading-guide.html#a",
    "title": "Lab 2 - Grading Guide",
    "section": "",
    "text": "How large is the nycflights dataset? (i.e. How many rows and columns does it have?)\nSuccess:\n\nUses glimpse() to obtain the size of the dataset\nSize of 32,735 rows and 16 columns\n\nGrowing:\n\nIf no code is present\nIf they flip the rows and columns\nIf they do not provide size of data\n\nFeedback for no code: For Question 1(a), you needed to write code to explore the size of the data and the types of variables.\nFeedback for not using glimpse(): For this course, we prefer you use the tools being taught in the textbook and course materials. What function have you learned in this class which give a preview of a dataset?\nFeedback for no / incorrect dimensions: For Question 1(a), look again at the output of the glimpse() function, how many rows and columns are included in the nycflights dataset?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#b",
    "href": "labs/grading-guides/lab-2-grading-guide.html#b",
    "title": "Lab 2 - Grading Guide",
    "section": "1 (b)",
    "text": "1 (b)\nAre there numerical variables in the dataset? If so, what are their names?\nSuccess:\n\nLists all variables with an int or dbl data type:\n\nyear\nmonth (many may miss)\nday\ndep_time\ndep_delay\narr_time\narr_delay\nflight (many may miss)\nair_time\ndistance\nhour\nminute\n\nPermitted to miss 1 variable\n\nGrowing:\n\nIf misses 2 or more variables\nIf includes categorical variables (labeled chr)\n\nFeedback: For Question 1(b), you need to be careful to include ALL of the variables that R believes are numerical, that includes variables which may behave more like a categorical variable."
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section",
    "title": "Lab 2 - Grading Guide",
    "section": "2",
    "text": "2\nCreate a histogram of the dep_delay variable from the nycflights data\nSuccess: Code should look like\n\nggplot(data = nycflights, mapping = aes(x = dep_delay)) + \n  geom_histogram() + \n  labs(x = \"Departure Delays (minutes)\")\n\n\n\n\n\n\n\nNote\n\n\n\nNote: May choose their own binwidth, but that is not required!\n\n\nGrowing:\n\nIf maps dep_delay to y-axis instead of x-axis\n\nFeedback: For Question 2, our histograms are always made with our quantitative variable on the x-axis!\n\nIf they don’t label the axis with the units\n\nFeedback: Every plot we make should have descriptive axis labels which include the units the variable was measured in. Were departure delays measured in hours? Minutes? Seconds? Days?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#a-1",
    "href": "labs/grading-guides/lab-2-grading-guide.html#a-1",
    "title": "Lab 2 - Grading Guide",
    "section": "3 (a)",
    "text": "3 (a)\nMake two other histograms, one with a binwidth of 15 and one with a binwidth of 150.\nSuccess: Code should look like:\n\nggplot(data = nycflights, \n       mapping = aes(x = dep_delay)) + \n  geom_histogram(binwidth = 15) + \n  labs(x = \"Departure Delays (minutes)\")\n\n\nggplot(data = nycflights, \n       mapping = aes(x = dep_delay)) + \n  geom_histogram(binwidth = 150) +\n  labs(x = \"Departure Delays (minutes)\")\n\n\n\n\n\n\n\nOkay if their axis labels are not adjusted\n\n\n\nIf they don’t have nice axis labels for this question, we don’t need to mark them down. But we can give this feedback:\nCareful! Look at the feedback for Question 2 about your axis labels.\n\n\nGrowing: If code does not use correct binwidths\nFeedback: For Question 3(a), pay attention to what binwidth you need to use within your geom_histogram()!"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#b-1",
    "href": "labs/grading-guides/lab-2-grading-guide.html#b-1",
    "title": "Lab 2 - Grading Guide",
    "section": "3 (b)",
    "text": "3 (b)\nHow do these three histograms compare? Are features revealed in one that are obscured in another?\nSuccess:\n\nDiscusses how wider bins make it hard to see where there are small differences and / or how smaller bins make it easier to see differences (i.e., smaller bins make the distribution more specific)\n\nGrowing:\n\nIf description only references how the bins are larger or smaller, but doesn’t talk about shape the distribution\nOnly talks about how the y-axis scale changes\n\nFeedback: For Question 3(b), your discussion should address how the binwidth affects the shape of the distribution (e.g., skew, peak) and what you are able to see."
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-1",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-1",
    "title": "Lab 2 - Grading Guide",
    "section": "4",
    "text": "4\nFill in the code to create a new dataframe named sfo_flights that is the result of filter()ing only the observations whose destination was San Francisco.\nSuccess: Code should look like:\n\nsfo_flights &lt;- filter(nycflights, \n                      dest == \"SFO\")\n\nGrowing: If they use the wrong destination in their filter()\nFeedback: For Question 4, you need to look at the description for how the airport names were coded!"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-2",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-2",
    "title": "Lab 2 - Grading Guide",
    "section": "5",
    "text": "5\nFill in the code below to find the number of flights flying into SFO in July that arrived early. What does the result tell you?\nSuccess: Code should look like:\n\nfilter(sfo_flights, \n       month == 7, \n       arr_delay &lt; 0) %&gt;% \n  dim()\n\nGrowing:\n\nIf they put 7 in quotations (\"7\")\n\nFeedback: We use quotations for variables that are categorical. Does R think that month is a categorical variable? (look back at what you said in Question 1b)\n\nIf they use a &gt; instead of a &lt;\n\nFeedback: Careful! If my flight arrived 5 minutes early, how would my arrival delay be recorded in the dataset? How does this translate to the inequality we want to use in our data filter?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-3",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-3",
    "title": "Lab 2 - Grading Guide",
    "section": "6",
    "text": "6\nWhen you ran the code above it output a preview of the filtered dataset. What does this output tell you about the number of flights that met your criteria (flying into SFO, in July, and arrived early)?\nThey should state that 45 flights (rows) met the criteria (arrived to SFO early in July).\nGrowing:\n\nIf they don’t interpret 45 as the number of flights – only as the number of rows\n\nFeedback: What are the observations included in the rows in the context of this dataset? Airplanes? Airports? Flights?\n\nIf they interpret 45 and 16 as flight numbers\n\nFeedback: The 45 and the 16 refer to the size of the filtered dataset. What does that tell you in terms of the number of flights that satisfied your criteria out of the sfo_flights dataset?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-4",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-4",
    "title": "Lab 2 - Grading Guide",
    "section": "7",
    "text": "7\nCalculate the following statistics for the arrival delays in the sfo_flights dataset:\n\nmean\nmedian\nmax\nmin\n\nSuccess: Code should look like:\n\nsummarise(sfo_flights, \n          mean_ad = mean(arr_delay), \n          median_ad = median(arr_delay), \n          max_ad = max(arr_delay),\n          min_ad = min(arr_delay)\n          )\n\n\nIf they don’t name their summary statistics\n\nFeedback: The output of your summary statistics looks a lot nicer if you give them names! (like the example that was given)\n\nI’d write a comment if they used _dd in their names, since it should be _ad for the arrival delays.\n\nFeedback: It would be more clear to use _ad at the end of your names, since you are summarizing the arrival delays (not the departure delays).\nGrowing:\n\nIf they miss some of the statistics\n\nFeedback: For Question 6, you were asked to provide four (4) statistics!\n\nIf they use the wrong dataset (nycflights instead of sfo_flights)\n\nFeedback: For Question 6, what dataset should you be using to calculate your summary statistics?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-5",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-5",
    "title": "Lab 2 - Grading Guide",
    "section": "8",
    "text": "8\nUsing the above summary statistics, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nSuccess\n\nMakes a statement about what I should expect (flight to be early / late)\nJustifies statement based on summary statistics\n\nGrowing\n\nDoes not justify their statement\n\nFeedback: In Statistics it is critical to back your claim with data! How did you decide what I should expect if I am flying from NYC to SFO?\n\nSays I will arrive early / late based on the median / mean\n\nFeedback: Why is the median / mean a good measure of the “typical” delay?\n\nOnly discuss the statistics but don’t address the question\n\nFeedback: You point out the statistics and what they mean in terms of an arrival delay, but how do these statistics connect with my question about what I should expect?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-6",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-6",
    "title": "Lab 2 - Grading Guide",
    "section": "9",
    "text": "9\nNow, rather than calculating summary statistics, plot the distribution of arrival delays for the sfo_flights dataset. Choose the type of plot you believe is appropriate for visualizing the distribution of arrival delays.\nSuccess: Code should look like:\n\nggplot(data = sfo_flights, \n       mapping = aes(x = arr_delay)) + \n  geom_histogram() + \n  labs(x = \"Arrival Delays (min)\")\n\n\nAcceptable geoms:\n\ngeom_histogram()\ngeom_dotplot() (though not great)\ngeom_density()\n\nIt’s okay for them not to change the binwidth\n\nGrowing:\n\nUses geom_boxplot()\n\nFeedback: For Question 9, we need to use a geom which allows for us to see the shape of the distribution. Boxplots hide distributions with multiple modes, so what type of plot would be better?\n\nDoesn’t change x-axis label and / or doesn’t include the units (minutes) in their label\n\nFeedback: You were asked to give your visualization nice axis labels. It is also important for the axis label to contain the units of the variable! What units were the arrival delays measured in?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-7",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-7",
    "title": "Lab 2 - Grading Guide",
    "section": "10",
    "text": "10\nUsing the plot above, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nSuccess\n\nMakes a statement about what I should expect (flight to be early / late)\nJustifies statement based on the plot\n\nGrowing\n\nDoes not justify their statement\n\nFeedback: In Statistics it is critical to back your claim with data! How did you use this distribution to decide what I should expect if I am flying from NYC to SFO?"
  },
  {
    "objectID": "labs/grading-guides/lab-2-grading-guide.html#section-8",
    "href": "labs/grading-guides/lab-2-grading-guide.html#section-8",
    "title": "Lab 2 - Grading Guide",
    "section": "11",
    "text": "11\nHow did your answer change when using the plot versus using the summary statistics? i.e. What were you able to see in the plot that could could not “see” with the summary statistics?\nSuccess:\n\nStates if / how their answer did / did not change\nDiscusses what could be see in the visualizations that could not be seen in the statistics\n\nskew\nmode / peak\n\n\nGrowing:\n\nResponse does not discuss what could be seen in the visualizations that could not be seen in the statistics\n\nFeedback: You should address what you were able to SEE in the visualization (e.g., shape, skew, number of peaks) that you could not “see” in the summary statistics.\n\nResponse doesn’t discuss if / how the answer to the question changed\n\nFeedback: Great job discussing how the shape of the distribution informs you choice of statistic! You did not, however, discuss if / how your answer to my question changed between looking at the plot versus looking at the statistics."
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html",
    "href": "labs/grading-guides/lab-3-grading-guide.html",
    "title": "Lab 3: Grading Guide",
    "section": "",
    "text": "Success:\n\nhas glimpse() somewhere in their code\nstates there are 32,209 rows and 16 columns\n\nGrowing:\n\nIf no code is present\nIf they flip the rows and columns\nIf they do not provide size of data\n\nFeedback for no code: You needed to write code to explore the size of the data and the types of variables.\nFeedback for no / incorrect dimensions: Look again at the output of the glimpse() function, how many rows and columns are included in the and_vertebrates dataset?"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-6-sources-of-variation",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-6-sources-of-variation",
    "title": "Lab 3: Grading Guide",
    "section": "Question 6 – Sources of variation",
    "text": "Question 6 – Sources of variation\nSuccess: Names three “reasonable” sources of variation in trout length\nGrowing: Names an unreasonable source of variation in trout lengths\nFeedback: We are interested in variables that, if changed, we would expect the length of the Cutthroat trout to change."
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-7-ridge-plot",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-7-ridge-plot",
    "title": "Lab 3: Grading Guide",
    "section": "Question 7 – Ridge plot",
    "text": "Question 7 – Ridge plot\nSuccess: Code should look like the following\n\nggplot(data = trout, \n       mapping = aes(x = length_1_mm, y = unittype)) +\n  geom_density_ridges() +\n  labs(x = \"Length (mm)\", \n       y = \"Channel Section\")\n\nGrowing:\n\nDoesn’t include units (mm) in x-axis label\n\nFeedback: It is important for the axis label to contain the units of the variable. What units were the lengths measured in?"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-8-adding-another-categorical-variable",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-8-adding-another-categorical-variable",
    "title": "Lab 3: Grading Guide",
    "section": "Question 8 – Adding another categorical variable",
    "text": "Question 8 – Adding another categorical variable\nSuccess: Uses either color or facets to incorporate species\n\n## Option 1 -- Facets\nggplot(data = trout, \n       mapping = aes(x = length_1_mm, y = unittype)) +\n  geom_density_ridges() +\n  facet_wrap(~species)\n\n## Option 2 -- Colors\nggplot(data = trout, \n       mapping = aes(x = length_1_mm, y = unittype, fill = species)) +\n  geom_density_ridges() \n\nGrowing:\n\ny-axis should say something about the type of channel\n\nFeedback: It is important for the axis label to describe the variable that is being measured. What would be a good y-axis title for what the unittype variable measured?\n\n\n\n\n\n\nNot marking down on axis label\n\n\n\nIf they got a growing on #8 for an axis label, they don’t get a growing here.\n\n\n\nIf doesn’t use facets or colors ::: callout-note # If they used color instead of fill\n\nFeedback: The color aesthetic only colors the outside of the ridge plot. However, if you were to use the fill aesthetic the entire ridge would be filled with color! :::"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-9-based-on-the-plot-how-different-are-the-lengths-between-the-channel-types-and-forest-sections",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-9-based-on-the-plot-how-different-are-the-lengths-between-the-channel-types-and-forest-sections",
    "title": "Lab 3: Grading Guide",
    "section": "Question 9 – Based on the plot, how different are the lengths between the channel types and forest sections?",
    "text": "Question 9 – Based on the plot, how different are the lengths between the channel types and forest sections?\nSuccess: Must have all of the following\n\nComparisons between forest sections (centers, spreads, shapes)\nComparisons between channel types\nState what channel types did not have both types of forest\n\nGrowing:\n\nIf they say the distributions are all fairly similar\n\nFeedback: While I would agree that most of these distributions are fairly similar (they overlap a lot), there are a few channel types where the lengths of trout are quite different between clear cut and old growth forests. Which channel types are these? In these channel types, how do the distributions of fish lengths differ (e.g., are trout in the CC section larger?)?\n\nComparisons are incomplete\n\nFeedback: Your comparison of these distributions should include similarities / differences in their centers and shapes.\n\nDoes not state what channel types did not have both types of forest\n\nFeedback: Careful! Were there clear cut and old growth forests for every type of channel? If not, what channel types only had one section of forest?"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-10-average-length-of-cutthroat-trout-between-channel-types",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-10-average-length-of-cutthroat-trout-between-channel-types",
    "title": "Lab 3: Grading Guide",
    "section": "Question 10 – Average length of Cutthroat trout between channel types",
    "text": "Question 10 – Average length of Cutthroat trout between channel types\nSuccess: Code should look similar to:\n\ntrout %&gt;%\n  group_by(unittype) %&gt;%\n  summarize(mean_length = mean(length_1_mm, na.rm = TRUE))\n\n\n\n\n\n\n\nIf they don’t name their summary statistics\n\n\n\nFeedback: For Question 11, the output of your summary statistics looks a lot nicer if you give them names! (like the example that was given)\n\n\nGrowing: If process is not correct"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-11-find-the-average-length-of-cutthroat-trout-between-channel-types-and-forest-section",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-11-find-the-average-length-of-cutthroat-trout-between-channel-types-and-forest-section",
    "title": "Lab 3: Grading Guide",
    "section": "Question 11 – Find the average length of Cutthroat trout between channel types and forest section",
    "text": "Question 11 – Find the average length of Cutthroat trout between channel types and forest section\nSuccess: Code should look similar to:\n\ntrout %&gt;%\n  group_by(unittype, species) %&gt;%\n  summarize(mean_length = mean(length_1_mm))\n\nGrowing: If process is not correct"
  },
  {
    "objectID": "labs/grading-guides/lab-3-grading-guide.html#question-12-differences-in-averages-compared-to-plot",
    "href": "labs/grading-guides/lab-3-grading-guide.html#question-12-differences-in-averages-compared-to-plot",
    "title": "Lab 3: Grading Guide",
    "section": "Question 12 – Differences in averages compared to plot",
    "text": "Question 12 – Differences in averages compared to plot\nSuccess:\n\nStates that the averages are all fairly similar (comparing the centers)\nConnects averages with the skew seen in the visualizations\n\nGrowing:\n\nIf their statement doesn’t connect the means to the shape of the distributions seen in Question 8\n\nFeedback: Note that in Question 11 you are specifically using the mean to summarize the center of the distribution. In Question 8 you saw the shape of each channel / forest section’s distribution. Based on what you saw, how do the shapes of the distributions influence the means you are seeing in Question 11?"
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html",
    "href": "labs/grading-guides/lab1_feedback.html",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they have answers to all eight (8) questions, they earn a “Complete”.\n\n\nList of favorite animals, include a picture, change section header\nIf their image doesn’t appear in the document\nQuestion 2 – Careful! Your image is not appearing in your rendered HTML file. There are a few different ways to fix this. First is to change the YAML (title section) of your document, as outlined in the Canvas announcement. If you did this and your image is still not showing up, then the issue might be how you are including your image. Generally, uploading an image to your project works better than including a URL link to an image. There are instructions on how to do this in the #lab channel of #Week 1 on Discord.\n\n\n\nIf they say something about #| include: false:\nQuestion 4 – Yes! You are on the right track! The #| include: false option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something about the code not running / the code not producing output:\nQuestion 4 – Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something not relevant to loading in a package:\nQuestion 4 – The code for Question 4 is associated with loading in the tidyverse package. Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\n\n\n\nIf they say it gives a preview of the dataset:\nQuestion 5 – You are on the right track! The glimpse() function outputs a preview of the dataset, specifically the column names, their data types, and a preview of the rows of each column.\n\n\n\nIf they say something about the code not showing:\nQuestion 6 – Yes! The #| echo: false option tells Quarto that the code should not be included in the rendered document, but the output of the code (the plot) should be included!\nIf they say something not relevant to the code not showing:\nQuestion 6 – The #| echo: false option controls how the rendered HTML file looks. Specifically, the #| echo: false option tells Quarto that the code should not be output in the rendered file, but the output of the code (the plot) should be included."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-1-2-3",
    "href": "labs/grading-guides/lab1_feedback.html#question-1-2-3",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "List of favorite animals, include a picture, change section header\nIf their image doesn’t appear in the document\nQuestion 2 – Careful! Your image is not appearing in your rendered HTML file. There are a few different ways to fix this. First is to change the YAML (title section) of your document, as outlined in the Canvas announcement. If you did this and your image is still not showing up, then the issue might be how you are including your image. Generally, uploading an image to your project works better than including a URL link to an image. There are instructions on how to do this in the #lab channel of #Week 1 on Discord."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-4",
    "href": "labs/grading-guides/lab1_feedback.html#question-4",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they say something about #| include: false:\nQuestion 4 – Yes! You are on the right track! The #| include: false option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something about the code not running / the code not producing output:\nQuestion 4 – Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document.\nIf they say something not relevant to loading in a package:\nQuestion 4 – The code for Question 4 is associated with loading in the tidyverse package. Technically, this code does run AND produces messages from loading in the package. The reason you can’t see the code or these messages is because of the #| include: false option. This option tells Quarto that neither the code nor the output (messages) should be included in the rendered document."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-5",
    "href": "labs/grading-guides/lab1_feedback.html#question-5",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they say it gives a preview of the dataset:\nQuestion 5 – You are on the right track! The glimpse() function outputs a preview of the dataset, specifically the column names, their data types, and a preview of the rows of each column."
  },
  {
    "objectID": "labs/grading-guides/lab1_feedback.html#question-6",
    "href": "labs/grading-guides/lab1_feedback.html#question-6",
    "title": "Lab 1 Feedback Guide",
    "section": "",
    "text": "If they say something about the code not showing:\nQuestion 6 – Yes! The #| echo: false option tells Quarto that the code should not be included in the rendered document, but the output of the code (the plot) should be included!\nIf they say something not relevant to the code not showing:\nQuestion 6 – The #| echo: false option controls how the rendered HTML file looks. Specifically, the #| echo: false option tells Quarto that the code should not be output in the rendered file, but the output of the code (the plot) should be included."
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html",
    "href": "labs/grading-guides/lab-9-grading-guide.html",
    "title": "Lab 9 – Grading Guide",
    "section": "",
    "text": "Q1:\nQ2:\nQ3:\nQ4:\nQ5:\nQ6:\nQ7:\nQ8:\nQ9:\nQ10:\nQ11:\nQ12:\nQ13:\nQ14:\nQ15:\nQ16:\nQ17:\nQ18:\nQ19:\nQ20:"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-1",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 1",
    "text": "Question 1\nTo earn a Success:\n\nfills in code\nadds axis label\n\n\n\n\n\n\n\nWarning\n\n\n\nTheir axis label needs to indicate this is a score for Grade 8 Math\n\n\nIf they do not have “grade 8” or “math score” in their title:\n\nWhat context are these math scores for? What exam? What grade?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-2",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\n\ncompares centers of plots\ncompares spreads of plots"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-3",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success:\n\nfills in code\nadds axis label\n\n\n\n\n\n\n\nWarning\n\n\n\nTheir axis label needs to indicate this is a score for Grade 8 Math\n\n\nIf they do not have “grade 8” or “math score” in their title:\n\nWhat context are these math scores for? What exam? What grade?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-4",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\n\ncompares centers of plots\ncompares spreads of plots"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-5-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-5-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 5 – Important!",
    "text": "Question 5 – Important!\nTo earn a Success:\n\n\nUsing fill:\n\nggplot(data = math_scores, \n       mapping = aes(x = grade_8_math_score, \n                     y = year_cat, \n                     fill = continent)) +\n  geom_density_ridges(alpha = 0.5, scale = 1) + \n  labs(x = \"Grade 8 Math Score on TIMSS\", \n       y = \"Year\", \n       fill = \"Continent\")\n\n\n\n\nUsing facets:\n\nggplot(data = math_scores, \n       mapping = aes(x = grade_8_math_score, \n                     y = year_cat)) +\n  geom_density_ridges(alpha = 0.5, scale = 1) + \n  labs(x = \"Grade 8 Math Score on TIMSS\", \n       y = \"Year\") + \n  facet_wrap(~continent)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTheir axis label needs to indicate this is a score for Grade 8 Math\n\n\nIf they don’t change the transparency of their plots:\n\nRemember you can use alpha to change the transparency of the density ridges so you can see them all!\n\nIf they do not have “grade 8” or “math score” in their title:\n\nWhat context are these math scores for? What exam? What grade?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-6-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-6-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 6 – Important!",
    "text": "Question 6 – Important!\nTo earn a Success: compares if distributions of continents move over time or if they stay similar"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-7-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-7-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 7 – Important!",
    "text": "Question 7 – Important!\nTo earn a Success:\n\ncompares observations within a continent\nnotices countries are recorded multiple times\nstates the condition is violated\n\nIf they do not state the condition is violated:\n\nLook at the observations for Africa! How many observations for Ghana are there? Is it reasonable to assume those observations are independent?\n\nIf they say observations for continents are related in time:\n\nDo we have one observation per continent? If not, what are the observations of? Are these observations within a continent related?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-8-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-8-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 8 – Important!",
    "text": "Question 8 – Important!\nTo earn a Success:\n\nconnects comparison with observations within a year or between continents\n\n\n\n\nsays countries / continents are not related\njustifies decision (no)\n\n\n\n\n\nsays countries / continents are related\njustifies decision (spatial relationship)\n\n\n\nIf they do not justify their decision:\n\nAn essential part of evaluating model conditions is to justify the decisions you make. What was your reasoning for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-9-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-9-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 9 – Important!",
    "text": "Question 9 – Important!\nTo earn a Success:\n\ncompares observations between continents\nstates that a country can only belong to one continent\nsays condition is not violated\n\nIf they don’t discuss how many continents a county can belong to:\n\nHow many continents can a country belong to? What does that say about between continent independence?\n\nIf they do not justify their decision:\n\nAn essential part of evaluating model conditions is to justify the decisions you make. What was your reasoning for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-10-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-10-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 10 – Important!",
    "text": "Question 10 – Important!\nTo earn a Success:\n\ncompares observations between years\nstates that observations are related in time\nsays condition is violated\n\nIf they do not state the condition is violated:\n\nIs the observation for Ghana in 2003 related to the observation for Ghana in 2007?\n\nIf they do not justify their decision:\n\nAn essential part of evaluating model conditions is to justify the decisions you make. What was your reasoning for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-11-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-11-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 11 – Important!",
    "text": "Question 11 – Important!\nTo earn a Success:\n\ndiscusses the shape of the distributions\nsays some distributions have more than one mode\nsays condition is violated\n\nIf they do not state the condition is violated:\n\nHow many peaks does the Normal distribution have? How many peaks do you see in some of these distributions?\n\nIf they don’t make a decision (violated / not):\n\nA critical part of evaluation model conditions is to make a decision! Do you feel that this condition is or is not violated?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-12-important",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-12-important",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 12 – Important!",
    "text": "Question 12 – Important!\nTo earn a Success:\n\ncompares the log variances between continents / years\nsays there are large differences in the log variances\nsays condition is violated\n\nIf they do not state the condition is violated:\n\nHow many times larger is the largest variance than the smallest variance? If it is less than 2, they could be similar values, but larger than 2 they start to seem pretty different."
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-13",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-13",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 13",
    "text": "Question 13\nTo earn a Success: Code should look like:\n\naov(grade_8_math_score ~ year, data = math_scores) %&gt;% \n  broom::tidy()\n\nIf they swap \\(y\\) and \\(x\\):\n\nCareful! In the lm() function the response goes first!"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-14",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-14",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 14",
    "text": "Question 14\nTo earn a Success:\n\nreject \\(H_0\\)\nstate that the p-value was less than 0.1\n\n\n\n\n\n\n\nFine to not include p-value\n\n\n\nIf they don’t state what the p-value was:\n\nRemember it is important to be transparent with your decision, what p-value did you obtain for this test?\n\n\n\nIf they don’t state their \\(\\alpha\\):\n\nWhat alpha was used for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-15",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-15",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 15",
    "text": "Question 15\nTo earn a Success: Conclude at least on year has a different mean grade 8 math score\nIf they say all the years have different means:\n\nWhat is the alternative hypothesis for a one-way ANOVA? Is it that all of the years have different means?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-16",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-16",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 16",
    "text": "Question 16\nTo earn a Success: Code should look like:\n\nnull_dist &lt;- math_scores %&gt;%\n  specify(response = grade_8_math_score, explanatory = continent) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-17",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-17",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 17",
    "text": "Question 17\nTo earn a Success: Code should look like:\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat = obs_F, direction = \"greater\")\n\nIf they don’t use \"greater\" in their direction:\n\nCareful! F-statistics can only be positive! What direction should we use to calculate the p-value for statistics that cannot be negative?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-18",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-18",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 18",
    "text": "Question 18\nTo earn a Success: Code should look like:\n\nget_p_value(null_dist, \n            obs_stat = obs_F, \n            direction = \"greater\")\n\nIf they don’t use \"greater\" in their direction:\n\nCareful! F-statistics can only be positive! What direction should we use to calculate the p-value for statistics that cannot be negative?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-19",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-19",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 19",
    "text": "Question 19\nTo earn a Success:\n\nreject \\(H_0\\)\nstate that the p-value was less than 0.1\n\n\n\n\n\n\n\nFine to not include p-value\n\n\n\nIf they don’t state what the p-value was:\n\nRemember it is important to be transparent with your decision, what p-value did you obtain for this test?\n\n\n\nIf they don’t state their \\(\\alpha\\):\n\nWhat alpha was used for your decision?"
  },
  {
    "objectID": "labs/grading-guides/lab-9-grading-guide.html#question-20",
    "href": "labs/grading-guides/lab-9-grading-guide.html#question-20",
    "title": "Lab 9 – Grading Guide",
    "section": "Question 20",
    "text": "Question 20\nTo earn a Success: Conclude at least on continent has a different mean grade 8 math score\nIf they say all the continents have different means:\n\nWhat is the alternative hypothesis for a one-way ANOVA? Is it that all of the continents have different means?"
  },
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "",
    "text": "Today we will explore the pie_crab dataset contained in the lterdatasampler R package. The data is from a study by Johnson et al. at the Plum Island Ecosystem Long Term Ecological Research site, studying the relationship between the size (carapace width) of a Fiddler Crab and the geographical location of its habitat. These data can be used to investigate if Bergmann’s Rule applies to Fiddler Crabs, or specifically that the size of a crab increases as the distance from the equator increases.\n\n\nThe students who investigated this relationship for their Midterm Project found that when both latitude and water temperature are included as explanatory variables in the multiple regression model, the coefficient associated with water temperature doesn’t make sense. Namely, the model suggests warmer water temperatures are associated with larger crab sizes. However, we know that the water is warmer near the equator, which is where the crab sizes should be smaller. Rather perplexing!\nThe moral of the story is that water temperature and latitude are high correlated with each other, so including them both as explanatory variables leads to multicollinearity – something we do not want in our multiple linear regression.\n\n\n\nThe focus of this lab is on quantifying the relationship between water temperature (response) and latitude (explanatory) for marshes (sites) along the Atlantic coast."
  },
  {
    "objectID": "labs/lab-7.html#data",
    "href": "labs/lab-7.html#data",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "",
    "text": "Today we will explore the pie_crab dataset contained in the lterdatasampler R package. The data is from a study by Johnson et al. at the Plum Island Ecosystem Long Term Ecological Research site, studying the relationship between the size (carapace width) of a Fiddler Crab and the geographical location of its habitat. These data can be used to investigate if Bergmann’s Rule applies to Fiddler Crabs, or specifically that the size of a crab increases as the distance from the equator increases.\n\n\nThe students who investigated this relationship for their Midterm Project found that when both latitude and water temperature are included as explanatory variables in the multiple regression model, the coefficient associated with water temperature doesn’t make sense. Namely, the model suggests warmer water temperatures are associated with larger crab sizes. However, we know that the water is warmer near the equator, which is where the crab sizes should be smaller. Rather perplexing!\nThe moral of the story is that water temperature and latitude are high correlated with each other, so including them both as explanatory variables leads to multicollinearity – something we do not want in our multiple linear regression.\n\n\n\nThe focus of this lab is on quantifying the relationship between water temperature (response) and latitude (explanatory) for marshes (sites) along the Atlantic coast."
  },
  {
    "objectID": "labs/lab-7.html#cleaning-the-data",
    "href": "labs/lab-7.html#cleaning-the-data",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nThe data contains information on at total of 392 Fiddler Crabs caught at 13 marshes on the Atlantic coast of the United States in summer 2016. However, at each marsh, there is only one recorded water temperature. Meaning, we need to collapse our dataset to have only one observation per marsh.\n1. Fill in the code below to create a new dataset called marsh_info which has 13 observations – one per marsh.\n\nmarsh_info &lt;- pie_crab %&gt;% \n  group_by(____) %&gt;% \n  slice_sample(n = 1) %&gt;% \n  ungroup()\n\nFrom this point forward, you should use the marsh_info dataset for EVERY problem. Keep in mind that you are no longer analyzing data on crabs! The dataset you have is on marshes along the Atlantic coast!"
  },
  {
    "objectID": "labs/lab-7.html#visualizing-relationships",
    "href": "labs/lab-7.html#visualizing-relationships",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Visualizing Relationships",
    "text": "Visualizing Relationships\n2. Create a scatterplot modeling the relationship between latitude (explanatory) and water temperature (response) for these 13 marshes. Don’t forget to add descriptive axis labels!\n3. Describe the relationship you see in the scatterplot. Be sure to address the four aspects we discussed in class: form, direction, strength, and unusual points! Keep in mind that you are no longer analyzing data on crabs! The dataset you have is on marshes along the Atlantic coast!\n\nSummarizing the Relationship\nNow that you’ve visualized the relationship, let’s summarize this relationship with a statistic. Specifically, we are interested in the slope statistic, as it captures the relationship between latitude and water temperature.\n4. Fill in the code below to calculate the observed slope for the relationship between the water temperature (response) and latitude (explanatory).\nNote: Nothing will be output when you run this code!\n\nobs_slope &lt;- marsh_info %&gt;% \n  specify(response = ____, \n          explanatory = ____) %&gt;% \n  calculate(stat = ____)"
  },
  {
    "objectID": "labs/lab-7.html#bootstrap-distribution",
    "href": "labs/lab-7.html#bootstrap-distribution",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Bootstrap Distribution",
    "text": "Bootstrap Distribution\nNow that we have the observed slope statistic, let’s see what variability we might get in the slope statistic for other samples (marshes) we might have gotten from the population (the Atlantic coast of the US).\nAs a refresher, when we use resampling to obtain our bootstrap distribution, our steps look like the following:\nStep 1: specify() the response and explanatory variables\nStep 2: generate() lots of bootstrap resamples\nStep 3: calculate() for each of the generated() samples, calculate the statistic you are interested in\nLet’s give this a try!\n5. Fill in the code to generate 500 bootstrap slope statistics (from 500 bootstrap resamples).\n\nbootstrap &lt;- marsh_info %&gt;% \n  specify(response = ____, \n          explanatory = ____) %&gt;% \n  generate(reps = ____, \n           type = ____) %&gt;% \n  calculate(stat = ____)\n\nAlright, now that we have the bootstrap slope statistics, let’s see how it looks! Let’s use the visualize() function (not ggplot()!) to make a quick visualization of the statistics you calculated above.\n6. Use the visualize() function to create a simple histogram of your 500 bootstrap statistics. It would be nice to change the x-axis label to describe what statistic is being plotted!\n\n# Code to visualize bootstrap statistics"
  },
  {
    "objectID": "labs/lab-7.html#confidence-interval",
    "href": "labs/lab-7.html#confidence-interval",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nThe next step to obtain our confidence interval! First we need to determine what percentage of statistics we want to keep in the confidence interval. 90%? 95%? 99%? 80%?\nThis seems like a study where we care a bit less about our interval capturing the true value, at least compared to something like a medical study. So, I think this could be a great instance to use a 90% confidence interval.\n7. Use the get_confidence_interval() function to find the 90% confidence interval from your bootstrap distribution, using the percentile method!\n\n# Code to obtain a 90% PERCENTILE based confidence interval\n\n8. Interpret the confidence interval you obtained in #7. Make sure to include the context of the data and the population of interest!\nJust for fun, let’s compare the confidence we obtained using a percentile method with an interval found using the SE method.\n9. Use the get_confidence_interval() function to find the 90% confidence interval from your bootstrap distribution, using the SE method! Remember – with the SE method, you need to specify the point estimate!\n\n# Code to obtain a 90% SE based confidence interval\n\n10. How do your confidence intervals compare? Based on the shape of the bootstrap distribution, would you expect for these methods to yield similar results?\nHint: Think about the conditions for using the SE method to obtain a confidence interval!"
  },
  {
    "objectID": "labs/lab-7.html#bootstrap-assumptions",
    "href": "labs/lab-7.html#bootstrap-assumptions",
    "title": "Lab 7: Confidence Intervals for Water Temperature and Latitude",
    "section": "Bootstrap Assumptions",
    "text": "Bootstrap Assumptions\nA bootstrap distribution aims to simulate the variability we’d get from other samples from our population. However, the accuracy of these samples relies on the quality of our original sample.\n11. Based on the information given, how do you feel about the assumption a bootstrap distribution makes about the original sample? What issues do you believe might prevent this assumption being appropriate?"
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Lab 1: Welcome to Posit Cloud!",
    "section": "",
    "text": "This is a Quarto document!\nQuarto is a software that allows you to interweave text and R code to create HTML, PDF, and Microsoft Word documents\nThere are two ways to view a Quarto document, (1) as the “Source” file, or (2) as the “Visual” file. We will only use the Visual option in this class, as it allows you to interact with Quarto similar to how you interact with Word.\n\n\nSimilar to a Word Doc, there are a variety of ways you can spice up a Quarto document! Let’s explore a few.\nQuestion 1: Using the formatting options, make a numbered list of your top three favorite animals.\nQuestion 2: Using the formatting options, insert an image of your favorite animal.\nQuestion 3: Now, change the “Formatting your Document” section name to the name of your favorite animal. Make sure your header is a level 1 – use the Header 1 formatting option!\n\n\n\nYou can differentiate the R code within a Quarto file from the body of the document, based on the gray boxes that start with an {r}.\nHere is an example of an R code chunk:\nNotice in the line after the {r} there are two lines that start with #| – this is the symbol that declares options for a code chunk. The #| label: allows us to specify a name for a code chunk, I typically choose a name that tells me what the code chunk does (e.g., load-packages, clean-data). The #| include: false option at the beginning of the code chunk controls how the code output looks in our final rendered document.\nThis code chunk has two things we want to pay attention to:\n\nThe library(tidyverse) code loads in an R package called the “tidyverse”. This is code you will have in every lab assignment for this class!\nCode comments which are denoted by a # symbol. Code comments are a way for you (and me) to write what the code is doing, without R thinking what we are writing is code it should execute.\n\n\n\n\nWhen you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nQuestion 4: Do you see the above code chunk when you render the document? Why do you think this is the case?\n\n\n\nYou can include code output in your rendered document:\n\nglimpse(mpg)\n\nQuestion 5: What do you think the above code does? What type of output does it give you?\nHint: You have saw this type of output on Tuesday!\n\n\n\n\nYou can also embed plots in the rendered document.\nHere is an example of a plot.\nQuestion 6: What do you think the echo: false option does in the above code chunk?\nQuestion 7: What do you think the mapping = aes(y = manufacturer, x = hwy)) code does?\nQuestion 8: What do you think the labs(x = \"Highway Miles Per Gallon\", y = \"Car Manufacturer\") code does?"
  },
  {
    "objectID": "labs/lab-1.html#formatting-your-document",
    "href": "labs/lab-1.html#formatting-your-document",
    "title": "Lab 1: Welcome to Posit Cloud!",
    "section": "",
    "text": "Similar to a Word Doc, there are a variety of ways you can spice up a Quarto document! Let’s explore a few.\nQuestion 1: Using the formatting options, make a numbered list of your top three favorite animals.\nQuestion 2: Using the formatting options, insert an image of your favorite animal.\nQuestion 3: Now, change the “Formatting your Document” section name to the name of your favorite animal. Make sure your header is a level 1 – use the Header 1 formatting option!"
  },
  {
    "objectID": "labs/lab-1.html#r-code",
    "href": "labs/lab-1.html#r-code",
    "title": "Lab 1: Welcome to Posit Cloud!",
    "section": "",
    "text": "You can differentiate the R code within a Quarto file from the body of the document, based on the gray boxes that start with an {r}.\nHere is an example of an R code chunk:\nNotice in the line after the {r} there are two lines that start with #| – this is the symbol that declares options for a code chunk. The #| label: allows us to specify a name for a code chunk, I typically choose a name that tells me what the code chunk does (e.g., load-packages, clean-data). The #| include: false option at the beginning of the code chunk controls how the code output looks in our final rendered document.\nThis code chunk has two things we want to pay attention to:\n\nThe library(tidyverse) code loads in an R package called the “tidyverse”. This is code you will have in every lab assignment for this class!\nCode comments which are denoted by a # symbol. Code comments are a way for you (and me) to write what the code is doing, without R thinking what we are writing is code it should execute."
  },
  {
    "objectID": "labs/lab-1.html#rendering",
    "href": "labs/lab-1.html#rendering",
    "title": "Lab 1: Welcome to Posit Cloud!",
    "section": "",
    "text": "When you click the Render button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nQuestion 4: Do you see the above code chunk when you render the document? Why do you think this is the case?"
  },
  {
    "objectID": "labs/lab-1.html#including-code-output",
    "href": "labs/lab-1.html#including-code-output",
    "title": "Lab 1: Welcome to Posit Cloud!",
    "section": "",
    "text": "You can include code output in your rendered document:\n\nglimpse(mpg)\n\nQuestion 5: What do you think the above code does? What type of output does it give you?\nHint: You have saw this type of output on Tuesday!"
  },
  {
    "objectID": "labs/lab-1.html#including-plots",
    "href": "labs/lab-1.html#including-plots",
    "title": "Lab 1: Welcome to Posit Cloud!",
    "section": "",
    "text": "You can also embed plots in the rendered document.\nHere is an example of a plot.\nQuestion 6: What do you think the echo: false option does in the above code chunk?\nQuestion 7: What do you think the mapping = aes(y = manufacturer, x = hwy)) code does?\nQuestion 8: What do you think the labs(x = \"Highway Miles Per Gallon\", y = \"Car Manufacturer\") code does?"
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2: Visualizing and Summarizing Numerical Data",
    "section": "",
    "text": "Let’s load the following packages:\n\nThe tidyverse “umbrella” package which houses a suite of many different R packages for data wrangling and data visualization\nThe openintro R package: houses the dataset we will be working with\n\n\n# Package for functions \nlibrary(tidyverse)\n\n# Package for data\nlibrary(openintro)\n\n\n\n\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes transportation data available, such as the flights data we will be working with in this lab.\nFirst, we’ll view the nycflights data frame. Run the following code to load in the data:\n\ndata(nycflights)\n\nThe codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?nycflights\n\nRemember that you can use glimpse() to take a quick peek at your data to understand its contents better.\nQuestion 1\n(a) How large is the nycflights dataset? (i.e. How many rows and columns does it have?)\n(b) Are there numerical variables in the dataset? If so, what are their names? Hint: What data types are numerical variables stored as?\n\n# You code for exercise 1 goes here! Yes, your answer should use code!\n\n\n\n\nLet’s start by examining the distribution of departure delays (dep_delay) of all flights with a histogram.\nQuestion 2 – Create a histogram of the dep_delay variable from the nycflights data. Don’t forget to give your visualization informative axis labels that include the units the variable was measured in!\n\n# Your code for exercise 2 goes here! \n\nHistograms are generally a very good way to see the shape of a single distribution of numerical data, but that shape can change depending on how the data into different bins.\nYou can easily define the binwidth you want to use, by specifying the binwidth argument inside of geom_histogram(), like so:\ngeom_histogram(binwidth = 15)\nQuestion 3\n(a) Make two other histograms, one with a binwidth of 15 and one with a binwidth of 150. Feel free to copy-and-paste the code you used for Question 2 and modify the binwidth.\n# Your code for exercise 3 goes here! \n\n\n\n\n\n(b) How do these three histograms compare? Are features revealed in one that are obscured in another?"
  },
  {
    "objectID": "labs/lab-2.html#getting-started",
    "href": "labs/lab-2.html#getting-started",
    "title": "Lab 2: Visualizing and Summarizing Numerical Data",
    "section": "",
    "text": "Let’s load the following packages:\n\nThe tidyverse “umbrella” package which houses a suite of many different R packages for data wrangling and data visualization\nThe openintro R package: houses the dataset we will be working with\n\n\n# Package for functions \nlibrary(tidyverse)\n\n# Package for data\nlibrary(openintro)\n\n\n\n\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes transportation data available, such as the flights data we will be working with in this lab.\nFirst, we’ll view the nycflights data frame. Run the following code to load in the data:\n\ndata(nycflights)\n\nThe codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?nycflights\n\nRemember that you can use glimpse() to take a quick peek at your data to understand its contents better.\nQuestion 1\n(a) How large is the nycflights dataset? (i.e. How many rows and columns does it have?)\n(b) Are there numerical variables in the dataset? If so, what are their names? Hint: What data types are numerical variables stored as?\n\n# You code for exercise 1 goes here! Yes, your answer should use code!\n\n\n\n\nLet’s start by examining the distribution of departure delays (dep_delay) of all flights with a histogram.\nQuestion 2 – Create a histogram of the dep_delay variable from the nycflights data. Don’t forget to give your visualization informative axis labels that include the units the variable was measured in!\n\n# Your code for exercise 2 goes here! \n\nHistograms are generally a very good way to see the shape of a single distribution of numerical data, but that shape can change depending on how the data into different bins.\nYou can easily define the binwidth you want to use, by specifying the binwidth argument inside of geom_histogram(), like so:\ngeom_histogram(binwidth = 15)\nQuestion 3\n(a) Make two other histograms, one with a binwidth of 15 and one with a binwidth of 150. Feel free to copy-and-paste the code you used for Question 2 and modify the binwidth.\n# Your code for exercise 3 goes here! \n\n\n\n\n\n(b) How do these three histograms compare? Are features revealed in one that are obscured in another?"
  },
  {
    "objectID": "labs/lab-2.html#sfo-destinations",
    "href": "labs/lab-2.html#sfo-destinations",
    "title": "Lab 2: Visualizing and Summarizing Numerical Data",
    "section": "SFO Destinations",
    "text": "SFO Destinations\nOne of the variables refers to the destination (i.e. airport) of the flight, which have three letter abbreviations. For example, flights into Los Angeles have a dest of \"LAX\", flights into San Francisco have a dest of \"SFO\", and flights into Chicago (O’Hare) have a dest of \"ORD\".\nIf you want to visualize only on delays of flights headed to Los Angeles, you need to first filter() the data for flights with that destination (e.g., filter(dest == \"LAX\")) and then make a histogram of the departure delays of only those flights.\nLogical operators: Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so, you can use the filter() function and a series of logical operators. The most commonly used logical operators for data analysis are as follows:\n\n== means “equal to”\n!= means “not equal to”\n&gt; or &lt; means “greater than” or “less than”\n&gt;= or &lt;= means “greater than or equal to” or “less than or equal to”\n\nQuestion 4 – Fill in the code to create a new dataframe named sfo_flights that is the result of filter()ing only the observations whose destination was San Francisco.\n\nsfo_flights &lt;- filter(nycflights, \n                      dest == )\n\n\nMultiple Data Filters\nYou can filter based on multiple criteria! Within the filter() function, each criteria is separated using commas. For example, suppose you are interested in flights leaving from LaGuardia (LGA) in February:\n\nfilter(nycflights, \n       origin == \"LGA\", \n       month == 2)\n\n## Remember months are coded as numbers (February = 2)!\n\nNote that you can separate the conditions using commas if you want flights that are both leaving from LGA and flights in February. If you are interested in either flights leaving from LGA or flights that happened in February, you can use the | instead of the comma.\nQuestion 5 – Fill in the code below to find the number of flights flying into SFO in July that arrived early.\n\nfilter(sfo_flights, \n       month == __, \n       arr_delay &gt; __) %&gt;% \n  glimpse()\n\nQuestion 6 – When you ran the code above output a preview of the filtered dataset. What does this output tell you about the number of flights that met your criteria (SFO, July, arrived early)?"
  },
  {
    "objectID": "labs/lab-2.html#data-summaries",
    "href": "labs/lab-2.html#data-summaries",
    "title": "Lab 2: Visualizing and Summarizing Numerical Data",
    "section": "Data Summaries",
    "text": "Data Summaries\nYou can also obtain numerical summaries for the flights headed to SFO, using the summarise() function:\n\nsummarise(sfo_flights, \n          mean_dd   = mean(dep_delay), \n          median_dd = median(dep_delay), \n          n         = n())\n\nNote that in the summarise() function I’ve created a list of three different numerical summaries that I’m interested in.\nThe names of these elements are user defined, like mean_dd, median_dd, n, and you can customize these names as you like (just don’t use spaces in your names!).\nCalculating these summary statistics also requires that you know the summary functions you would like to use.\nSummary statistics: Some useful function calls for summary statistics for a single numerical variable are as follows:\n\nmean(): calculates the average\nmedian(): calculates the median\nsd(): calculates the standard deviation\nvar(): calculates the variances\nIQR(): calculates the inner quartile range (Q3 - Q1)\nmin(): finds the minimum\nmax(): finds the maximum\nn(): reports the sample size\n\nNote that each of these functions takes a single variable as an input and returns a single value as an output."
  },
  {
    "objectID": "labs/lab-2.html#summaries-vs.-visualizations",
    "href": "labs/lab-2.html#summaries-vs.-visualizations",
    "title": "Lab 2: Visualizing and Summarizing Numerical Data",
    "section": "Summaries vs. Visualizations",
    "text": "Summaries vs. Visualizations\nIf I’m flying from New York to San Francisco, should I expect that my flights will typically arrive on time?\nLet’s think about how you could answer this question. One option is to summarize the data and inspect the output. Another option is to plot the delays and inspect the plots. Let’s try both!\nQuestion 7 – Calculate the following statistics for the arrival delays in the sfo_flights dataset:\n\nmean\nmedian\nmax\nmin\n\n\n## Code for exercise 7 goes here! \n\nQuestion 8 – Using the above summary statistics, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nQuestion 9 – Now, rather than calculating summary statistics, plot the distribution of arrival delays for the sfo_flights dataset.\nChoose the type of plot you believe is appropriate for visualizing the distribution of arrival delays. Don’t forget to give your visualization informative axis labels that include the units of measurement!\n\n## Code for exercise 9 goes here! \n\nQuestion 10 – Using the plot above, what is your answer be to my question? What should I expect if I am flying from New York to San Francisco?\nQuestion 11 – How did your answer change when using the plot versus using the summary statistics? i.e. What were you able to see in the plot that could could not “see” with the summary statistics?"
  },
  {
    "objectID": "labs/lab-8.html",
    "href": "labs/lab-8.html",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gapminder)\nlibrary(infer)\nlibrary(moderndive)"
  },
  {
    "objectID": "labs/lab-8.html#question-of-interest",
    "href": "labs/lab-8.html#question-of-interest",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Question of Interest",
    "text": "Question of Interest\nThe objective of this data analysis is to answer the question:\n\nWhat is the relationship between life expectancy GDP per capita?"
  },
  {
    "objectID": "labs/lab-8.html#data-visualization",
    "href": "labs/lab-8.html#data-visualization",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Data Visualization",
    "text": "Data Visualization\n1. Create a scatterplot of the relationship between life expectancy (response) and GDP (explanatory).\nRemember to include nice axis labels (with units!).\nWhat you see should make you concerned about using a linear regression! So, let’s play with some variable transformations.\nYou can explore if a log-transformation of the y-variable would make the relationship more linear by adding a scale_y_log10() layer to your plot, like so:\n\nlterdatasampler::hbr_maples %&gt;% \n  ggplot(mapping = aes(x = stem_length, \n                       y = stem_dry_mass)\n         ) +\n  geom_point() + \n  scale_y_log10()\n\nSimilarly, you can a log-transformation of the x-variable would be helpful by adding a scale_x_log10() layer to your plot.\n2. Using scale_x_log10() and scale_y_log10(), decide on what relationship between life expectancy and GDP per capita appears the most linear. There should only be one plot for this problem!\nRemember to include nice axis labels (with any transformed units!)."
  },
  {
    "objectID": "labs/lab-8.html#assessing-model-conditions",
    "href": "labs/lab-8.html#assessing-model-conditions",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Assessing Model Conditions",
    "text": "Assessing Model Conditions\nThe next step is to check the conditions of our statistical model, we do this by analyzing our residuals and how the data were collected.\n\nIndependence of Observations\nEach row of the gapminder dataset is an observation for one country for one year (from 1952 to 2007).\n4. Do you believe is it reasonable to assume these observations are independent of one another?\nHint: This condition says the rows of the dataset are independent of each other. Look at the rows of the dataset, is there any reason to believe there are relationships between the rows?\n\n\nNormality of Residuals\nI’ve provided code to visualize the residuals from the model you fit in #3 below.\n\nbroom::augment(gapminder_lm) %&gt;% \n  ggplot(mapping = aes(x = .resid)) +\n  geom_histogram() +\n  labs(x = \"Residual\")\n\n5. Based on the distribution of residuals, do you believe the condition of normality is violated? Why or why not?\n\n\nEqual Variance of Residuals\nI’ve provided code to visualize the residuals versus fitted values from the model you fit in #3 below. With this plot, we want to assess if the variability (spread) of the residuals changes based on the values of the explanatory variable.\n\nbroom::augment(gapminder_lm) %&gt;% \n  ggplot(mapping = aes(y = .resid, x = `log(gdpPercap)`)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\", linewidth = 3) +\n  labs(x = \"Log Transformed GDP Per Capita\")\n\n6. Based on the plot above, do you believe the condition of equal variance is violated? Why or why not?"
  },
  {
    "objectID": "labs/lab-8.html#stating-the-hypotheses",
    "href": "labs/lab-8.html#stating-the-hypotheses",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Stating the Hypotheses",
    "text": "Stating the Hypotheses\nNow that you’ve decided which regression appears the most linear, let’s perform a hypothesis test for the slope coefficient.\n7. Write the hypotheses in words for testing if there is a linear relationship between the variables you used for your model in #3.\nKeep in mind, if you log-transformed y, you are testing if there is a linear relationship between log(y) and x!\n\\(H_0\\):\n\\(H_A\\):"
  },
  {
    "objectID": "labs/lab-8.html#obtaining-a-p-value-using-simulation",
    "href": "labs/lab-8.html#obtaining-a-p-value-using-simulation",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Obtaining a p-value Using Simulation",
    "text": "Obtaining a p-value Using Simulation\nNext, we will work through creating a permutation distribution using tools from the infer package.\n8. First, we need to find the observed slope statistic, which we will save as obs_slope.\nKeep in mind, if you log-transformed y, you need to use log(y) as your response variable!\n\nobs_slope &lt;-  gapminder %&gt;%\n  specify(response = ____, explanatory = ____) %&gt;%\n  calculate(stat = \"slope\")\n\nAfter you have calculated your observed statistic, you need to create a permutation distribution of statistics that might have occurred if the null hypothesis was true.\n9. Generate 500 permuted statistics for the permutation distribution and save these statistics in an object named null_dist.\n\nnull_dist &lt;- \n\nWe can visualize this null distribution with the following code:\n\nvisualise(null_dist) \n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n10. Fill in the code below to calculate the p-value for the hypothesis test you stated in #7.\n\nget_p_value(null_dist, \n            obs_stat = ____, \n            direction = ____)\n\n11. Based on your p-value and an \\(\\alpha = 0.1\\), what decision would you reach regarding the hypotheses you stated in #7?"
  },
  {
    "objectID": "labs/lab-8.html#obtaining-a-p-value-using-theory",
    "href": "labs/lab-8.html#obtaining-a-p-value-using-theory",
    "title": "Lab 8: Evaluating Conditions & Conducting Hypothesis Tests",
    "section": "Obtaining a p-value Using Theory",
    "text": "Obtaining a p-value Using Theory\nAs we saw in the reading this week, the output from the get_regression_table() function provides us with theory-based estimates of our standard error, \\(t\\)-statistic, and p-value.\n12. Use the get_regression_table() function to obtain the theory-based p-value for your hypothesis test.\nHint: You’ll want to use the model you fit in #3.\n13. How does this p-value compare to what you obtained in #11?\n14. Why do you believe these p-values were similar or different?\n15. Based on your answers to #4-6, which p-value do you believe is the most reliable? Why? Note: If you believe neither are reliable, say so and state why."
  },
  {
    "objectID": "labs/lab-9.html",
    "href": "labs/lab-9.html",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(infer)\nlibrary(ggridges)\nlibrary(broom)"
  },
  {
    "objectID": "labs/lab-9.html#todays-data",
    "href": "labs/lab-9.html#todays-data",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "Today’s Data",
    "text": "Today’s Data\nThese data come from the Gapminder Foundation, an organization interested in increasing the use and understanding of statistics and other information about social, economic and environmental development at local, national and global levels.\nToday we will be comparing math achievement scores across continents and years. Math achievement was measured for 42 countries based on their average score for the grade 8 international TIMSS test.\n\nmath_scores &lt;- read_csv(here::here(\"labs\", \n                                   \"data\",\n                                   \"math_scores.csv\")\n                        )\n\n# Creating a year_cat variable that is the categorical version of year\nmath_scores &lt;- mutate(math_scores, \n                      year_cat = as.factor(year)\n                      )\n\n# Removing the missing values from the grade_8_math_score variable\nmath_scores &lt;- drop_na(data = math_scores, \n                       grade_8_math_score)"
  },
  {
    "objectID": "labs/lab-9.html#data-visualizations",
    "href": "labs/lab-9.html#data-visualizations",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "Data Visualizations",
    "text": "Data Visualizations\nThe first step for a statistical analysis should always be creating visualizations of the data. Similar to what you are expected to do for your project, you will make three density ridge plots:\n\nvisualizing the relationship between math score and year\nvisualizing the relationship between math score and continent\nvisualizing the relationship between math score with both year and continent\n\nQuestion 1 – Fill in the code below to visualize the distribution of grade 8 math scores over time.\nDon’t forget to include axis labels!\n\nggplot(data = math_scores, \n       mapping = aes(x = ____, \n                     y = ____)) +\n  geom_density_ridges(scale = 1) \n\nNote: I’ve included a scale = 1 argument to show you how you can get the density plots not to overlap!\nQuestion 2 – What do you see in the plot you made? How do the centers (means) of the distributions compare? What about the variability (spread) of the distributions?\nQuestion 3 – Write the code to visualize the distribution of grade 8 math scores for the six different continents.\nDon’t forget to include axis labels!\nQuestion 4 – What do you see in the plot you made? How do the centers (means) of the distributions compare? What about the variability (spread) of the distributions?\nQuestion 5 – Write the code to visualize the distribution of grade 8 math scores for the six different continents for each of the four years.\nRemember, you could either include a facet or a color here!Also remember you can use alpha to change the transparency of your density ridges!\nQuestion 6 – What do you see in the plot you made? Does it seem that the relationship between year and grade 8 math scores changes based on the continent of the student?"
  },
  {
    "objectID": "labs/lab-9.html#statistical-model",
    "href": "labs/lab-9.html#statistical-model",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "Statistical Model",
    "text": "Statistical Model\nFor our analysis we will be using an analysis of variance (ANOVA) model. An ANOVA is an appropriate statistical model as we have a continuous response variable (grade 8 math score) and categorical explanatory variables (year, continent). Year is not considered to be a continuous numerical variable as we have only four measurements in time (1996, 1999, 2003, 2007).\n\nModel Conditions\nAn ANOVA has model conditions that are very similar to what we learned for linear regression. In this section we will evaluate the conditions of the model.\nFor this section, it might be helpful to know how many observations there are for each year and for each continent. I have written code below to provide you with a table of these numbers:\n\ncount(math_scores, continent, year) %&gt;% \n  pivot_wider(names_from = continent, \n              values_from = n, \n              values_fill = 0) %&gt;% \n  janitor::adorn_totals(where = c(\"row\", \"col\"))\n\n\nIndependence\nBased on the table we know:\n\neach year has measurements on about six continents\neach continent has measurements for about four years\n\nUse this information to evaluate the condition of independence of observations.\nQuestion 7 – Is it reasonable to assume that the observations within a continent are independent of each other?\nQuestion 8 – Is it reasonable to assume that the observations within a year are independent of each other?\nQuestion 9 – Is it reasonable to assume that the observations between continents are independent of each other?\nQuestion 10 – Is it reasonable to assume that the observations between a years are independent of each other?\n\n\nNormality\nNow we will evaluate the normality of the the distributions of grade 8 math scores across years and across continents – the plot you created in #5. Keep in mind, the normality condition is very important when the sample sizes for each group are relatively small.\nQuestion 11 – Is it reasonable to say that the grade 8 math scores across the four years and six continents are normally distributed?\n\n\nEqual Variance\nNow we will evaluate the normality of the the distributions of grade 8 math scores across years and across continents – the plot you created in #5. Keep in mind, the constant variance condition is especially important when the sample sizes differ between groups.\nFor this section, it might be helpful to know the standard deviations for each year / continent combo. I have written code below to provide you with a table of these numbers:\nKeep in mind a standard deviation of NA can happen for two reasons, (1) there is no data, or (2) there is only one observation.\n\nmath_scores %&gt;% \n  group_by(year, continent) %&gt;% \n  summarize(var = var(grade_8_math_score, na.rm = TRUE)\n            ) %&gt;% \n  pivot_wider(names_from = continent, values_from = var)\n\nLooking at the table, we can see that the largest variance of 10257 (North America, 2007) is nearly 27 times larger than the smallest variance of 381 (Europe, 2003). That’s a lot! So, our equal variance condition is definitely violated.\nBut, we have learned tools to attempt to remedy this issue! Let’s take the log of grade_8_math_score and see how the variances compare.\n\nmath_scores %&gt;% \n  group_by(year, continent) %&gt;% \n  summarize(log_var = var(log(grade_8_math_score))\n            ) %&gt;%\n  pivot_wider(names_from = continent, values_from = log_var)\n\nQuestion 12 – Based on the variances in the table above, is it reasonable to say that the log grade 8 math scores across the four years and six continents have equal variability?"
  },
  {
    "objectID": "labs/lab-9.html#one-way-anova-inference",
    "href": "labs/lab-9.html#one-way-anova-inference",
    "title": "Lab 9 – One-Way ANOVA",
    "section": "One-Way ANOVA Inference",
    "text": "One-Way ANOVA Inference\nWe are going to test out both methods for conducting a hypothesis test for an ANOVA – theory-based and simulation-based methods. Keep in mind both methods require independence of observations and equal variability. Normality, however, is only a condition of theory-based methods.\n\nTesting for a Difference Between Years\nSince the distribution of grade 8 math scores across the four years wasn’t horribly not Normal, let’s give a theory-based method a try.\nQuestion 13 – Fill in the code below to conduct a one-way ANOVA modeling the relationship between mean grade 8 math score and the year\nKeep in mind the response variable comes first and the explanatory variable comes second!\n\naov(____ ~ ____, data = math_scores) %&gt;% \n  broom::tidy()\n\nQuestion 14 – At an \\(\\alpha = 0.1\\), what decision would you reach for your hypothesis test?\nQuestion 15 – What would you conclude about the relationship between the mean grade 8 math scores and year?\n\n\nTesting for a Difference Between Continents\nSince the distribution of grade 8 math scores across the six continents didn’t look very Normal, so let’s give a simulation-based method a try.\nI’ve gotten you started by calculating the observed F-statistic for the relationship between a country’s grade 8 math score and its continent.\n\nobs_F &lt;- math_scores %&gt;% \n  specify(response = grade_8_math_score, \n          explanatory = continent) %&gt;% \n  calculate(stat = \"F\")\n\nQuestion 16 – Write the code to generate a permutation distribution of resampled F-statistics.\nQuestion 17 – Visualize the null distribution and shade how the p-value should be calculated\nKeep in mind you only look at the right tail for an ANOVA!\nQuestion 18 – Calculate the p-value for the observed F-statistic\nQuestion 19 – At an \\(\\alpha = 0.1\\), what decision would you reach for your hypothesis test?\nQuestion 20 – What would you conclude about the relationship between the mean grade 8 math scores and continent?"
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "You should have at least one member of your lab group pull up the R resources from Canvas. Specifically, the “cheat sheets” from Weeks 2 & 3 will be very helpful while completing this assignment.\n\n\n\nIn this lab, we will explore and visualize the data using packages housed in the tidyverse suite of packages.\n\n## Package for ggplot and dplyr tools\nlibrary(tidyverse)\n\n## Package for ecological data\nlibrary(lterdatasampler)\n\n## Package for density ridge plots\nlibrary(ggridges)\n\n\n\n\nIn this lab we will work with data from the H.J. Andrews Experimental Forest. The following is a description of the data:\n\nPopulations of West Slope cutthroat trout (Onchorhyncus clarki clarki) in two standard reaches of Mack Creek in the H.J. Andrews Experimental Forest have been monitored since 1987. Monitoring of Pacific Giant Salamanders, Dicamptodon tenebrosus began in 1993. The two standard reaches are in a section of clearcut forest (ca. 1963) and an upstream 500 year old coniferous forest. Sub-reaches are sampled with 2-pass electrofishing, and all captured vertebrates are measured and weighed. Additionally, a set of channel measurements are taken with each sampling. This study constitutes one of the longest continuous records of salmonid populations on record.\n\nFirst, we’ll view the and_vertebrates dataframe where these data are stored.\n\nView(and_vertebrates)\n\n\n\n\nThe codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?and_vertebrates\n\nQuestion 1 – How large is the and_vertebrates dataset? (i.e. How many rows and columns does the dataset have?)\n\n## Your code for question 1 (and 2) goes here!\n\nQuestion 2 – Are there categorical variables in the dataset? If so, what are their names? Hint: What data types are categorical variables stored as?\n\n\n\nThe species variable refers to the species of the animal which was captured. You can use the distinct() function to access the distinct values of a categorical variable (e.g., distinct(nycflights, carrier)). Notice the first input is the name of the dataset and the second input is the name of the categorical variable!\nQuestion 3 – Use the distinct() function to discover the levels / values of the species variable.\n\n## Your code for question 3 goes here!"
  },
  {
    "objectID": "labs/lab-3.html#r-resources",
    "href": "labs/lab-3.html#r-resources",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "You should have at least one member of your lab group pull up the R resources from Canvas. Specifically, the “cheat sheets” from Weeks 2 & 3 will be very helpful while completing this assignment."
  },
  {
    "objectID": "labs/lab-3.html#load-packages",
    "href": "labs/lab-3.html#load-packages",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using packages housed in the tidyverse suite of packages.\n\n## Package for ggplot and dplyr tools\nlibrary(tidyverse)\n\n## Package for ecological data\nlibrary(lterdatasampler)\n\n## Package for density ridge plots\nlibrary(ggridges)"
  },
  {
    "objectID": "labs/lab-3.html#the-data",
    "href": "labs/lab-3.html#the-data",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "In this lab we will work with data from the H.J. Andrews Experimental Forest. The following is a description of the data:\n\nPopulations of West Slope cutthroat trout (Onchorhyncus clarki clarki) in two standard reaches of Mack Creek in the H.J. Andrews Experimental Forest have been monitored since 1987. Monitoring of Pacific Giant Salamanders, Dicamptodon tenebrosus began in 1993. The two standard reaches are in a section of clearcut forest (ca. 1963) and an upstream 500 year old coniferous forest. Sub-reaches are sampled with 2-pass electrofishing, and all captured vertebrates are measured and weighed. Additionally, a set of channel measurements are taken with each sampling. This study constitutes one of the longest continuous records of salmonid populations on record.\n\nFirst, we’ll view the and_vertebrates dataframe where these data are stored.\n\nView(and_vertebrates)"
  },
  {
    "objectID": "labs/lab-3.html#exploring-the-dataset",
    "href": "labs/lab-3.html#exploring-the-dataset",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "The codebook (description of the variables) can be accessed by pulling up the help file by typing a ? before the name of the dataset:\n\n?and_vertebrates\n\nQuestion 1 – How large is the and_vertebrates dataset? (i.e. How many rows and columns does the dataset have?)\n\n## Your code for question 1 (and 2) goes here!\n\nQuestion 2 – Are there categorical variables in the dataset? If so, what are their names? Hint: What data types are categorical variables stored as?"
  },
  {
    "objectID": "labs/lab-3.html#accessing-the-levels-of-a-variable",
    "href": "labs/lab-3.html#accessing-the-levels-of-a-variable",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "",
    "text": "The species variable refers to the species of the animal which was captured. You can use the distinct() function to access the distinct values of a categorical variable (e.g., distinct(nycflights, carrier)). Notice the first input is the name of the dataset and the second input is the name of the categorical variable!\nQuestion 3 – Use the distinct() function to discover the levels / values of the species variable.\n\n## Your code for question 3 goes here!"
  },
  {
    "objectID": "labs/lab-3.html#adding-categorical-variables",
    "href": "labs/lab-3.html#adding-categorical-variables",
    "title": "Lab 3: Incorporating Categorical Variables",
    "section": "Adding Categorical Variables",
    "text": "Adding Categorical Variables\nWhen we are interested in comparing the distribution of a numerical variable across groups of a categorical variable, we “typically” see people use stacked histograms or side-by-side boxplots. I believe an unsung hero of these types of comparisons is the ridge plot.\nAs introduced in Introduction to Modern Statistics, a ridge plot essentially has multiple density plots stacked in the same plotting window. A key feature of ridge plots is a categorical variable is always on the y-axis, with a numeric variable on the x-axis.\nIn R, we use the geom_density_ridges() function from the ggridges package to create a ridge plot. Yes, this is new, but don’t worry! The function has the same layout as things you’ve seen before.\nQuestion 7 – Fill in the code below to create a ridge plot comparing the lengths of Cutthroat trout between the different types of channels (unittype). Be sure to add nice axis labels to your plot, which describe the variables being plotted (and their units)!\n\nggplot(data = trout, \n       mapping = aes(x = &lt;NUMERICAL VARIABLE&gt;, \n                     y = &lt;CATEGORICAL VARIABLE&gt;)\n       ) +\n  geom_density_ridges() \n\nQuestion 8 – Modify your plot from #7 to incorporate the section of the forest into your plot, using either color or facets. Hint: The fill aesthetic will fill the ridge plots with color.\nQuestion 9 – Based on your plot, how different are the lengths of the Cutthroat trout between the different channel types and forest sections? Be sure to address how the centers and shapes of ALL these distributions compare!"
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4: Simple Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lterdatasampler)\n\n# New package from reading -- to get regression table\nlibrary(moderndive)"
  },
  {
    "objectID": "labs/lab-4.html#data-for-today",
    "href": "labs/lab-4.html#data-for-today",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Data for Today",
    "text": "Data for Today\nToday we’ll be working with data on lake ice duration for two lakes surrounding Madison, Wisconsin. This dataset contains information on the number of days of ice (ice duration) on each lake for years between 1851 and 2019. These data are stored in the ntl_icecover dataset, which lives in the lterdatsampler package.\nAccording to the EPA, lake ice duration can be an indicator of climate change. This is because lake ice is dependent on several environmental factors, so changes in these factors will influence the formation of ice on top of lakes. As a result, the study and analysis of lake ice formation can inform scientists about how quickly the climate is changing, and are critical to minimizing disruptions to lake ecosystems."
  },
  {
    "objectID": "labs/lab-4.html#inspecting-the-data",
    "href": "labs/lab-4.html#inspecting-the-data",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Inspecting the Data",
    "text": "Inspecting the Data\nQuestion 1 – How large is the ntl_icecover dataset? (i.e. How many rows and columns does it have?)\n\n# Code to answer question 1 goes here!"
  },
  {
    "objectID": "labs/lab-4.html#visualize-a-simple-linear-regression",
    "href": "labs/lab-4.html#visualize-a-simple-linear-regression",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Visualize a Simple Linear Regression",
    "text": "Visualize a Simple Linear Regression\nLet’s start with tools to visualize and summarize linear regression.\n\nTools\n\nVisualize the relationship between x & y – geom_point()\nVisualize the linear regression line – geom_smooth()\n\nWe will be investigating the relationship between the ice_duration of each lake and the year.\n\n\nStep 1\nQuestion 2 – Make a scatterplot of the relationship between the ice_duration (response) and the year (explanatory). Be sure to make the axis labels look nice, including any necessary units!\n\n# Code to answer question 2 goes here!\n\nQuestion 3 – Describe the relationship you see in the scatterplot. Be sure to address the four aspects we discussed in class: form, direction, strength, and unusual points. Hint: You need to explicitly state where the unusual observations are!\n\n\nStep 2\nTo add a regression line on top of a scatterplot, you add (+) a geom_smooth() layer to your plot. However, if you add a “plain” geom_smooth() to the plot, it uses a wiggly line. You need to tell geom_smooth() what type of smoother line you want for it to use! We can get a straight line by including method = \"lm\" inside of geom_smooth().\nQuestion 4 – Add a linear regression line to the scatterplot you made in Question 3. No code goes here, you need to modify your scatterplot from Question 3!"
  },
  {
    "objectID": "labs/lab-4.html#fit-a-simple-linear-regression-model",
    "href": "labs/lab-4.html#fit-a-simple-linear-regression-model",
    "title": "Lab 4: Simple Linear Regression",
    "section": "Fit a Simple Linear Regression Model",
    "text": "Fit a Simple Linear Regression Model\nNext, we are going to summarize the relationship between ice_duration and year with a linear regression equation.\n\nTools\n\nCalculate the correlation between x & y – get_correlation()\nModel the relationship between x & y – lm()\nExplore coefficient estimates – get_regression_table()\n\n\n\nStep 1\nQuestion 5 – Calculate the correlation between these variables, using the get_correlation() function.\n\n# Code to answer question 5 goes here!\n\n\n\nStep 2\nNext, we will “fit” a linear regression with the lm() function. Remember, the “formula” for lm() is response_variable ~ explanatory_variable. Also recall that you need to tell lm() where the data live using data = argument!\nQuestion 6 – Fit a linear regression modeling the relationship between between ice_duration and year. The only part you need to remove is the ...! Keep the ice_lm &lt;-!\n\n# Code to answer question 6 goes here!\n\nice_lm &lt;- ...\n\n\n\nStep 3\nFinally, to get the regression equation, we need grab the coefficients out of the linear model object you made in Step 2. The get_regression_table() function is a handy tool to do just that!\nQuestion 7 – Use the get_regression_table() function to obtain the coefficient estimates for the ice_lm regression you fit in Question 6.\n\n# Code to answer question 7 goes here!\n\nQuestion 8 – Using the coefficient estimates above, write out the estimated regression equation. Your equation needs to be in the context of the variables, not in generic \\(x\\) and \\(y\\) statements!\nQuestion 9 – Interpret the value of the slope coefficient. Your interpretation needs to be in the context of the variables!\nQuestion 10 – Sometimes interpreting a 1-unit increase in the explanatory variable is not a meaningful change, so we instead use larger increases. Based on your slope interpretation from Q9, what do you expect to happen to the duration of ice cover for an increase of 100 years?"
  },
  {
    "objectID": "labs/lab-4.html#a-preview-of-whats-to-come",
    "href": "labs/lab-4.html#a-preview-of-whats-to-come",
    "title": "Lab 4: Simple Linear Regression",
    "section": "A preview of what’s to come",
    "text": "A preview of what’s to come\nIn our analysis above, we only looked at the relationship between ice duration and year, not accounting for which lake the measurements came from. That is another (categorical) explanatory variable we could include in our regression model!\nQuestion 11 – Using the code you wrote for Question 4 (with the regression line added), add a color for the name of the lake (lakeid).\n\n# Code to answer question 11 goes here!"
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6: Predicting Professor Evaluation Scores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(moderndive)\nlibrary(openintro)\n\nevals &lt;- evals |&gt; \n  mutate(large_class = if_else(cls_students &gt; 100, \n                               \"large class\", \n                               \"regular class\"), \n         eval_completion = cls_did_eval / cls_students \n         ) |&gt; \n  select(-cls_did_eval, \n         -cls_students, \n         -prof_id,\n         -course_id, \n         -bty_f1lower, \n         -bty_f1upper, \n         -bty_f2upper, \n         -bty_m1lower, \n         -bty_m1upper, \n         -bty_m2upper)"
  },
  {
    "objectID": "labs/lab-6.html#your-challenge",
    "href": "labs/lab-6.html#your-challenge",
    "title": "Lab 6: Predicting Professor Evaluation Scores",
    "section": "Your Challenge",
    "text": "Your Challenge\nThis week you have learned about model selection. During class you worked on performing a backward selection process to determine the “best” model for penguin body mass.\nToday, you are going to use forward selection to determine the “best” model for professor’s evaluation score. This task will require you to fit tons of linear regressions. You must be able to show me exactly how you got to your top model. Meaning, I need to see a record of every model you fit and compared along the way."
  },
  {
    "objectID": "labs/lab-6.html#forward-selection",
    "href": "labs/lab-6.html#forward-selection",
    "title": "Lab 6: Predicting Professor Evaluation Scores",
    "section": "Forward Selection",
    "text": "Forward Selection\nThe forward selection process starts with a model with no predictor variables. That means, this model predicts the same mean evaluation score for every professor. I’ve fit this model for you below!\n\none_mean &lt;- lm(score ~ 1, data = evals)\n\nYou can pull out the adjusted \\(R^2\\) for this model using the get_regression_summaries() function.\n\nget_regression_summaries(one_mean)\n\nBased on this output, we are starting with a really low adjusted \\(R^2\\). So, things can only get better from here!\n\nStep 1\nRules: You can only add a variable to the model if it improves the adjusted \\(R^2\\) by at least 2% (0.02).\nAlright, so now we get cooking. The next step is to fit every model with one explanatory variable. I’ve provided a list of every explanatory variable you are allowed to consider!\n\nrank – rank of professor\nethnicity – ethnicity of the professor\ngender – gender of the professor\nlanguage – language of school where professor received education\nage – age of the professor\ncls_perc_eval – the percentage of students who completed the evaluation\ncls_level – class level\ncls_profs – number of professors teaching sections in course: single, multiple\ncls_credits – credits of class: one credit (lab, PE, etc.), multi credit\nbty_avg – average beauty rating of the professor\npic_outfit – outfit of professor in picture\npic_color – color of professor’s picture\nlarge_class – whether the class had over 100 students\neval_completion – proportion of students who completed the evaluation\n\nWoof, that’s 14 different variables. That means, for this first round, you will need to compare the adjusted \\(R^2\\) for 12 different models to decide what variable should be added.\nEvery model you fit will have the same format:\nname_of_model &lt;- lm(score ~ &lt;variable&gt;, data = evals)\nBut, the name of the model will need to change. I’ve started the process for you, using the naming style of one_ followed by the variable name (e.g., one_id, one_bty, etc.).\n\none_rank &lt;- lm(score ~ rank, data = evals)\none_ethnicity &lt;- lm(score ~ ethnicity, data = evals)\none_gender &lt;- lm(score ~ gender, data = evals)\none_language &lt;- lm(score ~ language, data = evals)\none_age &lt;- lm(score ~ age, data = evals)\none_perc_eval &lt;- lm(score ~ cls_perc_eval, data = evals)\none_level &lt;- lm(score ~ cls_level, data = evals)\n\n## Now, you need to fit the other seven models! \n\nAlright, now that you’ve fit the models, you need to inspect the adjusted \\(R^2\\) values to see which of these 14 models is the “top” model – the model with the highest adjusted \\(R^2\\)! Similar to before, I’ve provided you with some code to get you started, but you need to write the remaining code.\n\nget_regression_summaries(one_rank)\nget_regression_summaries(one_ethnicity)\nget_regression_summaries(one_gender)\nget_regression_summaries(one_language)\nget_regression_summaries(one_age)\nget_regression_summaries(one_perc_eval)\nget_regression_summaries(one_level)\n\n## Now, you need to compare the other seven models! \n\n1. What model was your top model? Specifically, which variable was selected to be included?\n\n\nStep 2 - Adding a Second Variable\nAlright, you’ve added one variable, the next step is to decide if you should add a second variable. This process looks nearly identical to the previous step, with one major change: every model you fit needs to contain the variable you decided to add. So, if you decided to add the bty_avg variable, every model you fit would look like this:\nname_of_model &lt;- lm(score ~ bty_avg + &lt;variable&gt;, data = evals)\nAgain, the name of the model will need to change. This round, you are on your own – I’ve provided you with no code. Here are my recommendations:\n\nname each model two_ followed by the names of both variables included in the model (e.g., two_bty_id)\ngo through each variable step-by-step just like you did before\n\n\n## Code to fit all 13 models that add a second variable to your top model goes here!\n\nAlright, now you should have 13 more models to compare! Like before, you need to inspect the adjusted \\(R^2\\) values to see which of these 13 models is the “top” model.\nRules: You can only add a variable to the model if it improves adjusted \\(R^2\\) by at least 2% (0.02) from the model you chose in Question 1.\n\n## Code to compare all 13 models you fit goes here!\n\n2. What model was your top model? State which variables are included in the model you chose!\n\n\nStep 3 - Adding a Third Variable\nAs you might have expected, in this step we add a third variable to our top model from the previous step. This process should be getting familiar at this point!\nThis process of fitting 12-14 models at a time is getting rather tedious! So, I’ve written some code that will carry out this process for us in one pipeline! This is how the code looks:\nevals %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-score,\n         -&lt;VARIABLE 1 SELECTED&gt;,\n         -&lt;VARIABLE 2 SELECTED&gt;\n         ) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\nWoah, that’s a lot. The only thing you need to change is:\n\nadd in the names of the variables you selected in Steps 1 & 2 in the ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals) step\nadd in the names of the variables you selected in Steps 1 & 2 in the select(-ID, -score, -&lt;VARIABLE 1 SELECTED&gt;, -&lt;VARIABLE 2 SELECTED&gt;) step\n\nFor example, if you chose gender and age in Steps 1 and 2, your code on the first line would look like:\nmap(.f = ~lm(score ~ .x + gender + age, data = evals)) %&gt;% \nand your code on the fourth line would look like:\n  select(-score,\n         -gender,\n         -sex\n         ) %&gt;% \nYour turn!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1 & 2\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1 & 2\n\nevals %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-score, \n         -&lt;VARIABLE 1 SELECTED&gt;, \n         -&lt;VARIABLE 2 SELECTED&gt;\n           ) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 2 and see if it improved adjusted \\(R^2\\) by at least 2% (0.02). If so, this variable should be added. If not, then your model from Step 2 is the “best” model!\n3. What model was your top model? State which variables are included in the model you chose!\n\n\nStep 4 - Adding a Fourth Variable\nIf you decided to add a variable in Step 3, then you keep going! If you didn’t add a variable in Step 3, then you stop!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1, 2, & 3\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1, 2 & 3\n\nevals %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-score, \n         -&lt;VARIABLE 1 SELECTED&gt;, \n         -&lt;VARIABLE 2 SELECTED&gt;, \n         -&lt;VARIABLE 3 SELECTED&gt;\n         ) %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 3 and see if it improved adjusted \\(R^2\\) by at least 2% (0.02). If so, this variable should be added. If not, then your model from Step 3 is the “best” model!\n4. What model was your top model? You must state which variables are included in the model you chose!\n\n\nStep 5 - Adding a Fifth Variable\nIf you decided to add a variable in Step 4, then you keep going! If you didn’t add a variable in Step 4, then you stop!\n\n## Change the &lt;VARIABLES SELECTED&gt; in line 2 to the names of the variables you selected in Steps 1, 2, 3 & 4\n## Change the &lt;VARIABLE 1 SELECTED&gt; and &lt;VARIABLE 2 SELECTED&gt; in line 4 to the names of the variables you selected in Steps 1, 2, 3 & 4\n\nevals %&gt;% \n  map(.f = ~lm(score ~ .x + &lt;VARIABLES SELECTED&gt;, data = evals)) %&gt;% \n  map_df(.f = ~get_regression_summaries(.x)$adj_r_squared) %&gt;% \n  select(-score, \n         -&lt;VARIABLE 1 SELECTED&gt;, \n         -&lt;VARIABLE 2 SELECTED&gt;, \n         -&lt;VARIABLE 3 SELECTED&gt;,\n         -&lt;VARIABLE 4 SELECTED&gt;\n          ) %&gt;%  \n  pivot_longer(cols = everything(), \n               names_to = \"variable\", \n               values_to = \"adj_r_sq\") %&gt;% \n  slice_max(adj_r_sq)\n\nThe output of this code is the variable that has the highest adjusted \\(R^2\\). Compare this value to the value of your “top” model from Step 4 and see if it improved adjusted \\(R^2\\) by at least 2% (0.02). If so, this variable should be added. If not, then your model from Step 4 is the “best” model!\n5. What model was your top model? You must state which variables are included in the model you chose!"
  },
  {
    "objectID": "labs/lab-6.html#comparing-with-the-step-function",
    "href": "labs/lab-6.html#comparing-with-the-step-function",
    "title": "Lab 6: Predicting Professor Evaluation Scores",
    "section": "Comparing with the step() Function",
    "text": "Comparing with the step() Function\nLet’s check the forward selection model you found with what model the step() function decides is best. Run the code chunk below to obtain the “best” model chosen by this function.\n\nfull_model &lt;- lm(score ~ ., data = evals)\nstep(full_model, direction = \"forward\")\n\n\nDid the step() function choose the same model as you? If your “best” models do not not agree, why do you think this might have happened?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html",
    "href": "labs/grading-guides/lab-8-grading-guide.html",
    "title": "Lab 8 Grading Guide",
    "section": "",
    "text": "To earn a Success: Creates a scatterplot with the following qualities:\n\nlife expectancy on y-axis\nGDP per capita on the x-axis\naxis labels for both axes, including units\n\nGrowing:\nIf they swap \\(x\\) and \\(y\\):\n\nCareful! What variable should be the response variable (located on the y-axis)?\n\nIf they don’t include axis labels:\n\nCareful! Your axis labels should also include the unit each variable was measured in! If you are unsure what unit a variable was measured in, you can consult the data documentation using ?gapminder. :)"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-1",
    "title": "Lab 8 Grading Guide",
    "section": "",
    "text": "To earn a Success: Creates a scatterplot with the following qualities:\n\nlife expectancy on y-axis\nGDP per capita on the x-axis\naxis labels for both axes, including units\n\nGrowing:\nIf they swap \\(x\\) and \\(y\\):\n\nCareful! What variable should be the response variable (located on the y-axis)?\n\nIf they don’t include axis labels:\n\nCareful! Your axis labels should also include the unit each variable was measured in! If you are unsure what unit a variable was measured in, you can consult the data documentation using ?gapminder. :)"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-2",
    "title": "Lab 8 Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\n\ntakes log of GDP (using scale_x_log10())\nrenames x-axis label to indicate log GDP (or log $) is what is being plotted\n\nGrowing:\nIf they log transform life expectancy (y):\n\nSimilar to the Principle of Parsimony, we want to transform as few variables as is necessary. Look back at the plot with just x log transformed. Is log transforming y and x notably better?\n\nIf their x-axis doesn’t indicate the log was taken:\n\nCareful! You axis label needs to indicate how the original variable was transformed!"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-3",
    "title": "Lab 8 Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success: Code should look like the following:\n\ngapminder_lm &lt;- lm(lifeExp ~ log(gdpPercap), data = gapminder)\n\nGrowing:\nIf they swap lifeExp and gdpPercap:\n\nCareful! What variable comes first in the lm() function? The response or explanatory variable?\n\nIf they don’t use log(gdpPercap) when fitting their regression:\n\nWhat variable did you decide to transform in #2? How should that variable appear in the regression model you are fitting here?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-4",
    "title": "Lab 8 Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\n\nSays it is not reasonable to assume the observations are independent\nDescribes how observations are not independent citing at least one of the following:\n\neach country has repeated observations\nobservations for each country are related in time (temporal correlation)\ncountries of close geographical proximity may share information (spatial correlation)\n\n\nGrowing:\nIf their justification doesn’t talk about the context of the data (e.g., temporal relationships between observations):\n\nYour justification needs to make direct reference to the context of the data.\n\nIf their reasoning doesn’t include any of the above justifications:\n\nHow many observations are their for each country? Are these observations related in some way? If so, how?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-5",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-5",
    "title": "Lab 8 Grading Guide",
    "section": "Question 5",
    "text": "Question 5\nTo earn a Success:\n\n\n\nSays the condition is violated\nJustifies with the left skew of the distribution\n\n\n\n\n\nSays the condition is not violated\nJustifies with characteristics of the distribution\n\n\n\nGrowing:\nIf their justification is insufficient:\n\nWhen evaluating conditions the choices are subjective, so it is necessary for you to justify WHY you made the decision you did. Your justification should make direct reference to characteristics of the distribution of residuals."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-6",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-6",
    "title": "Lab 8 Grading Guide",
    "section": "Question 6",
    "text": "Question 6\nTo earn a Success:\n\nSays the condition is violated\nReferences how the residuals change / decrease for larger values of log(GDP)\n\nGrowing:\nIf they say the condition is not violated:\n\nEqual variance requires that the spread of residuals / the vertical width (e.g., going from -20 to +20) stays the same for ALL values of the explanatory variable (across the x-axis). Is that the case? Why or why not?\n\nIf they say the condition is / is not violated but reference the number of observations above and below the line:\n\nEqual variance is not about having equal spread of points above and below the line – it is okay for there to be more residuals below the line compared to above the line. They key is that the spread of residuals / the vertical width (e.g., going from -20 to +20) stays the same for ALL values of the explanatory variable (across the x-axis). Is that the case? Why or why not?\n\nIf they say the condition is violated but have insufficient justification as to why:\n\nWhen evaluating conditions the choices are subjective, so it is necessary for you to justify WHY you made the decision you did. Your justification should make direct reference to characteristics of the plot of the residuals versus fitted values. Specifically, we are evaluating if the spread of residuals / the vertical width (e.g., going from -20 to +20) stays the same for ALL values of the explanatory variable (across the x-axis). Is that the case? Why or why not?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-7",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-7",
    "title": "Lab 8 Grading Guide",
    "section": "Question 7",
    "text": "Question 7\nTo earn a Success:\n\n\\(H_0\\): there is no linear relationship between log GDP per capita and life expectancy\n\\(H_A\\): there is a linear relationship between log GDP per capita and life expectancy\n\nGrowing:\nIf they say GDP instead of log GDP:\n\nCareful! How did you transform your variable(s) in #2? What variables are you looking at the linear relationship between?\n\nIf they say there is a positive relationship in their alternative:\n\nThe standard hypothesis test for the slope uses a two-sided alternative hypothesis, unless we knew 100% going in that the relationship between \\(x\\) and \\(y\\) was positive. Did you know that the relationship was positive BEFORE you made your visualizations?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-8",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-8",
    "title": "Lab 8 Grading Guide",
    "section": "Question 8",
    "text": "Question 8\nTo earn a Success: Code should look like the following:\n\nobs_slope &lt;-  gapminder %&gt;%\n  specify(response = lifeExp, explanatory = log(gdpPercap)) %&gt;%\n  calculate(stat = \"slope\")\n\nGrowing:\nIf they swap lifeExp and gdpPercap:\n\nCareful! What variable is your response variable?\n\nIf they use GDP instead of log(GDP) for their explanatory:\n\nCareful! You need to be consistent with the transformation you decided in #2."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-9",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-9",
    "title": "Lab 8 Grading Guide",
    "section": "Question 9",
    "text": "Question 9\nTo earn a Success: Code should look like the following:\n\nnull_dist &lt;- gapminder %&gt;%\n  specify(response = lifeExp, explanatory = log(gdpPercap)) %&gt;%\n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"slope\")\n\nGrowing:\nIf they also don’t use the log in this step:\n\nUpdate your code to use the same variable transformation that you use in #8."
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-10",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-10",
    "title": "Lab 8 Grading Guide",
    "section": "Question 10",
    "text": "Question 10\nTo earn a Success: Code should look like the following:\n\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\")\n\n\n\n\n\n\n\nConsistent with alternative hypothesis\n\n\n\nIf they said the relationship was positive in the alternative, then their direction should be \"greater\".\n\n\nGrowing:\n*If they don’t use a direction that is consistent with what they said in their alternative (\"two-sided\" or \"greater\"):\n\nHow many tails are there in the hypotheses stated in #7?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-11",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-11",
    "title": "Lab 8 Grading Guide",
    "section": "Question 11",
    "text": "Question 11\nTo earn a Success: States they reject \\(H_0\\) because the p-value is less than 0.1\nGrowing:\nIf they don’t reference their significance level:\n\nCareful! Hypothesis test decisions can differ based on the significance threshold that was used. What threshold did you use?\n\nIf they do not make a decision:\n\nYou were asked to make a decision regarding the hypotheses, which has two possible options. Which option do you choose and why?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-12",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-12",
    "title": "Lab 8 Grading Guide",
    "section": "Question 12",
    "text": "Question 12\nTo earn a Success: Code should look like the following:\n\nget_regression_table(gapminder_lm, conf.level = 0.9)\n\n\n\n\n\n\n\nIt’s okay if their conf.level isn’t 0.9\n\n\n\nThis doesn’t need to be consistent, since we aren’t making a confidence interval!"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-13",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-13",
    "title": "Lab 8 Grading Guide",
    "section": "Question 13",
    "text": "Question 13\nTo earn a Success: States that the p-value is essentially the same"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-14",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-14",
    "title": "Lab 8 Grading Guide",
    "section": "Question 14",
    "text": "Question 14\nTo earn a Success:\n\nstates that the permutation distribution is approximately Normal\nstates that a t-distribution is a reasonable approximation\n\nGrowing:\nIf they don’t reference the permutation distribution:\n\nTheory-based methods are an approximation for simulation-based methods. Based on the permutation distribution you created, do you believe a t-distribution is a good approximation for THIS permutation distribution? Why or why not?"
  },
  {
    "objectID": "labs/grading-guides/lab-8-grading-guide.html#question-15",
    "href": "labs/grading-guides/lab-8-grading-guide.html#question-15",
    "title": "Lab 8 Grading Guide",
    "section": "Question 15",
    "text": "Question 15\nTo earn a Success: Answer must agree with what they said in #4-6\n\n\nIf they said equal variance and / or independence was violated, they must say that neither p-value is reliable.\n\n\n\nIf they said normality was violated but equal variance and independence were not violated, they must say that the simulation-based p-value is more reliable.\n\n\n\nIf they said none of the conditions were violated, they must say that both p-values are reliable.\n\n\nGrowing:\nIf they choose the wrong method:\n\nLook back at the conditions required for each of these methods. Which conditions did you say were violated? What does that imply for the method(s) which give you the most reliable p-value?"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html",
    "href": "labs/grading-guides/lab-6-grading-guide.html",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "Note\n\n\n\nBased on my code, the model selection process should go:\ncls_credits \\(\\rightarrow\\) bty_avg \\(\\rightarrow\\) gender\nIn Step 4 they are given two variables (cls_perc_eval, eval_completion) which both increase adjusted \\(R^2\\) by 0.018, not the 0.02 which is required to add another variable. So, everyone should stop at Step 4.\n\n\n\n\nTo earn a Success:\n\nfits other 7 models (rank, pic_outfit, pic_color, large_class, eval_completion, cls_level)\nfinds other 7 adjusted \\(R^2\\) values\nstates top model includes cls_credits variable\n\nIf they do not state the top model or what variable was chosen:\n\nCareful! Technically, you told me the name of the object that contains the model you decided was best. I want you to tell me the name of the variable(s) included in that model!\n\n\n\n\nTo earn a Success:\n\nfits 113models that include the variable chosen in #1 as an explanatory variable\nfinds 13 adjusted \\(R^2\\) values\nstates top model includes cls_credits & bty_avg\n\n\n\n\n\n\n\nIndicating Multiple Variables\n\n\n\nThe response needs to indicate in some way that there are two variables included in the model. This can be by saying bty_avg was “added” to the previous model or by stating both variables included in the model.\n\n\nIf they do not indicate that there are two variables in the model (e.g., “beauty average was chosen” or “beauty score was included”):\n\nCareful! Your model contains more variables than just bty_avg. Be specific about naming every variable included in your top model!\n\n\n\n\nTo earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1 and #2 as an explanatory variables\nstates top model includes all the variables they’ve chosen (cls_credits, bty_avg, gender)\n\nIf they don’t include a variable from #1 or #2 as an explanatory variable\n\nCareful! Thus far, you’ve chosen two variables to be included in your model. So, your code (the map() function and select() function) need to have both these variables included, not just one!\n\n\n\n\n\n\n\nIndicating Multiple Variables\n\n\n\nThe response needs to indicate in some way that there are two variables included in the model. This can be by saying bty_avg was “added” to the previous model or by stating both variables included in the model.\n\n\nIf they do not indicate that there are three variables in the model (e.g., “gender was included in the model” or “gender was chosen”):\n\nCareful! You were specifically asked to name EVERY variable included in your top model.\n\n\n\n\nTo earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1, #2, and #3 as an explanatory variables\nstate the top model is the model from Step 3 (cls_credits, bty_avg, gender)\n\nIf they don’t include a variable from #1, #2, or #3 as an explanatory variable\n\nCareful! Thus far, you’ve chosen three variables to be included in your model. So, your code (the map() function and select() function) need to have both these variables included, not just one!\n\nIf they choose to add either cls_perc_eval or eval_completion:\n\nCareful! Look back at the adjusted R-squareds for these variables. Did they increase 2% (0.02) from your last adjusted R-squared?\n\n\n\n\nTo earn a Success:\n\nstates that the step() function did not choose the same model\nstate that this difference was caused by different criteria (or the use of AIC)\n\nIf they don’t talk about the step() function using a different model comparison criteria:\n\nLook back at the output from the step() function, specifically the first line of the output. Based on this line, do you think the step() function is using the same criteria that you used to compare your models (adjusted R-squared)?"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-1",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nfits other 7 models (rank, pic_outfit, pic_color, large_class, eval_completion, cls_level)\nfinds other 7 adjusted \\(R^2\\) values\nstates top model includes cls_credits variable\n\nIf they do not state the top model or what variable was chosen:\n\nCareful! Technically, you told me the name of the object that contains the model you decided was best. I want you to tell me the name of the variable(s) included in that model!"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-2",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nfits 113models that include the variable chosen in #1 as an explanatory variable\nfinds 13 adjusted \\(R^2\\) values\nstates top model includes cls_credits & bty_avg\n\n\n\n\n\n\n\nIndicating Multiple Variables\n\n\n\nThe response needs to indicate in some way that there are two variables included in the model. This can be by saying bty_avg was “added” to the previous model or by stating both variables included in the model.\n\n\nIf they do not indicate that there are two variables in the model (e.g., “beauty average was chosen” or “beauty score was included”):\n\nCareful! Your model contains more variables than just bty_avg. Be specific about naming every variable included in your top model!"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-3",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1 and #2 as an explanatory variables\nstates top model includes all the variables they’ve chosen (cls_credits, bty_avg, gender)\n\nIf they don’t include a variable from #1 or #2 as an explanatory variable\n\nCareful! Thus far, you’ve chosen two variables to be included in your model. So, your code (the map() function and select() function) need to have both these variables included, not just one!\n\n\n\n\n\n\n\nIndicating Multiple Variables\n\n\n\nThe response needs to indicate in some way that there are two variables included in the model. This can be by saying bty_avg was “added” to the previous model or by stating both variables included in the model.\n\n\nIf they do not indicate that there are three variables in the model (e.g., “gender was included in the model” or “gender was chosen”):\n\nCareful! You were specifically asked to name EVERY variable included in your top model."
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-4",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nmodifies code to fit models that include the variables chosen in #1, #2, and #3 as an explanatory variables\nstate the top model is the model from Step 3 (cls_credits, bty_avg, gender)\n\nIf they don’t include a variable from #1, #2, or #3 as an explanatory variable\n\nCareful! Thus far, you’ve chosen three variables to be included in your model. So, your code (the map() function and select() function) need to have both these variables included, not just one!\n\nIf they choose to add either cls_perc_eval or eval_completion:\n\nCareful! Look back at the adjusted R-squareds for these variables. Did they increase 2% (0.02) from your last adjusted R-squared?"
  },
  {
    "objectID": "labs/grading-guides/lab-6-grading-guide.html#question-6",
    "href": "labs/grading-guides/lab-6-grading-guide.html#question-6",
    "title": "Lab 6 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nstates that the step() function did not choose the same model\nstate that this difference was caused by different criteria (or the use of AIC)\n\nIf they don’t talk about the step() function using a different model comparison criteria:\n\nLook back at the output from the step() function, specifically the first line of the output. Based on this line, do you think the step() function is using the same criteria that you used to compare your models (adjusted R-squared)?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html",
    "href": "labs/grading-guides/lab-7-grading-guide.html",
    "title": "Lab 7 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nincludes either site or name inside of group_by()\n\nIf uses another variable (e.g., latitude, water_temp):\n\nYour approach works, since there was only one measurement per site. But what if there wasn’t? This variable doesn’t uniquely define the location of each marsh! What variable(s) do?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-1",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-1",
    "title": "Lab 7 Grading Guide",
    "section": "",
    "text": "To earn a Success:\n\nincludes either site or name inside of group_by()\n\nIf uses another variable (e.g., latitude, water_temp):\n\nYour approach works, since there was only one measurement per site. But what if there wasn’t? This variable doesn’t uniquely define the location of each marsh! What variable(s) do?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-2",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-2",
    "title": "Lab 7 Grading Guide",
    "section": "Question 2",
    "text": "Question 2\nTo earn a Success:\nCreates scatterplot with:\n\nlatitude on x-axis\nwater temperature on y-axis\naxis labels include variable units (Celsius for water temp; degrees for latitude)\n\n\n\n\n\n\n\nNote\n\n\n\nThey are not required to include a regression line, but if they do, great!\n\n\nIf they don’t have axis labels with the units:\n\nIt is important to include an axis label stating what the units of each variable are! If you are wondering what the units of water temperature are, you can look up the help file (using ?pie_crab)\n\nIf they only put temperature on their y-axis:\n\nWhat specific variable is being plotted on the y-axis? Air temperature? Ground temperature? Water temperature? Be specific!"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-3",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-3",
    "title": "Lab 7 Grading Guide",
    "section": "Question 3",
    "text": "Question 3\nTo earn a Success:\nDescription includes all of the following:\n\nform of relationship (linear)\ndirection of relationship (negative)\nstrength of relationship (strong)\nlocation of any outliers (e.g., at a latitude of 45 degrees and 17.5 C temperature)\n\nIf they forget one of these:\n\nCareful! You were asked to describe the form, direction, strength, and unusual points for the plot. Remember, it is important to explicitly state where you believe the outliers are, so the reader knows where to look!\n\nIf they don’t state where the outlier is:\n\nIt is important to explicitly state where you believe the outliers are, so the reader knows where to look!"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-4",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-4",
    "title": "Lab 7 Grading Guide",
    "section": "Question 4",
    "text": "Question 4\nTo earn a Success:\nCode should look like the following:\n\nobs_slope &lt;- obs_slope &lt;- marsh_info %&gt;% \n  specify(response = water_temp, \n          explanatory = latitude) %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-5",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-5",
    "title": "Lab 7 Grading Guide",
    "section": "Question 5",
    "text": "Question 5\nTo earn a Success:\nCode should look like the following:\n\nbootstrap_dist &lt;- marsh_info %&gt;% \n  specify(response = water_temp, \n          explanatory = latitude) %&gt;% \n  generate(reps = 500, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-6",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-6",
    "title": "Lab 7 Grading Guide",
    "section": "Question 6",
    "text": "Question 6\nTo earn a Success:\nCode should look like the following:\n\nvisualise(bootstrap_dist) + \n  labs(x = \"&lt;SOME AXIS LABEL&gt;\")\n\nTheir axis label must describe what is being plotted (e.g., slope statistics, sample slope, etc.)\nIf they did not include an x-axis label:\n\nIt is important to include an axis label stating what is being plotted! What statistics are being plotted on this bootstrap distribution? Your axis label should tell the reader what statistics you are plotting!\n\nIf their x-axis label is not about the slope statistic:\n\nWhat is being plotted in this distribution? What is it a distribution of? What statistics did you calculate in the previous step that are being plotted here?\n\nIf their y-axis label is something other than number of resamples:\n\nCareful! This is a histogram of the bootstrap statistics that you found in #5. Think back to when we learned about histograms, what do the numbers (e.g., 25, 50, 75) represent? Hint: this is not a variable in the dataset!"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-7",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-7",
    "title": "Lab 7 Grading Guide",
    "section": "Question 7",
    "text": "Question 7\nTo earn a Success:\nCode should look like the following:\n\nget_confidence_interval(bootstrap_dist, \n                        level = 0.90, \n                        type = \"percentile\")\n\n\n\n\n\n\n\n\"percentile\" is the default\n\n\n\nSo, it is okay if they don’t specify it!\n\n\nIf they use a level other than 0.90:\n\nLook back at the lab instructions, what percentage confidence interval were you asked to construct?"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-8",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-8",
    "title": "Lab 7 Grading Guide",
    "section": "Question 8",
    "text": "Question 8\nTo earn a Success:\nThe interpretation must state:\n\nconfidence: they are 90% confident\nstatistic: the slope between water temperature and latitude\npopulation: for all marshes along the eastern US\ninterval: is between [lower bound] and [upper bound]\n\n\n\n\n\n\n\nNote\n\n\n\nNote that every group will get a different interval, due to the randomness of bootstrapping!\n\n\nIf they don’t state their confidence:\n\nHow much confidence do you have in your interval?\n\nIf they don’t state the statistic in context:\n\nWe need to be specific about the what parameter we believe is in our interval. The slope statistic is measuring the relationship between which variables?\n\nIf they have state an incorrect population:\n\nCareful! Are you analyzing data on crabs or on marshes? Where were these marshes located? What population would you characterize these 13 marshes belonging to? That is the population your interval applies to!"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-9",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-9",
    "title": "Lab 7 Grading Guide",
    "section": "Question 9",
    "text": "Question 9\nTo earn a Success:\nCode should look like the following:\n\nget_confidence_interval(bootstrap_dist, \n                        level = 0.90, \n                        point_estimate = obs_slope, \n                        type = \"se\")\n\nIf they don’t have type = \"se\":\n\nWhat method do you want the function to use when calculating your confidence interval? Percentile intervals are the default!\n\n\n\n\n\n\n\nNote\n\n\n\nIf they type in the actual number for the estimated slope rather than using the obs_slope object, you can give them this reminder:\n\nIt is better practice not to “hard-code” numbers in our code, but to reference values stored in objects. This makes our code more resistant to changes in the dataset."
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-10",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-10",
    "title": "Lab 7 Grading Guide",
    "section": "Question 10",
    "text": "Question 10\nTo earn a Success:\n\nstates the intervals are similar\nstates similarity is due to the normality of the bootstrap distribution\n\nIf they say the intervals are different:\n\nGive these intervals another look, yes their endpoints differ by a bit, but are they vastly different intervals?\n\nIf they don’t talk about the bootstrap distribution being approximately normal:\n\nWhat condition do we need to check when using the SE method to construct confidence intervals? Hint: it is a condition about the shape of the bootstrap distribution"
  },
  {
    "objectID": "labs/grading-guides/lab-7-grading-guide.html#question-11",
    "href": "labs/grading-guides/lab-7-grading-guide.html#question-11",
    "title": "Lab 7 Grading Guide",
    "section": "Question 11",
    "text": "Question 11\nTo earn a Success:\n\nstates whether they do / do not believe the sample of 13 marshes is representative of all marshes along the Atlantic coast\njustifies why using something something that is true\n\n\n\n\n\n\n\nNote\n\n\n\nI’m willing to be pretty flexible here, but the key aspect is that they need to justify their reasoning.\n\n\nIf their justification is about crabs:\n\nCareful! Are you analyzing data on crabs or on marshes? The population of marshes you define in #8 is what you should use to assess if the assumption of bootstrapping is reasonable.\n\nIf their justification is not reasonable:\n\nBootstrapping assumes that the original sample is representative of the population from which the observations were sampled. Where were these 13 marshes located geographically? Do you believe that these 13 marshes are representative of the broader population they were sampled from?\n\nIf their justification is about the shape of the bootstrap distribution:\n\nYou are on the right track, we need for the sample to be representative of the population from which it was sampled. However, we cannot assess the representativeness of a sample using a bootstrap distribution. This can only be assessed by inspecting how the data were collected."
  },
  {
    "objectID": "labs/grading-guides/lab-4-grading-guide.html",
    "href": "labs/grading-guides/lab-4-grading-guide.html",
    "title": "Lab 4: Grading Guide",
    "section": "",
    "text": "Question 1 – Size of ntl_icecover dataset\nSuccess:\n\nhas glimpse() somewhere in their code\nstates there are 334 rows and 5 columns\n\nGrowing:\n\nIf no code is present\nIf they provide incorrect size of data\n\n\n\n\n\n\n\nNote\n\n\n\nIf they say there are 431 rows, they got their answer from the help file!\nFeedback: Careful! The best way to obtain information about the dataset is using the glimpse() function!\n\n\n\n\nQuestion 2 – Scatterplot\nSuccess: Code should look like the following\n\nggplot(data = ntl_icecover, \n       mapping = aes(y = ice_duration, x = year)) +\n  geom_point() +\n  labs(x = \"Year\", \n       y = \"Ice Duration (days)\")\n\nGrowing:\n\nDoesn’t change y-axis labels\n\nFeedback: You were asked to give your visualization nice axis labels. Also, it is important for the axis label to contain the units of the variable. What unit was ice duration measured in?\n\nSwaps the x- and y-variable\n\nFeedback: Careful! Which variable is supposed to be the response?\n\n\nQuestion 3 – Describe the regression line\nSuccess: Addresses form, direction, strength and unusual points.\n\nForm: Linear\nDirection: Negative\nStrength: Moderate\nOutliers: Maybe the 2001 observation with ~20 days of ice?\n\nGrowing:\n\nIf they don’t discuss one of the for aspects.\n\nFeedback: You were asked to describe the form, direction, strength, and unusual points. Make sure you discuss all four aspects!\n\nIf they say there are unusual points / outliers but don’t say where. The description should point to the exact x and y location (not just one or the other).\n\nFeedback: If you believe there are unusual observations you need to tell the reader where these points are, which includes both their x- and y-coordinates!\n\n\nQuestion 4 – Add a regression line\nSuccess:\n\nIncludes regression line with geom_smooth() and method = \"lm\" option\n\n\n\n\n\n\n\nNote\n\n\n\nThey don’t need to turn the standard errors off (se = FALSE), but that’s fine if they did!\n\n\nGrowing:\n\nIf they do not add regression line\n\nFeedback: For this question you needed to add a regression line to the previous scatterplot!\n\nIf they do not use method = \"lm\" to get straight line\n\nFeedback: You need to have a straight line, not a wiggly line! Take a look at the R resources to see how you specify a straight line!\n\n\nQuestion 5 – Find correlation\nSuccess: Code should look like the following\n\nget_correlation(data = ntl_icecover, \n                ice_duration ~ year, \n                na.rm = TRUE)\n\nGrowing:\n\nDoesn’t remove NAs\n\nFeedback: It is important to remove the NAs before calculation the correlation! Is there another argument (input) for the get_correlation() function that allows you to remove the missing values? Hint: it looks like the option we used to remove missing values in the mean() function in Week 3!\n\n\nQuestion 6 – Fit linear regression\nSuccess: Code should look like the following\n\nmy_model &lt;- lm(ice_duration ~ year, data = ntl_icecover)\n\nGrowing:\n\nSwaps response and explanatory\n\nFeedback: Careful! With the lm() function the response variable goes first (before the ~) and the explanatory comes second (after the ~). Which variable were you instructed to use as the response?\nWhen you make this change, your coefficient table (and resulting interpretations) will change! These changes to Questions 8, 9, 10, and 11 should be included in your revisions.\n\n\nQuestion 7 – Get regression table\nSuccess: Code should look like the following\n\nget_regression_table(my_model)\n\nGrowing:\n\nDoesn’t use the get_regression_table() function\n\nFeedback: We want out model output to look tidy, which is why we use the get_regression_table() function!\n\n\nQuestion 8 – Write out regression equation\nSuccess:\n\\[ \\widehat{\\text{ice duration}} = 495 - 0.203 \\times (\\text{year})\\]\n\nIndicates the response (ice duration) is estimated / predicted / the mean with either a hat or with language\nInputs values in correct location\nReferences year and ice duration (not x and y)\n\nGrowing:\n\nDoes not indicate the response (ice duration) is estimated / predicted / the mean\n\nFeedback: Remember, the estimated regression equation has a hat over y. What does that hat mean? How can you incorporate that language into your interpretation?\n\nUses x and y instead of variable names\n\nFeedback: When writing out the regression equation, you need to reference the context of the line that was found. What variable was the response? What variable was the explanatory?\n\n\n\n\n\n\nNote\n\n\n\nIf they use x and y in the equation but then define what variables they are associated with, that is okay!\n\n\n\n\nQuestion 9 – Interpret slope\nSuccess:\n\nCorrect interpretation of slope for 1 year increase\n\n1 year increase\nmean / predicted / estimated ice duration\ndecreases by 0.203 days\n\n\nGrowing:\n\n\n\nDoesn’t interpret the value of -0.203\n\nFeedback: Careful! You did not interpret what the value of -0.203 means in the context of the regression line!\n\nStates the duration of ice increases\n\nFeedback: Look at the sign associated with the slope coefficient! What does the sign tell you about whether ice duration should increase or decrease?\n\nDoesn’t indicate the response is mean / estimate / predicted ice duration\n\nFeedback: Careful! Your interpretation sounds like it is guaranteed that every 1 year increase in time will result in exactly a 0.203 day decrease in ice duration. Will there always be exactly this decrease? How do we indicate that our slope estimate is not exact?\n\nUses the word “cause”\n\nFeedback: The word “cause” in statistics means a very specific thing. We can only use cause and effect statements in an experiment. Was this study an experiment? If not, what word would be more appropriate to use?\n\n\nQuestion 10 – 100 Year Increase\n\nCorrect interpretation of slope for 100 year increase\n\n100 year increase\nmean / predicted / estimated / approximate ice duration\ndecreases by 20.3 days\n\n\nGrowing:\n\nUses y-intercept when increasing 100 years\n\nFeedback: Careful! Look back at your interpretation for Question 9. Did you include the y-intercept when you interpreted the value of the slope? How can you translate what you did in Question 9 to an increase of 100 years instead of 1 year?\n\nDon’t provide an estimate of the change\n\nFeedback: You need to use your estimated regression equation to state what the expected change in the ice duration would be if the number of years was increased by 100.\n\n\nQuestion 11 – Multivariate plot\nSuccess: Code should look like the following\n\nggplot(data = ntl_icecover, \n       mapping = aes(y = ice_duration, x = year, fill = \"lakeid\")) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Year\", \n       y = \"Ice Duration (days)\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf they got a Growing for Question 4 from using a wiggly line, they don’t need to get a Growing here!\n\n\nGrowing:\n\nIf they use fill instead of color\n\nFeedback: I know it is confusing, but to change the color of the points and the lines in a scatterplot we need to use the color aesthetic, not the fill."
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html",
    "href": "weeks/tutorial/infer-CI-tutorial.html",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "",
    "text": "Per the California Privacy Rights Act (CPRA), the salaries of employees of the state of California are required to be public. We will revisit the 2019 salaries of head coaches from CSU and UC universities introduced this week.\nFirst, let’s load the coaches data set into your workspace and take a look at what the data look like.\n\n\nglimpse(coaches)"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#data",
    "href": "weeks/tutorial/infer-CI-tutorial.html#data",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "",
    "text": "Per the California Privacy Rights Act (CPRA), the salaries of employees of the state of California are required to be public. We will revisit the 2019 salaries of head coaches from CSU and UC universities introduced this week.\nFirst, let’s load the coaches data set into your workspace and take a look at what the data look like.\n\n\nglimpse(coaches)"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-1",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-1",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 1",
    "text": "Section 1\n\nStarting with one sample\nFor our investigation, we are interested in estimating the median salary for all CSU and UC coaches. In class we talked about creating the sampling distribution of the median salary, since I had the salaries of every CSU and UC head coach.\nToday, I’ve given you a simple random sample of 50 coaches from the master dataset of all CSU and UC coaches. This context should feel more familiar — you are interested in estimating the value of the true median salary of all CSU and UC head coaches, given a sample of 50 coaches.\nThe population median salary of all CSU and UC coaches is: $137,619. Let’s see how close our sample median is to this population median!\nUse the summarize() function to calculate the median Total Pay & Benefits for your sample of 50 coaches.\n\n\n\n\n\n\n\nsummarize(coaches)\n# What goes inside the summarize() function?\n\n\n\n\nsummarize(coaches, \n          median = median(`Total Pay & Benefits`))\n\n\nPlotting the data is possibly more important than calculating the sample statistic, since it gives us an idea of the distribution of these 50 salaries.\nCreate a histogram of the salaries from the sample of 50 coaches. I’ve given you some code to start with, but you’ll need to add to it!\n\n\nggplot(data = coaches, \n       mapping = aes(x = ___)) \n\n\n\n\n\n# Use the numerical variable we are interested in!\nggplot(data = coaches, \n       mapping = aes(x = `Total Pay & Benefits`)) \n\n\n\n\n# Add a histogram to the plot\nggplot(data = coaches, \n       mapping = aes(x = `Total Pay & Benefits`)) +\n  geom_histogram()\n\n\n\n\n# Make the histogram look a bit better with binwidths! \nggplot(data = coaches, \n       mapping = aes(x = `Total Pay & Benefits`)) + \n  geom_histogram(binwidth = 75000)"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-2",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-2",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 2",
    "text": "Section 2\nNow, let’s take our single sample of 50 and see what we might have gotten from other random samples!\n\nOne Resample\nWe’ll start with getting one bootstrap resample. To do this we take our original sample (stored in the coaches dataset) and resample with replacement 50 times. The rep_sample_n() function from the infer package helps us obtain one of these resamples. All we need to do is specify the size and replace arguments.\nKeep in mind: how large your resample should be and whether you should sample with or without replacement\n\n\none_resample &lt;- rep_sample_n(coaches, \n                             size = ___, \n                             replace = ___)\n\n\n\n\n\none_resample &lt;- rep_sample_n(coaches, \n                             size = 50, \n                             replace = ___)\n\n\n\n\none_resample &lt;- rep_sample_n(coaches, \n                             size = 50, \n                             replace = TRUE)\n\n\nLet’s compare our resample with our original sample. Your original sample had a median of $137,619. Use the summarize() function to find the median of this resample.\n\n\n\n\n\n\nHint: Remember to use the one_resample dataset!\n\n\n\nMultiple Resamples\nNow, let’s take what we learned about creating and summarizing one bootstrap resample and scale it up!\nFirst, we’re going to create 500 different bootstrap resamples, each of size 50. We’re still using the rep_sample_n() function to do this, but now we specify the number of resamples we want with the reps argument.\nModify your previous code to create 500 bootstrap resamples.\n\n\nmultiple_resamples &lt;- rep_sample_n(coaches, \n                                   size = ___, \n                                   replace = ___, \n                                   reps = ___)\n\n\n\n\n\n# These are the options we used for one resample! \nmultiple_resamples &lt;- rep_sample_n(coaches, \n                                   size = 50, \n                                   replace = TRUE, \n                                   reps = ___)\n\n\n\n\n# We want 500 resamples! \nmultiple_resamples &lt;- rep_sample_n(coaches, \n                                   size = 50, \n                                   replace = TRUE, \n                                   reps = 500)\n\n\nNow that we have 500 resamples we need to summarize each sample with a single statistic. In our investigation we are interested in the sample median, so that’s the statistic we will use.\nCalculate the median Total Pay & Benefits for each resample. The multiple_resamples dataset contains 25000 numbers — 50 observations for 500 samples. So, to get each resample’s median we need to group_by() the sample ID column (replicate) before we calculate the median.\n\n\nmultiple_resamples %&gt;% \n\n\n\n\n\n# Make different groups for each sample\nmultiple_resamples %&gt;% \n  group_by(replicate)\n\n\n\n\n# Find the median of each group\nmultiple_resamples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(median = median(`Total Pay & Benefits`))"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-3",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-3",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 3",
    "text": "Section 3\nFor confidence intervals, we’d like to have a method that is a bit more dynamic than what we’ve been doing with rep_sample_n(), group_by(), and summarize(). The infer package includes tools that help us create bootstrap resamples, visualize the distribution of our bootstrap statistics, and find confidence intervals.\n\nThe infer pipeline\nGenerating bootstrap statistics has a similar process to what we used previously, but it has a very different feel. The process looks something like this:\nLet’s walk through each of the components of the infer “pipeline.”\nStep 1: specify() your response (and explanatory) variable(s)\nThis step declares to R what variable(s) you are interested in. For this tutorial we are only interested in one variable: Total Pay & Benefits. In the future, we will include an explanatory variable to help explain the variability in the response variable.\n\nStep 2: generate() resamples\nThe generate() step takes your original data and generates bootstrap resamples. It knows how many resamples to generate from the quantity specified in the reps argument. Additionally, it knows to sample with replacement when the type argument is set to \"bootstrap\".\n\nStep 3: calculate() bootstrap statistics\nNow that you’ve generated lots of bootstrap resamples, you need to summarize them with a single statistic. That statistic is what you tell the calculate() function to find! There are lots of different statistics we will explore, but for this investigation our stat will be a \"median\".\n\n\n\nYour turn!\nTake what you’ve learned about the infer pipeline and create a new dataset, named coaches_resample that contains median salaries calculated from 1000 bootstrap resamples.\n\n\ncoaches_resample &lt;- coaches %&gt;% \n  specify(___) %&gt;% \n  generate(___) %&gt;% \n  calculate(___)\n\n\n\n\n\n# Specify what your response variable is!\ncoaches_resample &lt;- coaches %&gt;% \n  specify(response = `Total Pay & Benefits`) %&gt;% \n  generate(___) %&gt;% \n  calculate(___)\n\n\n\n\n# Generate 1000 bootstrap resamples!\ncoaches_resample &lt;- coaches %&gt;% \n  specify(response = `Total Pay & Benefits`) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(___)\n\n\n\n\n# Calculate the median for each resample! \ncoaches_resample &lt;- coaches %&gt;% \n  specify(response = `Total Pay & Benefits`) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(stat = \"median\")"
  },
  {
    "objectID": "weeks/tutorial/infer-CI-tutorial.html#section-4",
    "href": "weeks/tutorial/infer-CI-tutorial.html#section-4",
    "title": "Creating a Bootstrap Distribution with infer",
    "section": "Section 4",
    "text": "Section 4\nAlright, our final step is to use our bootstrap statistics to calculate a confidence interval for the true median salary of all CSU and UC head coaches.\nA nice first step is to visualize how the distribution of bootstrap statistics looks. The coaches_resample object contains 1000 medians calculated from 1000 different bootstrap resamples. Let’s use the built-in visualize() function to make a quick histogram of these bootstrap medians.\n\nvisualise(coaches_resample)\n\n\n\n\n\n\n\n\nIt looks like the majority of medians fall between $170,398 and $131,463, but let’s quantify this range with a confidence interval.\nThe get_confidence_interval() function is what we use to find a confidence interval from a set of bootstrap statistics. This function takes three arguments:\n\na dataset containing bootstrap statistics\nthe level of confidence that should be used\nthe type of method to use when making the interval\n\nThis looks something like:\n\n\n\nUsing these tools, calculate a 99% confidence interval for the population median using the 1000 bootstrap statistics you found previously. Use the percentile method to calculate your confidence interval.\nWhen you are done, preview what your confidence interval looks like!\n\n\ncoaches_CI &lt;- coaches_resample %&gt;% \n  get_confidence_interval(level = ___, type = ___)\n\n\n\n\n\n# For a 99% interval your level is 0.99\ncoaches_CI &lt;- coaches_resample %&gt;% \n  get_confidence_interval(level = 0.99, type = ___)\n\n\n\n\n# For a percentile CI you use the \"percentile\" method\ncoaches_CI &lt;- coaches_resample %&gt;% \n  get_confidence_interval(level = 0.99, type = \"percentile\")\n\n\nHow would you interpret the interval you got?"
  },
  {
    "objectID": "weeks/reading-guide/week1-answers.html",
    "href": "weeks/reading-guide/week1-answers.html",
    "title": "Answers to Questions on Week 1 Reading Guide",
    "section": "",
    "text": "In a data frame, rows correspond to: observations\nIn a data frame, columns correspond to: variables\n\nTrue or False: A pair of variables can be both associated AND independent.\nFalse – They can only be one or the other!\n\nTrue or False: Given a pair of variables, one will always be the explanatory variable and one the response variable.\nTechnically, this is false. For a variable to be the “explanatory variable” it needs to explain the changes in the “response variable”. You could have two variables that are not related at all.\n\nTrue or False: If a study does have an explanatory and a response variable, that means changes in the explanatory variable must cause changes in the response variable.\nFalse! For an explanatory variable to cause changes in a response variable, we need to have an experiment where the explanatory variable is randomly assigned.\n\nTrue or False: Observational studies can show a naturally occurring association between variables.\nTrue! Observational studies can show associations between variables, but they cannot show that one variable causes changes in another. This is because observational studies do not have random assignment."
  },
  {
    "objectID": "weeks/reading-guide/week1-answers.html#chapter-1-notes",
    "href": "weeks/reading-guide/week1-answers.html#chapter-1-notes",
    "title": "Answers to Questions on Week 1 Reading Guide",
    "section": "",
    "text": "In a data frame, rows correspond to: observations\nIn a data frame, columns correspond to: variables\n\nTrue or False: A pair of variables can be both associated AND independent.\nFalse – They can only be one or the other!\n\nTrue or False: Given a pair of variables, one will always be the explanatory variable and one the response variable.\nTechnically, this is false. For a variable to be the “explanatory variable” it needs to explain the changes in the “response variable”. You could have two variables that are not related at all.\n\nTrue or False: If a study does have an explanatory and a response variable, that means changes in the explanatory variable must cause changes in the response variable.\nFalse! For an explanatory variable to cause changes in a response variable, we need to have an experiment where the explanatory variable is randomly assigned.\n\nTrue or False: Observational studies can show a naturally occurring association between variables.\nTrue! Observational studies can show associations between variables, but they cannot show that one variable causes changes in another. This is because observational studies do not have random assignment."
  },
  {
    "objectID": "weeks/reading-guide/week1-answers.html#chapter-2-notes",
    "href": "weeks/reading-guide/week1-answers.html#chapter-2-notes",
    "title": "Answers to Questions on Week 1 Reading Guide",
    "section": "Chapter 2 Notes",
    "text": "Chapter 2 Notes\nTrue or False: Convenience sampling tends to result in non-response bias.\nFalse! Convenience sampling makes it so our sample is not representative of the population, or we have a biased sample. Non-response bias requires the people sampled to choose not to respond.\n\nTrue or False: Volunteer sampling tends to result in response bias.\n**True! If our sample is made up of volunteers, it is more likely to have people with strong views. This makes it so our \nTrue or False: Random sampling helps to resolve selection bias, but has no impact on non-response or response bias.\n\nTrue or False: Observational studies can show an association between two variables, but cannot determine a causal relationship.\nTrue! We need random assignment of our explanatory variable to determine if a relationship is causal.\n\nTrue or False: In order for an experiment to be valid, a placebo must be used.\nTrue! An experiment needs to have a “baseline” treatment to compare with. \nTrue or False: If random sampling of the target population is used, and no other types of bias is suspected, results from the sample can be generalized to the entire target population.\nTrue! Mathematically, we know that random sampling should make our sample look similar to the population, so we are not systematically including / excluding certain individuals more / less.\n\nTrue or False: If random sampling of the target population is used, and no other types of bias are suspected, results from the sample can be inferred as a causal relationship between the explanatory and response variables.\nFalse! Random sampling allows us to infer that our sample is representative of the population, it does not allow us to say anything about the relationship between the explanatory and response variables."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html",
    "href": "weeks/chapters/week7-reading1.html",
    "title": "Week 7 – Exploring Sampling",
    "section": "",
    "text": "This week, we begin our adventure into statistical inference by learning about sampling. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we’ll cover in the second portion of this week’s reading and next week. We will see that the skills you learned during the first three weeks of class, in particular data visualization and data wrangling, will also play an important role in the development of your understanding of sampling.\nThis week’s reading comes primarily from Chater 7 from ModernDive (Kim et al. 2020), with a smattering of my own ideas."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sampling-bowl-activity",
    "href": "weeks/chapters/week7-reading1.html#sampling-bowl-activity",
    "title": "Week 7 – Exploring Sampling",
    "section": "1 Sampling bowl activity",
    "text": "1 Sampling bowl activity\nLet’s start with a hands-on activity.\n\n1.1 What proportion of this bowl’s balls are red?\n\n\n\n\n\n\nFigure 1: A bowl with red and white balls.\n\n\n\nTake a look at the bowl in Figure 1. It has a certain number of red and a certain number of white balls all of equal size. Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls.\nLet’s now ask ourselves, what proportion of this bowl’s balls are red?\nOne way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process.\n\n\n1.2 Using the shovel once\n\n\n\n\n\n\nFigure 2: Inserting a shovel into the bowl.\n\n\n\n\n\n\n\n\n\nFigure 3: Removing 50 balls from the bowl.\n\n\n\nInstead of performing an exhaustive count, let’s insert a shovel into the bowl as seen in Figure 2. Using the shovel, let’s remove \\(5 \\times 10 = 50\\) balls, as seen in Figure 3.\nObserve that 17 of the balls are red and thus 0.34 = 34% of the shovel’s balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34% took much less time and energy to make.\nHowever, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% again? Maybe?\n\n\n\n\n\n\n\n\n \n\n\n\n\nFigure 4: Repeating sampling activity 33 times.\n\n\n\nWhat if we repeated this activity several times following the process, as shown in Figure 4? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl’s balls that are red be exactly 34% every time? Surely not. Let’s repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition.\n\n\n1.3 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 balls each.\nCount the number of red balls and thus compute the proportion of the 50 balls that are red.\nReturn the balls into the bowl.\nMix the contents of the bowl a little to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then marks their proportion of their 50 balls that were red in the appropriate bin in a hand-drawn histogram as seen in Figure 5.\n\n\n\n\n\n\nFigure 5: Constructing a histogram of proportions.\n\n\n\nRecall from Week 2 that histograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in Figure 5.\n\n\n\n\n\n\nFigure 6: Hand-drawn histogram of first 10 out of 33 proportions.\n\n\n\nObserve the following in the histogram in Figure 6:\n\nAt the low end, one group removed 50 balls from the bowl with proportion red between 0.20 and 0.25.\nAt the high end, another group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution.\nThe shape of this distribution is somewhat bell-shaped.\n\nLet’s construct this same hand-drawn histogram in R using your data visualization skills that you honed in Week 2! We saved our 33 groups of friends’ results in the tactile_prop_red data frame included in the moderndive package. Run the following to display the first 10 of 33 rows:\n\ntactile_prop_red\n\n# A tibble: 33 × 4\n   group            replicate red_balls prop_red\n   &lt;chr&gt;                &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n 1 Ilyas, Yohan             1        21     0.42\n 2 Morgan, Terrance         2        17     0.34\n 3 Martin, Thomas           3        21     0.42\n 4 Clark, Frank             4        21     0.42\n 5 Riddhi, Karina           5        18     0.36\n 6 Andrew, Tyler            6        19     0.38\n 7 Julia                    7        19     0.38\n 8 Rachel, Lauren           8        11     0.22\n 9 Daniel, Caroline         9        15     0.3 \n10 Josh, Maeve             10        17     0.34\n# ℹ 23 more rows\n\n\nObserve for each group that we have their names, the number of red_balls they obtained, and the corresponding proportion out of 50 balls that were red named prop_red. We also have a replicate variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red.\nLet’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05 in Figure 7. This is a computerized and complete version of the partially completed hand-drawn histogram you saw in Figure 6. Note that setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. This helps us to more closely align this histogram with the hand-drawn histogram in Figure 6.\n\nggplot(tactile_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", \n       title = \"Distribution of 33 proportions red\") \n\n\n\n\n\n\n\nFigure 7: Distribution of 33 proportions based on 33 samples of size 50.\n\n\n\n\n\n\n\n1.4 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We would like to know the proportion of the bowl’s balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a sample of 50 balls using the shovel to make an estimate. Using this sample of 50 balls, we estimated the proportion of the bowl’s balls that are red to be 34%.\nMoreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure 7. This is known as the concept of sampling variation.\nThe purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling:\n\nUnderstanding the effect of sampling variation.\nUnderstanding the effect of sample size on sampling variation.\n\nIn the next section, we’ll mimic the hands-on sampling activity we just performed on a computer. This will allow us not only to repeat the sampling exercise much more than 33 times, but it will also allow us to use shovels with different numbers of slots than just 50.\nAfterwards, we’ll present you with definitions, terminology, and notation related to sampling. As in many disciplines, such necessary background knowledge may seem inaccessible and even confusing at first. However, as with many difficult topics, if you truly understand the underlying concepts and practice, practice, practice, you’ll be able to master them.\nTo close this chapter, we’ll generalize the “sampling from a bowl” exercise to other sampling scenarios and present a theoretical result known as the Central Limit Theorem."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sec-sampling-simulation",
    "href": "weeks/chapters/week7-reading1.html#sec-sampling-simulation",
    "title": "Week 7 – Exploring Sampling",
    "section": "2 Virtual sampling",
    "text": "2 Virtual sampling\nIn the previous section, we performed a tactile sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we’ll mimic this tactile sampling activity with a virtual sampling activity using a computer. In other words, we’ll use a virtual analog to the bowl of balls and a virtual analog to the shovel.\n\n2.1 Using the virtual shovel once\nLet’s start by performing the virtual analog of the tactile sampling exercise we performed before. We first need a virtual analog of the bowl seen in Figure Figure 1. To this end, we included a data frame named bowl in the moderndive package. The rows of bowl correspond exactly with the contents of the actual bowl.\n\nbowl\n\n# A tibble: 2,400 × 2\n   ball_ID color\n     &lt;int&gt; &lt;chr&gt;\n 1       1 white\n 2       2 white\n 3       3 white\n 4       4 red  \n 5       5 white\n 6       6 white\n 7       7 red  \n 8       8 white\n 9       9 red  \n10      10 white\n# ℹ 2,390 more rows\n\n\nObserve that bowl has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable ball_ID is used as an identification variable – none of the balls in the actual bowl are marked with numbers. The second variable color indicates whether a particular virtual ball is red or white. You can view the contents of the bowl in RStudio’s data viewer, scrolling through the contents to convince yourself that bowl is indeed a virtual analog of the actual bowl in Figure 1.\nNow that we have a virtual analog of our bowl, we next need a virtual analog to the shovel seen in Figure 3 to generate virtual samples of 50 balls. We’re going to use the rep_sample_n() function included in the infer package. This function allows us to take repeated, or replicated, samples of size n from a given dataset.\nLooking at the code below you should notice:\n\nThe dataset (bowl) is the first argument\nsize corresponds to the size of each sample\nreps corresponds to the number of samples\nreplace corresponds to whether each sampled observation should be replaced after it is drawn\n\n\nvirtual_shovel &lt;- rep_sample_n(bowl,\n                               size = 50, \n                               replace = FALSE,\n                               reps = 1)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1     306 red  \n 2         1     800 red  \n 3         1     996 white\n 4         1    1143 white\n 5         1     874 red  \n 6         1     189 white\n 7         1    2021 white\n 8         1    1093 white\n 9         1    1759 white\n10         1    1413 white\n# ℹ 40 more rows\n\n\nObserve that virtual_shovel has 50 rows corresponding to our virtual sample of size 50. The ball_ID variable identifies which of the 2400 balls from bowl are included in our sample of 50 balls while color denotes its color. However, what does the replicate variable indicate? In virtual_shovel’s case, replicate is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to the first repeated/replicated use of the shovel, in our case our first sample. We’ll see shortly that when we “virtually” take 33 samples, replicate will take values between 1 and 33.\nLet’s compute the proportion of balls in our virtual sample that are red using the dplyr data wrangling verbs you learned in Week 3. First, for each of our 50 sampled balls, let’s identify if it is red or not using a test for equality with ==. Let’s create a new Boolean variable is_red using the mutate() function:\n\nmutate(.data = virtual_shovel, \n       is_red = (color == \"red\")\n       )\n\n# A tibble: 50 × 4\n# Groups:   replicate [1]\n   replicate ball_ID color is_red\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; \n 1         1     306 red   TRUE  \n 2         1     800 red   TRUE  \n 3         1     996 white FALSE \n 4         1    1143 white FALSE \n 5         1     874 red   TRUE  \n 6         1     189 white FALSE \n 7         1    2021 white FALSE \n 8         1    1093 white FALSE \n 9         1    1759 white FALSE \n10         1    1413 white FALSE \n# ℹ 40 more rows\n\n\nObserve that for every row where color == \"red\", the Boolean (logical) value TRUE is returned and for every row where color is not equal to \"red\", the Boolean FALSE is returned.\nSecond, let’s compute the number of balls out of 50 that are red using the summarize() function. Recall from Week 3 that summarize() takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the mean() or median(). In this case, we use the sum() function:\n\nmutate(.data = virtual_shovel, \n       is_red = (color == \"red\")\n       ) %&gt;% \nsummarize(num_red = sum(is_red)\n          )\n\n# A tibble: 1 × 2\n  replicate num_red\n      &lt;int&gt;   &lt;int&gt;\n1         1      17\n\n\nWhy does this work? Because R treats TRUE like the number 1 and FALSE like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of balls where color is red. In our case, 17 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling.\nThird and lastly, let’s compute the proportion of the 50 sampled balls that are red by dividing num_red by 50:\n\nmutate(.data = virtual_shovel, \n       is_red = (color == \"red\")\n       ) %&gt;% \nsummarize(num_red = sum(is_red) / 50\n          )\n\n# A tibble: 1 × 2\n  replicate num_red\n      &lt;int&gt;   &lt;dbl&gt;\n1         1    0.34\n\n\nGreat! 34% of virtual_shovel’s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the bowl’s balls that are red is 34%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 34% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure 6. We saw that these estimates varied. Let’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n\n2.2 Using the virtual shovel 33 times\nRecall that in our tactile sampling exercise, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated / replicated using the shovel 33 times. We can perform this repeated / replicated sampling virtually by once again using our virtual shovel function rep_sample_n(), but by adding the reps = 33 argument. This is telling R that we want to repeat the sampling 33 times.\nWe’ll save these results in a data frame called virtual_samples. We provide a preview of the first 10 rows of virtual_samples below:\n\n\n# A tibble: 1,650 × 3\n# Groups:   replicate [33]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1      27 red  \n 2         1    1706 white\n 3         1     538 white\n 4         1    2309 white\n 5         1    1244 red  \n 6         1    1092 red  \n 7         1     894 white\n 8         1    1591 white\n 9         1      38 white\n10         1    2191 red  \n# ℹ 1,640 more rows\n\n\nObserve that the first 50 rows of replicate are equal to 1. If you were to continue scrolling through the dataset (like you can in RStudio’s data previewer), you would find that the next 50 rows of replicate are equal to 2. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all reps = 33 replicates and thus virtual_samples has 33 \\(\\times\\) 50 = 1650 rows.\nLet’s now take virtual_samples and compute the resulting 33 proportions red. We’ll use the same dplyr verbs as before, but this time with an additional group_by() of the replicate variable. Recall from Week 3 that by assigning the grouping variable before we summarize(), we’ll obtain a different summary statistic for each level of our group variable. Thus, we should end up with 33 different proportions, since we have 33 different replicates. We display a preview of the first 10 out of 33 rows:\n\n\n# A tibble: 33 × 2\n   replicate prop_red\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1     0.44\n 2         2     0.34\n 3         3     0.34\n 4         4     0.44\n 5         5     0.4 \n 6         6     0.34\n 7         7     0.36\n 8         8     0.46\n 9         9     0.48\n10        10     0.26\n# ℹ 23 more rows\n\n\nAs with our 33 groups of friends’ tactile samples, there is variation in the resulting 33 virtual proportions red. Let’s visualize this variation in a histogram! In Figure 8 we’ve added binwidth = 0.05 and boundary = 0.4 arguments as well. Recall that setting boundary = 0.4 ensures a binning scheme with one of the bins’ boundaries at 0.4. Since the binwidth = 0.05 is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well.\n\nggplot(data = virtual_prop_red, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\") \n\n\n\n\n\n\n\nFigure 8: Distribution of 33 proportions based on 33 samples of size 50.\n\n\n\n\n\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 40% (for 11 out of 33 samples). Why do we have these differences in proportions red? Because of sampling variation.\nLet’s now compare our virtual results with our tactile results from the previous section in Figure 9. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\n\n\n\n\n\n\n\nFigure 9: Comparing 33 virtual and 33 tactile proportions red.\n\n\n\n\n\n\n\n2.3 Using the virtual shovel 1000 times\nNow say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000! We have two choices at this point:\n\nWe could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. This would be a tedious and time-consuming task and would likely leave our friends upset with us.\nWe could use the power of R to automate this repetitive task!\n\nI believe all of us would choose option 2, so let’s abandon tactile sampling in favor of only virtual sampling. Once again let’s use the rep_sample_n() function with sample size set to be 50 once again, but this time with the number of replicates (reps) set to 1000.\n\nvirtual_samples &lt;- rep_sample_n(bowl, \n                                size = 50, \n                                reps = 1000, \n                                replace = FALSE)\nvirtual_samples\n\n# A tibble: 50,000 × 3\n# Groups:   replicate [1,000]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1    1178 white\n 2         1     212 red  \n 3         1    1023 white\n 4         1    1127 white\n 5         1      38 white\n 6         1     452 red  \n 7         1    2072 white\n 8         1    2254 white\n 9         1     111 white\n10         1     669 white\n# ℹ 49,990 more rows\n\n\nObserve that now virtual_samples has 1000 \\(\\times\\) 50 = 50000 rows, instead of the 33 \\(\\times\\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let’s take the data frame virtual_samples with 1000 \\(\\times\\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls.\n\nvirtual_prop_red &lt;- virtual_samples %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 50) \n\nvirtual_prop_red\n\n# A tibble: 1,000 × 2\n   replicate prop_red\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1     0.26\n 2         2     0.32\n 3         3     0.26\n 4         4     0.48\n 5         5     0.3 \n 6         6     0.24\n 7         7     0.3 \n 8         8     0.44\n 9         9     0.42\n10        10     0.4 \n# ℹ 990 more rows\n\n\nObserve that we now have 1000 replicates of prop_red, the proportion of 50 balls that are red. Using the same code as earlier, let’s now visualize the distribution of these 1000 replicates of prop_red in a histogram in Figure 10.\n\nggplot(virtual_prop_red, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\", \n       title = \"Distribution of 1000 proportions red\") \n\n\n\n\n\n\n\nFigure 10: Distribution of 1000 proportions based on 1000 samples of size 50.\n\n\n\n\n\nOnce again, the most frequently occurring proportions of red balls occur between 35% and 40%. Every now and then, we obtain proportions as low as between 20% and 25%, and others as high as between 55% and 60%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, approximated well by a normal distribution. At this point we recommend you read the “Normal distribution” section (Appendix Section 5) for a brief discussion on the properties of the normal distribution.\n\n\n2.4 Using different shovels\nNow say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100.\n\n\n\n\n\n\n\n\nA shovel with 25 slots\n\n\n\n\n\n\n\nA shovel with 50 slots\n\n\n\n\n\n\n\n\n\nA shovel with 100 slots\n\n\n\n\n\n\nFigure 11: Three shovels to extract three different sample sizes.\n\n\n\nIf your goal is still to estimate the proportion of the bowl’s balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the “best” guess of the proportion of the bowl’s balls that are red. Let’s define some criteria for “best” in this subsection.\nUsing our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes! In other words, let’s use rep_sample_n() with size set to 25, 50, and 100, respectively, while keeping the number of repeated/replicated samples at 1000:\n\nVirtually use the appropriate shovel to generate 1000 samples with size balls.\nCompute the resulting 1000 replicates of the proportion of the shovel’s balls that are red.\nVisualize the distribution of these 1000 proportions red using a histogram.\n\nRun each of the following code segments individually and then compare the three resulting histograms.\n\n# Segment 1: sample size = 25 ------------------------------\n# 1.a) Virtually use shovel 1000 times\nvirtual_samples_25 &lt;- rep_sample_n(bowl, \n                                   size = 25, \n                                   reps = 1000, \n                                   replace = FALSE)\n\n# 1.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_25 &lt;- virtual_samples_25 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 25)\n\n# 1.c) Plot distribution via a histogram\nggplot(data = virtual_prop_red_25, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 25 balls that were red\") \n\n# Segment 2: sample size = 50 ------------------------------\n# 2.a) Virtually use shovel 1000 times\nvirtual_samples_50 &lt;- rep_sample_n(bowl, \n                                   size = 50, \n                                   reps = 1000, \n                                   replace = FALSE)\n\n# 2.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_50 &lt;- virtual_samples_50 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") /50) \n\n# 2.c) Plot distribution via a histogram\nggplot(data = virtual_prop_red_50, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 50 balls that were red\")  \n\n# Segment 3: sample size = 100 ------------------------------\n# 3.a) Virtually using shovel with 100 slots 1000 times\nvirtual_samples_100 &lt;- rep_sample_n(bowl, \n                                    size = 100, \n                                    reps = 1000)\n\n# 3.b) Compute resulting 1000 replicates of proportion red\nvirtual_prop_red_100 &lt;- virtual_samples_100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 100) \n\n# 3.c) Plot distribution via a histogram\nggplot(data = virtual_prop_red_100, \n       mapping = aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of 100 balls that were red\") \n\n\n\n\n\n\n\n\n\n\n\n(a) n = 25\n\n\n\n\n\n\n\n\n\n\n\n(b) n = 50\n\n\n\n\n\n\n\n\n\n\n\n(c) n = 100\n\n\n\n\n\n\n\nFigure 12: Comparing the distributions of proportion red for different sample sizes.\n\n\n\n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure Figure 12.\nObserve that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure 12, all three histograms appear to center around roughly 40%.\nWe can be numerically explicit about the amount of variation in our three sets of 1000 values of prop_red using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable (see Appendix Section 5) for a brief discussion on the properties of the standard deviation. For all three sample sizes, let’s compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the sd() summary function.\n\n# n = 25\nvirtual_prop_red_25 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# A tibble: 1 × 1\n      sd\n   &lt;dbl&gt;\n1 0.0945\n\n# n = 50\nvirtual_prop_red_50 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# A tibble: 1 × 1\n      sd\n   &lt;dbl&gt;\n1 0.0671\n\n# n = 100\nvirtual_prop_red_100 %&gt;% \n  summarize(sd = sd(prop_red))\n\n# A tibble: 1 × 1\n      sd\n   &lt;dbl&gt;\n1 0.0482\n\n\nLet’s compare these three measures of distributional variation in Table 1.\n\n\n\n\nTable 1: Comparing standard deviations of proportions red for three different shovels.\n\n\n\n\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.095\n\n\n50\n0.067\n\n\n100\n0.048"
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sampling-framework",
    "href": "weeks/chapters/week7-reading1.html#sampling-framework",
    "title": "Week 7 – Exploring Sampling",
    "section": "3 Sampling framework",
    "text": "3 Sampling framework\nIn both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to estimate the proportion of the bowl’s balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure 12 and Table 1 – comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nNow that you have built some intuition relating to sampling, let’s now attach words and labels to the various concepts we’ve explored so far. Specifically in the next section, we’ll introduce terminology and notation as well as statistical definitions related to sampling. This will allow us to succinctly summarize and refer to the ideas behind sampling for the rest of this book.\n\n3.1 Terminology and notation\nLet’s now attach words and labels to the various sampling concepts we’ve seen so far by introducing some terminology and mathematical notation. While they may seem daunting at first, we’ll make sure to tie each of them to sampling bowl activities you performed earlier. Furthermore, throughout this book we’ll give you plenty of opportunity for practice, as the best method for mastering these terms is repetition.\nThe first set of terms and notation relate to populations:\n\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population. We mathematically denote the population’s size using upper-case \\(N\\).\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the average height of all Canadians, the population parameter of interest is the population mean.\nA census is an exhaustive enumeration or counting of all \\(N\\) individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number \\(N\\) of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\nSo in our sampling activities, the population is the collection of \\(N\\) = 2400 identically sized red and white balls in the bowl shown in Figure 1. Recall that we also represented the bowl “virtually” in the data frame bowl:\n\nbowl\n\n# A tibble: 2,400 × 2\n   ball_ID color\n     &lt;int&gt; &lt;chr&gt;\n 1       1 white\n 2       2 white\n 3       3 white\n 4       4 red  \n 5       5 white\n 6       6 white\n 7       7 red  \n 8       8 white\n 9       9 red  \n10      10 white\n# ℹ 2,390 more rows\n\n\nThe population parameter here is the proportion of the bowl’s balls that are red. Whenever we’re interested in a proportion of some value in a population, the population parameter has a specific name: the population proportion. We denote population proportions with the letter \\(p\\). We’ll see later on that we can also consider other types of population parameters, like population means and population slopes.\nIn order to compute this population proportion \\(p\\) exactly, we need to first conduct a census by going through all \\(N\\) = 2400 and counting the number that are red. We then divide this count by 2400 to obtain the proportion red.\nYou might be now asking yourself: “Wait. I understand that performing a census on the actual bowl would take a long time. But can’t we conduct a ‘virtual’ census using the virtual bowl?” You are absolutely correct! In fact when the ModernDive authors created the bowl data frame, they made its contents match the contents of actual bowl not by doing a census, but by reading the contents written on the box the bowl came in!\nLet’s conduct this “virtual” census by using the same dplyr verbs you used earlier to count the number of balls that are red:\n\nsummarize(bowl, \n          total_red = sum(color == \"red\")\n          ) \n\n# A tibble: 1 × 1\n  total_red\n      &lt;int&gt;\n1       900\n\n\nSince 900 of the 2400 are red, the proportion is 900/2400 = 0.375 = 37.5%. So we know the value of the population parameter: in our case, the population proportion \\(p\\) is equal to 0.375.\nAt this point, you might be further asking yourself: “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Great question! Normally, you wouldn’t do any sampling! However, the sampling activities we did this chapter are merely simulations of how sampling is done in real-life! We perform these simulations in order to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nIn real-life sampling not only will the population size \\(N\\) be very large making a census expensive, but sometimes we won’t even know how big the population is! For now however, we press on with our next set of terms and notation.\nThe second set of terms and notation relate to samples:\n\nSampling is the act of collecting a sample from the population, which we generally only do when we can’t perform a census. We mathematically denote the sample size using lower case \\(n\\), as opposed to upper case \\(N\\) which denotes the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). Thus sampling is a much cheaper alternative than performing a census.\nA point estimate, also known as a sample statistic, is a summary statistic computed from a sample that estimates the unknown population parameter.\n\nSo previously we conducted sampling using a shovel with 50 slots to extract samples of size \\(n\\) = 50. To perform the virtual analog of this sampling, recall that we used the rep_sample_n() function as follows:\n\nvirtual_shovel &lt;- rep_sample_n(bowl, \n                               size = 50, \n                               reps = 1, \n                               replace = FALSE)\nvirtual_shovel\n\n# A tibble: 50 × 3\n# Groups:   replicate [1]\n   replicate ball_ID color\n       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;\n 1         1     231 red  \n 2         1    1308 red  \n 3         1    1306 white\n 4         1    2274 red  \n 5         1    1627 white\n 6         1    2195 white\n 7         1     527 red  \n 8         1    1744 white\n 9         1    1054 red  \n10         1    1223 red  \n# ℹ 40 more rows\n\n\nUsing the sample of 50 balls contained in virtual_shovel, we generated an estimate of the proportion of the bowl’s balls that are red prop_red\n\nvirtual_shovel %&gt;% \n  summarize(prop_red = sum(color == \"red\") / 50) \n\n# A tibble: 1 × 2\n  replicate prop_red\n      &lt;int&gt;    &lt;dbl&gt;\n1         1     0.44\n\n\nSo in our case, the value of prop_red is the point estimate of the population proportion \\(p\\) since it estimates the latter’s value. Furthermore, this point estimate has a specific name when considering proportions: the sample proportion. It is denoted using \\(\\widehat{p}\\) because it is a common convention in statistics to use a “hat” symbol to denote point estimates.\nThe third set of terms relate to sampling methodology: the method used to collect samples. You’ll see here and throughout the rest of your book that the way you collect samples directly influences their quality.\n\nA sample is said to be representative if it roughly “looks like” the population. In other words, if the sample’s characteristics are a “good” representation of the population’s characteristics.\nWe say a sample is generalizable if any results based on the sample can generalize to the population. In other words, if we can make “good” guesses about the population using the sample.\nWe say a sampling procedure is biased if certain individuals in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every individual in a population has an equal chance of being sampled.\n\nWe say a sample of \\(n\\) balls extracted using our shovel is representative of the population if it’s contents “roughly resemble” the contents of the bowl. If so, then the proportion of the shovel’s balls that are red can generalize to the proportion of the bowl’s \\(N\\) = 2400 balls that are red. Or expressed differently, \\(\\widehat{p}\\) is a “good guess” of \\(p\\). Now say we cheated when using the shovel and removed a number of white balls in favor of red balls. Then this sample would be biased towards red balls, and thus the sample would no longer be representative of the bowl.\nThe fourth and final set of terms and notation relate to the goal of sampling:\n\nOne way to ensure that a sample is unbiased and representative of the population is by using random sampling\nInference is the act of “making a guess” about some unknown. Statistical inference is the act of making a guess about a population using a sample.\n\nIn our case, since the rep_sample_n() function uses your computer’s random number generator, we were in fact performing random sampling.\nLet’s now put all four sets of terms and notation together, keeping our sampling activities in mind:\n\nSince we extracted a sample of \\(n\\) = 50 balls at random, we mixed all of the equally sized balls before using the shovel, then\nthe contents of the shovel are unbiased and representative of the contents of the bowl, thus\nany result based on the shovel can generalize to the bowl, thus\nthe sample proportion \\(\\widehat{p}\\) of the \\(n\\) = 50 balls in the shovel that are red is a “good guess” of the population proportion \\(p\\) of the bowl’s \\(N\\) = 2400 balls that are red, thus\ninstead of conducting a census of the 2400 balls in the bowl, we can infer about the bowl using the sample from the shovel.\n\nWhat you have been performing is statistical inference. This is one of the most important concepts in all of statistics. So much so, we included this term in the title of our book: “Statistical Inference via Data Science”. More generally speaking,\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can infer about the population using sampling.\n\nIn the second reading this week, we’ll introduce the infer package, which makes statistical inference “tidy” and transparent.\n\n\n3.2 Statistical definitions\nTo further attach words and labels to the various sampling concepts we’ve seen so far, we also introduce some important statistical definitions related to sampling. As a refresher of our 1000 repeated / replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 2, let’s display Figure 12 again.\n\n\n\n\n\n\n\n\n\nn = 25\n\n\n\n\n\n\n\nn = 50\n\n\n\n\n\n\n\nn = 100\n\n\n\n\n\nThese types of distributions have a special name: sampling distributions of point estimates. Their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \\(\\widehat{p}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we can typically expect. Unfortunately, the term sampling distribution is often confused with a sample’s distribution which is merely the distribution of the values in a single sample.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around 0.4 = 40%. Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of 0.2 = 20% when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions in Table 1, which we display again below:\n\n\n\nPreviously seen comparing standard deviations of proportions red for three different shovels\n\n\nNumber of slots in shovel\nStandard deviation of proportions red\n\n\n\n\n25\n0.095\n\n\n50\n0.067\n\n\n100\n0.048\n\n\n\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: standard error of a point estimate. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel’s balls that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nSimilarly to confusion between sampling distributions with a sample’s distribution, people often confuse the standard error with the standard deviation. This is especially the case since a standard error is itself a kind of standard deviation. The best advice we can give is that a standard error is merely a kind of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error.\nTo help reinforce these concepts, let’s re-display Figure 12 but using our new terminology, notation, and definitions relating to sampling in Figure 13.\n\n\n\n\n\n\n\n\nFigure 13: Three sampling distributions of the sample proportion \\(\\widehat{p}\\).\n\n\n\n\n\nFurthermore, let’s re-display Table 1 but using our new terminology, notation, and definitions relating to sampling in Table 2.\n\n\n\n\nTable 2: Standard errors of the sample proportion based on sample sizes of 25, 50, and 100\n\n\n\n\n\n\nSample size (n)\nStandard error of $\\widehat{p}$\n\n\n\n\nn = 25\n0.095\n\n\nn = 50\n0.067\n\n\nn = 100\n0.048\n\n\n\n\n\n\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n\n3.3 The moral of the story\nLet’s recap this section so far. We’ve seen that if a sample is generated at random, then the resulting point estimate is a “good guess” of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \\(\\widehat{p}\\) of the shovel’s balls that were red was a “good guess” of the population proportion \\(p\\) of the bowl’s balls that were red.\nHowever, what do we mean by our point estimate being a “good guess”? Sometimes, we’ll get an estimate that is less than the true value of the population parameter, while at other times we’ll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will “on average” be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion.\nIn our sampling activities, sometimes our sample proportion \\(\\widehat{p}\\) was less than the true population proportion \\(p\\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \\(\\widehat{p}\\) were “on average” correct and thus were centered at the true value of the population proportion \\(p\\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an accurate estimate.\nRecall from earlier that the value of the population proportion \\(p\\) of the \\(N\\) = 2400 balls in the bowl was 900/2400 = 0.375 = prop_red * 100%. We computed this value by performing a virtual census of bowl. Let’s re-display our sampling distributions from Figure 12 and Figure 13, but now with a vertical red line marking the true population proportion \\(p\\) of balls that are red = 37.5% in Figure 14. We see that while there is a certain amount of error in the sample proportions \\(\\widehat{p}\\) for all three sampling distributions, on average the \\(\\widehat{p}\\) are centered at the true population proportion red \\(p\\).\n\n\n\n\n\n\n\n\nFigure 14: Three sampling distributions with population proportion \\(p\\) marked by vertical line.\n\n\n\n\n\nWe also saw in this section that as your sample size \\(n\\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing standard error. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \\(\\widehat{p}\\) decreased. You can observe this behavior in Figure 14. This is also known as having a precise estimate.\nSo random sampling ensures our point estimates are accurate, while on the other hand having a large sample size ensures our point estimates are precise. While the terms “accuracy” and “precision” may sound like they mean the same thing, there is a subtle difference. Accuracy describes how “on target” our estimates are, whereas precision describes how “consistent” our estimates are. Figure 15 illustrates the difference.\n\n\n\n\n\n\nFigure 15: “Comparing accuracy and precision.”\n\n\n\nAt this point, you might be asking yourself: “Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn’t we be taking only one sample that’s as large as possible?”. If you did ask yourself these questions, your suspicion is correct! Recall from earlier when we asked ourselves “If we had a way of knowing that the proportion of the balls that are red is 37.5%, then why did we do any sampling?” Similarly, we took 1000 repeated samples as a simulation of how sampling is done in real-life! We used these simulations to study:\n\nThe effect of sampling variation on our estimates.\nThe effect of sample size on sampling variation.\n\nThis is not how sampling is done in real life! In a real-life scenario, we wouldn’t take 1000 repeated/replicated samples, but rather a single sample that’s as large as we can afford. In the next chapter you will read about methods we can use to approximate these sampling distributions, when we have only one sample."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#central-limit-theorem",
    "href": "weeks/chapters/week7-reading1.html#central-limit-theorem",
    "title": "Week 7 – Exploring Sampling",
    "section": "4 Central Limit Theorem",
    "text": "4 Central Limit Theorem\nThis chapter began with (virtual) access to a large bowl of balls (our population) and a desire to figure out the proportion of red balls. Despite having access to this population, in reality, you almost never will have access to the population, either because the population is too large, ever changing, or too expensive to take a census of. Accepting this reality means accepting that we need to use statistical inference.\nIn Section 3.1, we stated that “statistical inference is the act of making a guess about a population using a sample.” But how do we do this inference? In the previous section, we defined the sampling framework only to state that in reality we take one large sample, instead of many samples as done in the sampling framework (which we modeled physically by taking many samples from the bowl).\nIn reality, we take only one sample and use that one sample to make statements about the population parameter. This ability of making statements about the population is allowable by a famous theorem, or mathematically proven truth, called the Central Limit Theorem. What you visualized in Figure 12 and Figure 13 and summarized in Table 1 and Table 2 was a demonstration of this theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow.\nIn other words, as our sample size gets larger (1) the sampling distribution of a point estimate (like a sample proportion) increasingly follows a normal distribution and (2) the variation of these sampling distributions gets smaller, as quantified by their standard errors. We discuss the properties of the normal distribution in Appendix Section 5.1.\nShuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created a 3-minute and 38-second video at https://youtu.be/jvoxEYmQHNM explaining this crucial statistical theorem using the average weight of wild bunny rabbits and the average wingspan of dragons as examples. It’s very clever!\n\n\n\n\n\n\nFigure 16\n\n\n\nHere’s what is so surprising about the Central Limit Theorem: regardless of the shape of the underlying population distribution, the sampling distribution of means (such as the sample mean of bunny weights or the sample mean of the length of dragon wings) and proportions (such as the sample proportion red in our shovels) will be normal. Normal distributions are defined by where they are centered and how wide they are, and the Central Limit Theorem gives us both:\n\nThe sampling distribution of the point estimate is centered at the true population parameter\nWe have an estimate for how wide the sampling distribution of the point estimate is, given by the standard error (which we will discuss in the next chapter on confidence intervals)\n\nWhat the Central Limit Theorem creates for us is a ladder between a single sample and the population. By the Central Limit Theorem, we can say that (1) our sample’s point estimate is drawn from a normal distribution centered at the true population parameter and (2)that the width of that normal distribution is governed by the standard error of our point estimate. Relating this to our bowl, if we pull one sample and get the sample proportion of red balls \\(\\widehat{p}\\), this value of \\(\\widehat{p}\\) is drawn from the normal curve centered at the true population proportion of red balls \\(p\\) with the computed standard error."
  },
  {
    "objectID": "weeks/chapters/week7-reading1.html#sec-appendix",
    "href": "weeks/chapters/week7-reading1.html#sec-appendix",
    "title": "Week 7 – Exploring Sampling",
    "section": "5 Appendix",
    "text": "5 Appendix\n\n5.1 Normal distribution\nLet’s review one specific distribution: the Normal Distribution. Bell shaped distributions like the Normal Distribution are defined by two values: (1) the mean \\(\\mu\\) (“mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (“sigma”) which determines the variation of the distribution. In Figure 17, we plot three normal distributions where:\n\nThe solid normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).\nThe dotted normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).\nThe dashed normal curve has mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\n\n\n\n\n\n\n\n\n\nFigure 17: Three normal distributions.\n\n\n\n\n\nNotice how the solid and dotted line Normal Distributions have the same center due to their common mean \\(\\mu\\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \\(\\sigma\\) = 5. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \\(\\sigma\\) = 2. However, they are centered at different locations.\nWhen the mean \\(\\mu\\) = 0 and the standard deviation \\(\\sigma\\) = 1, the normal distribution has a special name. It’s called the Standard Normal Distribution or the \\(z\\)-curve.\nFurthermore, if a variable follows a normal curve, there are three rules of thumb we can use:\n\n68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean.\n95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean.\n99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean.\n\nLet’s illustrate this on a standard normal curve in Figure 18. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example:\n\nThe middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values.\nThe middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5% = 95% of the area under the curve. In other words, 95% of values.\nThe middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values.\n\n\n\n\n\n\n\n\n\nFigure 18: Rules of thumb about areas under normal curves."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html",
    "href": "weeks/chapters/week8-reading.html",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "",
    "text": "In this week’s coursework we are finally talking about p-values! What we learned last week should have helped make the connection between a sampling distribution and a bootstrap (resampling) distribution. Hopefully, you understand that we create confidence intervals based the statistics we saw in other samples.\nThis week we are going to connect these ideas to the framework of hypothesis testing. Hypothesis testing requires an additional component we didn’t see last week—the null hypothesis. We will learn how we integrate the null hypothesis in our resampling procedure to create a sampling distribution that could have happened if the null hypothesis was true. We will use this distribution to compare what we saw in our data and evaluate the plausibility of competing hypotheses.\nThis reading will walk you through the general framework for understanding hypothesis tests. By understanding this general framework, you’ll be able to adapt it to many different scenarios. The same can be said for confidence intervals. There was one general framework that applies to all confidence intervals. I believe this focus on the conceptual framework is better for long-term learning than focusing on specific details for specific instances."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#reading-guide",
    "href": "weeks/chapters/week8-reading.html#reading-guide",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "0.1 Reading Guide",
    "text": "0.1 Reading Guide\nDownload the reading guide as a Word Document here"
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#observed-data",
    "href": "weeks/chapters/week8-reading.html#observed-data",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.1 Observed data",
    "text": "1.1 Observed data\nFigure 1 visualizes the relationship between mage and weight for this sample of 1,000 birth records.\n\nggplot(data = births14, \n       mapping = aes(x = mage, y = weight)) + \n  geom_jitter() + \n  geom_smooth(method = \"lm\") +\n  labs(x = \"Mother's Age\", \n       y = \"Birth Weight of Baby (lbs)\") + \n  my_theme\n\n\n\n\n\n\n\nFigure 1: Weight of baby at birth (in lbs) as explained by mother’s age.\n\n\n\n\n\nTable 1 displays the estimated regression coefficients for modeling the relationship between mage and weight for this sample of 1,000 birth records.\n\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n\nget_regression_table(births_lm)\n\n\n\n\n\nTable 1: The least squares estimates of the intercept and slope for modeling the relationship between baby’s birth weight and mother’s age.\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\nintercept\n6.793\n\n\nmage\n0.014\n\n\n\n\n\n\n\n\n\n\nBased on these coefficients, the estimated regression equation is:\n\\[ \\widehat{\\text{birth weight}} = -6.793 + 0.014 \\times \\text{mother's age}\\]\nBased on the regression equation, it appear that for every year older a mother is, we would expect the mean birth weight to increase by approximately 0.014 lbs. This seems like a small value, which is confirmed when we increase the mother’s age by 10-years, which is associated with a 0.14 lb increase in birth weight."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#research-question",
    "href": "weeks/chapters/week8-reading.html#research-question",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.2 Research question",
    "text": "1.2 Research question\nThis raises the question of whether this change is “large1 enough” to suggest that there is a relationship between the birth weight of a baby and the age of the mother. Could we obtain a slope statistic of 0.014 occur just by chance, in a hypothetical world where there is no relationship between a baby’s birth weight and a mother’s age? In other words, what role does sampling variation play in this hypothetical world? To answer this question, we’ll again rely on a computer to run simulations."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-ht-activity",
    "href": "weeks/chapters/week8-reading.html#sec-ht-activity",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.3 Simulating data",
    "text": "1.3 Simulating data\nFirst, try to imagine a hypothetical universe where there is no relationship between the birth weight of a baby and a mother’s age. In such a hypothetical universe, the birth weight of a baby would be entirely determined from other variables (e.g., genetics, mother’s habits, etc.)\nBringing things back to the births14 data frame, the mage variable would thus be an irrelevant label, as is has no relationship with the birth weight of the baby. Since is has no bearing on the birth weight, we could randomly reassign these ages by “shuffling” them!\nTo illustrate this idea, let’s narrow our focus to six arbitrarily chosen birth records (of the 1,000) Table 2. The weight column displays the birth weight of the baby. The mage column displays the age of the mother. However, in our hypothesized universe there is no relationship between a baby’s birth weight and a mother’s age, so it would be of no consequence to randomly “shuffle” the values of mage. The shuffled_mage column shows one such possible random shuffling.\n\n\n\n\nTable 2: One example of shuffling mage variable.\n\n\n\n\n\n\nID\nweight\nmage\nshuffled_mage\n\n\n\n\n726\n8.75\n18\n26\n\n\n602\n8.53\n32\n22\n\n\n326\n6.81\n28\n24\n\n\n79\n7.01\n22\n18\n\n\n974\n6.03\n24\n32\n\n\n884\n6.32\n26\n28\n\n\n\n\n\n\n\n\n\n\nAgain, such random shuffling of the mage label only makes sense in our hypothesized universe where there is no relationship between the birth weight of a baby and a mother’s age. How could we extend this shuffling of the mage variable to all 1,000 birth records by hand?\nOne way would be writing the 1,000 weights and mages on a set of 1,000 cards. We would then rip each card in half, as we are assuming there is no relationship between these variables. We would then be left with 1,000 weight cards in one hat and 1,000 mage cards in a different hat. You could then draw one card out of the weight hat and one card out of the mage hat and staple them together to make a new (weight, mage) ordered pair. This process of drawing one card out of each hat and stapling them together would be repeated until we had paired each mage card with a new weight card.\nI’ve done one such reshuffling and plotted the original dataset and the shuffled dataset side-by-side in Figure 2. It appears that the slope of the regression line for the shuffled data is much less steep (closer to horizontal) than in the original data.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original values of mother’s age\n\n\n\n\n\n\n\n\n\n\n\n(b) Shuffled values of mother’s age\n\n\n\n\n\n\n\nFigure 2: Scatterplots of relationship between baby’s birth weight and mother’s age.\n\n\n\n\nLet’s compare these slope estimates between the two datasets:\n\nshuffled_births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  calculate(stat = \"slope\")\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 0.0142\n\n\nSo, in this hypothetical universe where there is no relationship between a baby’s birth weight and a mother’s age, we obtained a slope of 0.014.\nNotice that this slope statistic is not the same as the slope statistic of 0.014 that we originally observed. This is once again due to sampling variation. How can we better understand the effect of this sampling variation? By repeating this shuffling several times!"
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#shuffling-10-times",
    "href": "weeks/chapters/week8-reading.html#shuffling-10-times",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.4 Shuffling 10 times",
    "text": "1.4 Shuffling 10 times\nAlright, I’ve carried out the process of shuffling the dataset 9 more times. Table 3 displays the results of these shufflings.\n\n\n\n\nTable 3: One example of shuffling mage variable.\n\n\n\n\n\n\nID\nweight\nmage\nshuffle1\nshuffle2\nshuffle3\nshuffle4\nshuffle5\nshuffle6\nshuffle7\nshuffle8\nshuffle9\nshuffle10\n\n\n\n\n744\n10.13\n30\n37\n35\n28\n33\n24\n22\n23\n37\n28\n31\n\n\n426\n5.94\n36\n26\n27\n33\n23\n36\n33\n34\n23\n19\n26\n\n\n144\n5.07\n23\n36\n33\n30\n27\n37\n23\n38\n23\n34\n20\n\n\n132\n5.97\n28\n27\n22\n31\n20\n26\n28\n27\n33\n29\n27\n\n\n244\n6.50\n25\n33\n20\n30\n23\n36\n23\n22\n30\n39\n36\n\n\n581\n7.28\n29\n19\n27\n33\n38\n22\n35\n32\n27\n27\n14\n\n\n\n\n\n\n\n\n\n\n\nFor each of these 10 shuffles, I computed the slope statistic, and in Figure 3 I display their distribution in a histogram. I’ve also marked the observed slope statistic with a dark red line.\n\n\n\n\n\n\n\n\nFigure 3: Distribution of shuffled slope statistics.\n\n\n\n\n\nBefore we discuss the distribution of the histogram, we need to remember one key detail: this histogram represents relationship between a baby’s birth weight and mother’s age that one would observe in our hypothesized universe where there is no relationship between these variables.\nObserve first that the histogram is roughly centered at 0, which makes sense. A slope of 0 means there is no relationship between a baby’s birth weight and mother’s age, which is exactly what our hypothetical universe assumes!\nHowever, while the values are centered at 0, there is variation about 0. This is because even in a hypothesized universe of no relationship between a baby’s birth weight and mother’s age, you will still likely observe a slight relationship because of chance sampling variation. Looking at the histogram in Figure 3, such differences could even be as extreme as 0.02.\nTurning our attention to what we observed in the births14 dataset: the observed slope of 0.014 is is marked with a vertical dark line. Ask yourself: in a hypothesized world of no relationship between a baby’s birth weight and mother’s age, how likely would it be that we observe this slope statistic? That’s hard to say! It looks like there is only one statistic larger than the observed statistic out of ten, but that means we would expect to see a statistic bigger than what we saw 10% of the time in this hypothetical universe. To me, something happening 10% of the time doesn’t seem like a rare event."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#what-just-happened",
    "href": "weeks/chapters/week8-reading.html#what-just-happened",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "1.5 What just happened?",
    "text": "1.5 What just happened?\nWhat we just demonstrated in this activity is the statistical procedure known as hypothesis testing using a permutation test. The term ““permutation” is the mathematical term for “shuffling”: taking a series of values and reordering them randomly, as you did with the mage values.\nIn fact, permutations are another form of resampling, like the bootstrap method you performed last week. While the bootstrap method involves resampling with replacement, permutation methods involve resampling without replacement.\nThink back to the exercise involving the slips of paper representing the 1,000 birth records from last week. After sampling a paper, you put the paper back into the hat. However, in this scenario, once we drew a weight and age card they were stapled together and never redrawn.\nIn our example, we saw that the observed slope in the births14 dataset was somewhat inconsistent with the hypothetical universe, but only slightly. Thus, I would not be inclined to say that the observed relationship between a baby’s birth weight and a mother’s age (seen in the births14 dataset) is that different from what I would expect to see if there was no relationship between these variables."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#specify-variables",
    "href": "weeks/chapters/week8-reading.html#specify-variables",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.1 specify() variables",
    "text": "3.1 specify() variables\nRecall that we use the specify() verb to specify the response variable and, if needed, any explanatory variables for our study. In this case, since we are interested in the linear relationship between a baby’s birth weight and a mother’s age, we set weight as the response variable and mage as the explanatory variable.\n\nbirths14 %&gt;% \n  specify(response = weight, explanatory = mage) \n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\n# A tibble: 1,000 × 2\n   weight  mage\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   6.96    34\n 2   8.86    31\n 3   7.51    36\n 4   6.19    16\n 5   6.75    31\n 6   6.69    26\n 7   6.13    36\n 8   6.74    24\n 9   8.94    32\n10   9.12    26\n# ℹ 990 more rows\n\n\nAgain, notice how the births14 data itself doesn’t change, but the Response: weight (numeric) and Explanatory: mage (numeric) meta-data do. This is similar to how the group_by() verb from dplyr doesn’t change the data, but only adds “grouping” meta-data, as we saw in Week 3."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#hypothesize-the-null",
    "href": "weeks/chapters/week8-reading.html#hypothesize-the-null",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.2 hypothesize() the null",
    "text": "3.2 hypothesize() the null\nIn order to conduct hypothesis tests using the infer workflow, we need a new step not present for confidence intervals: hypothesize(). Recall from Section 2 that our hypothesis test was\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_A: \\beta_1 \\neq 0\\)\n\nIn other words, the null hypothesis \\(H_0\\) corresponding to our “hypothesized universe” stated that there was no relationship between a baby’s birth weight and a mother’s age. We set this null hypothesis \\(H_0\\) in our infer workflow using the hypothesize() function. We do, however, need to declare what we are assuming about the relationship between our variables. For our regression we are assuming there is no relationship between our variables or that they are\"independent\".\n\nbirths14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  hypothesize(null = \"independence\")\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   weight  mage\n    &lt;dbl&gt; &lt;dbl&gt;\n 1   6.96    34\n 2   8.86    31\n 3   7.51    36\n 4   6.19    16\n 5   6.75    31\n 6   6.69    26\n 7   6.13    36\n 8   6.74    24\n 9   8.94    32\n10   9.12    26\n# ℹ 990 more rows\n\n\nAgain, the data has not changed yet. In fact, we’ve just added one additional piece of meta-data the null hypothesis we are assuming."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#generate-replicates",
    "href": "weeks/chapters/week8-reading.html#generate-replicates",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.3 generate() replicates",
    "text": "3.3 generate() replicates\nAfter we hypothesize() the null hypothesis, we generate() replicates of “shuffled” datasets assuming the null hypothesis is true. We do this by repeating the shuffling exercise you performed in Section 1.3 several times. Instead of merely doing it 10 times, let’s use the computer to repeat this 1000 times by setting reps = 1000 in the generate() function. However, unlike for confidence intervals where we generated replicates using type = \"bootstrap\" resampling with replacement, we’ll now perform shuffles/permutations by setting type = \"permute\". Recall that shuffles / permutations are a kind of resampling, but unlike the bootstrap method, they involve resampling without replacement.\n\nbirths_generate &lt;- births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")\n\nbirths_generate\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000,000 × 3\n# Groups:   replicate [1,000]\n   weight  mage replicate\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;\n 1   6.56    34         1\n 2   9       31         1\n 3   6.96    36         1\n 4   7.8     16         1\n 5   8.39    31         1\n 6   6.56    26         1\n 7   8.95    36         1\n 8   7.94    24         1\n 9   8.52    32         1\n10   6.13    26         1\n# ℹ 999,990 more rows\n\n\nObserve that the resulting data frame has 1,000,000 rows. This is because we performed shuffles / permutations for each of the 1000 rows 1000 times and \\(1,000,000 = 1000 \\cdot 1000\\). If you explore the births_generate data frame, you would notice that the variable replicate indicates which resample each row belongs to. So it has the value 1 1000 times, the value 2 1000 times, all the way through to the value 1000 1000 times."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#calculate-summary-statistics",
    "href": "weeks/chapters/week8-reading.html#calculate-summary-statistics",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.4 calculate() summary statistics",
    "text": "3.4 calculate() summary statistics\nNow that we have generated 1000 replicates of “shuffles” assuming the null hypothesis is true, let’s calculate() the appropriate summary statistic for each of our 1000 shuffles. From Section 2, point estimates related to hypothesis testing have a specific name: test statistics. Since the unknown population parameter of interest is the relationship between baby birth weights and mother’s ages for all births in the US in 2014, \\(\\beta_1\\), the test statistic here is the sample slope, \\(b_1\\). For each of our 1000 shuffles, we can calculate this test statistic by setting stat = \"slope\".\nLet’s save the result in a data frame called null_distribution:\n\nnull_distribution &lt;- births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")%&gt;% \n  calculate(stat = \"slope\")\n\nnull_distribution\n\nResponse: weight (numeric)\nExplanatory: mage (numeric)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate     stat\n       &lt;int&gt;    &lt;dbl&gt;\n 1         1  0.00258\n 2         2  0.00641\n 3         3 -0.00803\n 4         4  0.00647\n 5         5 -0.00373\n 6         6 -0.0193 \n 7         7 -0.00970\n 8         8  0.00394\n 9         9 -0.0201 \n10        10 -0.00136\n# ℹ 990 more rows\n\n\nObserve that we have 1000 values of stat, each representing one instance of \\(b_1\\) in a hypothesized world of no relationship between these variables. Observe as well that we chose the name of this data frame carefully: null_distribution. Recall once again from Section 2 that sampling distributions when the null hypothesis \\(H_0\\) is assumed to be true have a special name: the null distribution.\nWhat was the observed relationship between a baby’s birth weight and a mother’s age in the births14 dataset? In other words, what was the observed test statistic \\(b_1\\)? We could calculate this statistic two ways:\n\nfitting a lm() and grabbing the coefficients using get_regression_table()\nusing infer to calculate() the observed statistic\n\nGoing with option two, the process would look like this:\n\nobs_slope &lt;- births14 %&gt;% \n  specify(response = weight, explanatory = mage) %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n mage \n0.014"
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#visualize-the-p-value",
    "href": "weeks/chapters/week8-reading.html#visualize-the-p-value",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "3.5 visualize() the p-value",
    "text": "3.5 visualize() the p-value\nThe final step is to measure how surprised we are by a slope of 0.014 in a hypothesized universe where there is no relationship between a baby’s birth weight and a mother’s age. If the observed slope of 0.014 is highly unlikely, then we would be inclined to reject the validity of our hypothesized universe.\nWe start by visualizing the null distribution of our 1000 values of \\(b_1\\) using visualize() in Figure 7. Recall that these are values of the sample slope assuming \\(H_0\\) is true.\n\n\n\n\n\n\n\n\nFigure 7: Null distribution of slope statistic.\n\n\n\n\n\nLet’s now add what happened in real life to Figure 7, the observed slope for the relationship between baby birth weights and mother’s ages of 0.014. Instead of merely adding a vertical line using geom_vline(), let’s use the shade_p_value() function with obs_stat set to the observed test statistic (observed slope statistic) value we saved in obs_slope.\nFurthermore, we’ll set the direction = \"two-sided\" reflecting our alternative hypothesis \\(H_A: \\beta_1 \\neq 0\\), stating that there is a relationship between a baby’s birth weight and a mother’s age. As stated in Section 2, a two-sided hypothesis test does not make any assumptions about the direction of the relationship, meaning \\(\\beta_1 &lt; 0\\) and \\(\\beta_1 &gt; 0\\) are equally plausible. So, when calculating statistics that are “more extreme” than what we observed, we need to look in both tails.\n\nvisualize(null_distribution, bins = 10) + \n  labs(x = \"Permuted (Shuffled) Slope Statistic\") +\n  shade_p_value(obs_stat = obs_slope, direction = \"two-sided\")\n\n\n\n\n\n\n\nFigure 8: Shaded histogram to show \\(p\\)-value.\n\n\n\n\n\nIn the resulting Figure 8, the solid dark line marks 0.014. However, what does the shaded-region correspond to? This is the \\(p\\)-value. Recall the definition of the \\(p\\)-value from Section 2:\n\nA \\(p\\)-value is the probability of obtaining a test statistic just as or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nSo judging by the shaded region in Figure 8, it seems we would somewhat rarely observe a slope statistic of 0.014 or more extreme in a hypothesized universe where there is no relationship between a baby’s birth weight and a mother’s age. In other words, the \\(p\\)-value is somewhat small. Hence, we might be inclined to reject this hypothesized universe, or using statistical language we would “reject \\(H_0\\).”\nWhat fraction of the null distribution is shaded? In other words, what is the exact value of the \\(p\\)-value? We can compute it using the get_p_value() function with the same arguments as the previous shade_p_value() code:\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.054\n\n\nKeeping the definition of a \\(p\\)-value in mind, the probability of observing a slope statistic as large as 0.014 or something more extreme due to sampling variation alone in the null distribution is 0.054 = 5.4%. Since this \\(p\\)-value is larger than our pre-specified significance level \\(\\alpha\\) = 0.05, we fail to reject the null hypothesis \\(H_0: \\beta_1 = 0\\). In other words, this \\(p\\)-value is not small enough to reject our hypothesized universe where there is no relationship between a baby’s birth weight and a mother’s age. Notice how our interpretation of this p-value does not state that we accept the null hypothesis. Rather, we have insufficient evidence to reject it. I will discuss these differences more in Section 4.1.\nObserve that whether we reject the null hypothesis \\(H_0\\) or not depends in large part on our choice of significance level \\(\\alpha\\), as if we had chosen an \\(\\alpha\\) of 0.1, we would have had sufficient evidence to reject the null hypothesis. We’ll discuss this more in Section 4.2."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-trial",
    "href": "weeks/chapters/week8-reading.html#sec-trial",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "4.1 Two possible outcomes",
    "text": "4.1 Two possible outcomes\nIn Section 2, we mentioned that given a pre-specified significance level \\(\\alpha\\) there are two possible outcomes of a hypothesis test:\n\nIf the \\(p\\)-value is less than \\(\\alpha\\), then we reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\).\nIf the \\(p\\)-value is greater than or equal to \\(\\alpha\\), we fail to reject the null hypothesis \\(H_0\\).\n\nUnfortunately, the latter result is often misinterpreted as “accepting the null hypothesis \\(H_0\\).” While at first glance it may seem that the statements “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent, there actually is a subtle difference. Saying that we “accept the null hypothesis \\(H_0\\)” is equivalent to stating that “we think the null hypothesis \\(H_0\\) is true.” However, saying that we “fail to reject the null hypothesis \\(H_0\\)” is saying something else: “While \\(H_0\\) might still be false, we don’t have enough evidence to say so.” In other words, there is an absence of enough proof. However, the absence of proof is not proof of absence. 🧐\nTo further shed light on this distinction, let’s use the United States criminal justice system as an analogy. A criminal trial in the United States is a similar situation to hypothesis tests whereby a choice between two contradictory claims must be made about a defendant who is on trial:\n\nThe defendant is truly either “innocent” or “guilty.”\nThe defendant is presumed “innocent until proven guilty.”\nThe defendant is found guilty only if there is strong evidence that the defendant is guilty. The phrase “beyond a reasonable doubt” is often used as a guideline for determining a cutoff for when enough evidence exists to find the defendant guilty.\nThe defendant is found to be either “not guilty” or “guilty” in the ultimate verdict.\n\nIn other words, not guilty verdicts are not suggesting the defendant is innocent, but instead that “while the defendant may still actually be guilty, there wasn’t enough evidence to prove this fact.” Now let’s make the connection with hypothesis tests:\n\nEither the null hypothesis \\(H_0\\) or the alternative hypothesis \\(H_A\\) is true.\nHypothesis tests are conducted assuming the null hypothesis \\(H_0\\) is true.\nWe reject the null hypothesis \\(H_0\\) in favor of \\(H_A\\) only if the evidence found in the sample suggests that \\(H_A\\) is true. The significance level \\(\\alpha\\) is used as a guideline to set the threshold on just how strong of evidence we require.\nWe ultimately decide to either “fail to reject \\(H_0\\)” or “reject \\(H_0\\).”\n\nSo while gut instinct may suggest “failing to reject \\(H_0\\)” and “accepting \\(H_0\\)” are equivalent statements, they are not. “Accepting \\(H_0\\)” is equivalent to finding a defendant innocent. However, courts do not find defendants “innocent,” but rather they find them “not guilty.” Putting things differently, defense attorneys do not need to prove that their clients are innocent, rather they only need to prove that clients are not “guilty beyond a reasonable doubt”.\nSo going back to the births14 dataset, recall that our hypothesis test was \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\) and that we used a pre-specified significance level of \\(\\alpha\\) = 0.05. We found a \\(p\\)-value of 0.054. Since the \\(p\\)-value was smaller than \\(\\alpha\\) = 0.05, we rejected \\(H_0\\). In other words, we found needed levels of evidence in this particular sample to say that \\(H_0\\) is false at the \\(\\alpha\\) = 0.05 significance level. We also state this conclusion using non-statistical language: we found enough evidence in this data to suggest that there was gender discrimination at play."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-errors",
    "href": "weeks/chapters/week8-reading.html#sec-errors",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "4.2 Types of errors",
    "text": "4.2 Types of errors\nUnfortunately, there is some chance a jury or a judge can make an incorrect decision in a criminal trial by reaching the wrong verdict. For example, finding a truly innocent defendant “guilty”. Or on the other hand, finding a truly guilty defendant “not guilty.” This can often stem from the fact that prosecutors don’t have access to all the relevant evidence, but instead are limited to whatever evidence the police can find.\nThe same holds for hypothesis tests. We can make incorrect decisions about a population parameter because we only have a sample of data from the population and thus sampling variation can lead us to incorrect conclusions.\nThere are two possible erroneous conclusions in a criminal trial: either (1) a truly innocent person is found guilty or (2) a truly guilty person is found not guilty. Similarly, there are two possible errors in a hypothesis test: either (1) rejecting \\(H_0\\) when in fact \\(H_0\\) is true, called a Type I error or (2) failing to reject \\(H_0\\) when in fact \\(H_0\\) is false, called a Type II error. Another term used for “Type I error” is “false positive,” while another term for “Type II error” is “false negative.”\nThis risk of error is the price researchers pay for basing inference on a sample instead of performing a census on the entire population. But as we’ve seen in our numerous examples and activities so far, censuses are often very expensive and other times impossible, and thus researchers have no choice but to use a sample. Thus in any hypothesis test based on a sample, we have no choice but to tolerate some chance that a Type I error will be made and some chance that a Type II error will occur.\nTo help understand the concepts of Type I error and Type II errors, we apply these terms to our criminal justice analogy in Figure 9.\n\n\n\n\n\n\nFigure 9: Type I and Type II errors in criminal trials.\n\n\n\nThus a Type I error corresponds to incorrectly putting a truly innocent person in jail, whereas a Type II error corresponds to letting a truly guilty person go free. Let’s show the corresponding table in Figure 10 for hypothesis tests.\n\n\n\n\n\n\nFigure 10: Type I and Type II errors in hypothesis tests."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#sec-choosing-alpha",
    "href": "weeks/chapters/week8-reading.html#sec-choosing-alpha",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "4.3 How do we choose alpha?",
    "text": "4.3 How do we choose alpha?\nIf we are using a sample to make inferences about a population, we run the risk of making errors. For confidence intervals, a corresponding “error” would be constructing a confidence interval that does not contain the true value of the population parameter. For hypothesis tests, this would be making either a Type I or Type II error. Obviously, we want to minimize the probability of either error; we want a small probability of making an incorrect conclusion:\n\nThe probability of a Type I Error occurring is denoted by \\(\\alpha\\). The value of \\(\\alpha\\) is called the significance level of the hypothesis test, which we defined in Section 2.\nThe probability of a Type II Error is denoted by \\(\\beta\\). The value of \\(1-\\beta\\) is known as the power of the hypothesis test.\n\nIn other words, \\(\\alpha\\) corresponds to the probability of incorrectly rejecting \\(H_0\\) when in fact \\(H_0\\) is true. On the other hand, \\(\\beta\\) corresponds to the probability of incorrectly failing to reject \\(H_0\\) when in fact \\(H_0\\) is false.\nIdeally, we want \\(\\alpha = 0\\) and \\(\\beta = 0\\), meaning that the chance of making either error is 0. However, this can never be the case in any situation where we are sampling for inference. There will always be the possibility of making either error when we use sample data. Furthermore, these two error probabilities are inversely related. As the probability of a Type I error goes down, the probability of a Type II error goes up.\nWhat is typically done in practice is to fix the probability of a Type I error by pre-specifying a significance level \\(\\alpha\\) and then try to minimize \\(\\beta\\). In other words, we will tolerate a certain fraction of incorrect rejections of the null hypothesis \\(H_0\\), and then try to minimize the fraction of incorrect non-rejections of \\(H_0\\).\nSo for example if we used \\(\\alpha\\) = 0.01, we would be using a hypothesis testing procedure that in the long run would incorrectly reject the null hypothesis \\(H_0\\) one percent of the time. This is analogous to setting the confidence level of a confidence interval.\nSo what value should you use for \\(\\alpha\\)? Different fields have different conventions, but some commonly used values include 0.10, 0.05, 0.01, and 0.001. However, it is important to keep in mind that if you use a relatively small value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have a harder time being less than \\(\\alpha\\). Thus we would reject the null hypothesis less often. In other words, we would reject the null hypothesis \\(H_0\\) only if we have very strong evidence to do so. This is known as a “conservative” test.\nOn the other hand, if we used a relatively large value of \\(\\alpha\\), then all things being equal, \\(p\\)-values will have an easier time being less than \\(\\alpha\\). Thus we would reject the null hypothesis more often. In other words, we would reject the null hypothesis \\(H_0\\) even if we only have mild evidence to do so. This is known as a “liberal” test."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#regression-interp",
    "href": "weeks/chapters/week8-reading.html#regression-interp",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "6.1 Interpreting regression tables",
    "text": "6.1 Interpreting regression tables\nPreviously, when we interpreted a regression table, like the one shown in Table 4, we focused entirely on the term and estimate columns. Let’s now shift our attention to the remaining columns: std_error, statistic, p_value, lower_ci and upper_ci.\n\n\n\n\nTable 4: Regression table for estimated coefficients from regression on birth weight and mother’s age.\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n6.793\n0.208\n32.651\n0.000\n6.385\n7.201\n\n\nmage\n0.014\n0.007\n1.987\n0.047\n0.000\n0.028\n\n\n\n\n\n\n\n\n\n\nR, the programming language has been around since roughly 1997, a period when computers were not able to run the simulations we can today. Thus, the calculations R provides in the std_error, statistic, p_value, lower_ci and upper_ci columns are not found using simulation-based methods. Rather, R uses a theory-based approach using mathematical formulas. These formulas were derived in a time when computers didn’t exist, so it would’ve been incredibly labor intensive to run extensive simulations.\n\n6.1.1 Standard error\nThe third column of the regression table in Table 4, std_error, corresponds to the standard error of our estimates. Recall the definition of standard error we saw last week:\n\nThe standard error is the standard deviation of any point estimate computed from a sample.\n\nSo what does this mean in terms of the fitted slope \\(b_1\\) = 0.014? This value is just one possible value of the fitted slope resulting from this particular sample of \\(n\\) = 1000 birth records. However, if we collected a different sample of \\(n\\) = 1000 birth records, we will almost certainly obtain a different fitted slope \\(b_1\\). This is due to sampling variability.\nSay we hypothetically collected 1000 such birth records, computed the 1000 resulting values of the fitted slope \\(b_1\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b_1\\), which we defined last week. Further recall that the standard deviation of the sampling distribution of \\(b_1\\) has a special name: the standard error.\nLast week, we used the infer package to construct the bootstrap distribution for \\(b_1\\) in this case. Recall that the bootstrap distribution is an approximation to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar standard errors. However, unlike the sampling distribution, the bootstrap distribution is constructed from a single sample, which is a practice more aligned with what’s done in real life.\nRather than resampling from our original sample to create a bootstrap distribution, we could have instead used theory-based methods to estimated the standard error of the sampling distribution. In particular, there is a formula for the standard error of the fitted slope \\(b_1\\):\n\\[\\text{SE}_{b_1} = \\dfrac{\\dfrac{s_y}{s_x} \\cdot \\sqrt{1-r^2}}{\\sqrt{n-2}}\\]\nAs with many formulas in statistics, there’s a lot going on here, so let’s first break down what each symbol represents. First \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the explanatory variable mage and the response variable weight, respectively. Second, \\(r\\) is the sample correlation coefficient between mage and weight. This was computed as 0.063. Lastly, \\(n\\) is the number of pairs of points (birth weight, mother’s age) in the births14 data frame, here 1000.\nTo put this formula into words, the standard error of \\(b_1\\) depends on the relationship between the variability of the response variable and the variability of the explanatory variable as measured in the \\(s_y / s_x\\) term. Next, it looks into how the two variables relate to each other in the \\(\\sqrt{1-r^2}\\) term.\nHowever, the most important observation to make in the previous formula is that there is an \\(n - 2\\) in the denominator. In other words, as the sample size \\(n\\) increases, the standard error \\(\\text{SE}_{b_1}\\) decreases. Just as we demonstrated last week, when we increase our sample size the amount of sampling variation of the fitted slope \\(b_1\\) will depend on the sample size \\(n\\). In particular, as the sample size increases, both the sampling and bootstrap distributions narrow and the standard error \\(\\text{SE}_{b_1}\\) decreases. Hence, our estimates of \\(b_1\\) for the true population slope \\(\\beta_1\\) get more and more precise.\n\n\n6.1.2 Test statistic\nThe fourth column of the regression table in Table 4, statistic, corresponds to a test statistic relating to the following hypothesis test:\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_A: \\beta_1 \\neq 0\\)\n\nHere, our null hypothesis \\(H_0\\) assumes that the population slope \\(\\beta_1\\) is 0. If the population slope \\(\\beta_1\\) is truly 0, then this is saying that there is no relationship between a baby’s birth weight and a mother’s age for all births in the US in 2014. In other words, \\(x\\) = mother’s age would have no associated effect on \\(y\\) = baby’s birth weight.\nThe alternative hypothesis \\(H_A\\), on the other hand, assumes that the population slope \\(\\beta_1\\) is not 0, meaning it could be either positive or negative. This suggests either a positive or negative relationship between a baby’s birth weight and a mother’s age. Recall we called such alternative hypotheses two-sided.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBy convention, all hypothesis testing for regression assumes two-sided alternatives.\n\n\nThe statistic column in the regression table is a tricky one, however. It corresponds to a standardized t-test statistic. The null distribution can be mathematically proven to be a \\(t\\)-distribution, under specific conditions (described in Section 7). R uses the following \\(t\\)-statistic as the test statistic for hypothesis testing:\n\\[\nt = \\dfrac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}}\n\\]\nAnd since the null hypothesis \\(H_0: \\beta_1 = 0\\) is assumed during the hypothesis test, the \\(t\\)-statistic becomes\n\\[\nt = \\dfrac{ b_1 - 0}{ \\text{SE}_{b_1}} = \\dfrac{ b_1 }{ \\text{SE}_{b_1}}\n\\]\nWhat are the values of \\(b_1\\) and \\(\\text{SE}_{b_1}\\)? They are in the estimate and std_error column of the regression table in Table 4. Thus the value of 1.987 in the table is computed as 0.014 / 0.007 = 2. Note there is a difference due to some rounding error here.\n\n\n6.1.3 p-value\nThe fifth column of the regression table in Table 4, p_value, corresponds to the p-value of the hypothesis test \\(H_0: \\beta_1 = 0\\) versus \\(H_A: \\beta_1 \\neq 0\\).\nAgain recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section 2, let’s focus on the definition of the \\(p\\)-value:\n\nA p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nRecall that you can intuitively think of the \\(p\\)-value as quantifying how “extreme” the observed fitted slope of \\(b_1\\) = 0.014 is in a “hypothesized universe” where there is no relationship between a baby’s birth weight and a mother’s age.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re a bit rusty on the \\(t\\)-distribution, here is an article describing how degrees of freedom relate to the shape of the \\(t\\)-distribution.\n\n\nMore precisely, however, the \\(p\\)-value corresponds to how extreme the observed test statistic of 0.014 is when compared to the appropriate null distribution. Recall from Section 2, that a null distribution is the sampling distribution of the test statistic assuming the null hypothesis \\(H_0\\) is true. It can be mathematically proven that a \\(t\\)-distribution with degrees of freedom equal to \\(df = n - 2 = 1000 - 2 = 998\\) is a reasonable approximation to the null distribution, if certain conditions for inference are not violated. We will discuss these conditions shortly in Section 7.\n\n\n6.1.4 Confidence interval\nThe two rightmost columns of the regression table in Table 4, lower_ci and upper_ci, correspond to the endpoints of the 95% confidence interval for the population slope \\(\\beta_1\\). What are the calculations that went into computing the two endpoints of the 95% confidence interval for \\(\\beta_1\\)?\nRecall from the SE method for constructing confidence intervals, for any distribution that resemble a Normal Distribution2, we could use the empiracle rule to create a 95% confidence interval:\n\n\\(b_1 \\pm SE_{b_1} \\times\\) 1.96\n\n\nWhat is the value of the standard error \\(SE_{b_1}\\)? It is in fact in the third column of the regression table in Table 4: 0.007. Plugging in the respective values, we have:\n\n0.014 \\(\\pm\\) 0.007 \\(\\times\\) 1.96\n0.014 \\(\\pm\\) 0.01372\n(0.00028, 0.02772)\n\n\nThis closely matches the \\((0,0.028)\\) confidence interval in the last two columns of Table 4, with slight differences due to rounding.\nMuch like p-values, the results of this confidence interval also are only valid if the conditions for inference are not violated."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#conclusion",
    "href": "weeks/chapters/week8-reading.html#conclusion",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "6.2 Conclusion",
    "text": "6.2 Conclusion\nDon’t worry if you’re feeling a little overwhelmed at this point. There is a lot of background theory to understand before you can fully make sense of the equations for theory-based methods. That being said, theory-based methods and simulation-based methods for constructing confidence intervals and conducting hypothesis tests often yield consistent results. As mentioned before, in my opinion, two large benefits of simulation-based methods over theory-based are that (1) they are easier for people new to statistical inference to understand, and (2) they also work in situations where theory-based methods and mathematical formulas don’t exist."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#everything-is-one-test",
    "href": "weeks/chapters/week8-reading.html#everything-is-one-test",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "6.3 Everything is one test",
    "text": "6.3 Everything is one test\nWhile this is a lot to digest, especially the first time you encounter hypothesis testing, the nice thing is that once you understand this general framework, then you can understand any hypothesis test. In a famous blog post, computer scientist Allen Downey called this the “There is only one test” framework, for which he created the flowchart displayed in Figure 13.\n\n\n\n\n\n\nFigure 13: Allen Downey’s hypothesis testing framework.\n\n\n\nNotice its similarity with the “hypothesis testing with infer” diagram you saw in Figure 6. That’s because the infer package was explicitly designed to match the “There is only one test” framework. So if you can understand the framework, you can easily generalize these ideas for all hypothesis testing scenarios."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#conditions-for-regression",
    "href": "weeks/chapters/week8-reading.html#conditions-for-regression",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "7.1 Conditions for regression",
    "text": "7.1 Conditions for regression\nFor inference for regression, there are four conditions that cannot be violated. Note the first four letters of these conditions are highlighted in bold in what follows: LINE. This can serve as a nice reminder of what to check for whenever you perform linear regression.\n\nLinearity of relationship between variables\nIndependence of the residuals\nNormality of the residuals\nEquality of variance of the residuals\n\nConditions L, N, and E can be verified through what is known as a residual analysis. Condition I can only be verified through an understanding of how the data was collected.\n\n7.1.1 Residuals refresher\nRecall our definition of a residual: it is the observed value minus the fitted value denoted by \\(y - \\widehat{y}\\). Recall that residuals can be thought of as the error or the “lack-of-fit” between the observed value \\(y\\) and the fitted value \\(\\widehat{y}\\) on the regression line in Figure 1. In Figure 14, we illustrate one particular residual out of 1000 using an arrow, as well as its corresponding observed and fitted values using a circle and a square, respectively.\n\n\n\n\n\n\n\n\nFigure 14: Example of observed value, fitted value, and residual.\n\n\n\n\n\nFurthermore, we can automate the calculation of all \\(n\\) = 1000 residuals by applying the get_regression_points() function to our saved regression model in births_lm. Observe how the resulting values of residual are roughly equal to mage - mage_hat (there is potentially a slight difference due to rounding error).\n\n# Fit regression model:\nbirths_lm &lt;- lm(weight ~ mage, data = births14)\n# Get regression points:\nregression_points &lt;- get_regression_points(births_lm)\nregression_points\n\n# A tibble: 1,000 × 5\n      ID weight  mage weight_hat residual\n   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   6.96    34       7.28   -0.317\n 2     2   8.86    31       7.23    1.63 \n 3     3   7.51    36       7.31    0.204\n 4     4   6.19    16       7.02   -0.831\n 5     5   6.75    31       7.23   -0.484\n 6     6   6.69    26       7.16   -0.473\n 7     7   6.13    36       7.31   -1.18 \n 8     8   6.74    24       7.14   -0.395\n 9     9   8.94    32       7.25    1.69 \n10    10   9.12    26       7.16    1.96 \n# ℹ 990 more rows\n\n\nA residual analysis is used to verify conditions L, N, and E and can be performed using appropriate data visualizations. While there are more sophisticated statistical approaches that can also be done, we’ll focus on the much simpler approach of looking at plots.\n\n\n7.1.2 Linearity of relationship\nThe first condition is that the relationship between the outcome variable \\(y\\) and the explanatory variable \\(x\\) must be Linear. Recall the scatterplot in Figure 1 where we had the explanatory variable \\(x\\) as mother’s age and the outcome variable \\(y\\) as baby’s birth weight. Would you say that the relationship between \\(x\\) and \\(y\\) is linear? It’s hard to say because of the scatter of the points about the line. In my opinion, this relationship is “linear enough.”\nLet’s present an example where the relationship between \\(x\\) and \\(y\\) is clearly not linear in Figure 15. In this case, the points clearly follow a curved relationship, more closely resembling a logarithmic scale. In this case, any results from an inference for regression would not be valid.\n\n\n\n\n\n\n\n\nFigure 15: Example of a clearly non-linear relationship.\n\n\n\n\n\n\n\n7.1.3 Independence of residuals\nThe second condition is that the residuals must be Independent. In other words, the different observations in our data must be independent of one another.\nFor the evals dataset, while there is data on 463 courses, these 463 courses were actually taught by 94 unique instructors. In other words, the same professor is often included more than once in our data. For a professor in the evals data who taught multiple classes, it seems reasonable to expect that their teaching scores will be related to each other. If a professor gets a high score in one class, chances are fairly good they’ll get a high score in another. This dataset thus provides different information than if we had 463 unique instructors teaching the 463 courses.\nIn this case, we say there exists dependence between observations. So in this case, the independence condition is violated. The most appropriate analysis would take into account that we have repeated measures for the same profs, but that is beyond the scope of this class. What we could do, however, is devise a method to collapse the multiple observations for each professor into one observation. This could be done a variety of ways:\n\nrandomly selecting one observation per professor\ntaking the mean of their scores3\nselecting the maximum / minimum / median value\n\n\n\n7.1.4 Normality of residuals\nThe third condition is that the residuals should follow a Normal distribution. Furthermore, the center of this distribution should be 0. In other words, sometimes the regression model will make positive errors: \\(y - \\widehat{y} &gt; 0\\). Other times, the regression model will make equally negative errors: \\(y - \\widehat{y} &lt; 0\\). However, on average the errors should equal 0 and their shape should be similar to that of a bell.\nThe simplest way to check the normality of the residuals is to look at a histogram, which we visualize in Figure 16.\n\nregression_points &lt;- get_regression_points(births_lm)\n\nggplot(data = regression_points, \n       mapping = aes(x = residual)) +\n  geom_histogram(binwidth = 0.25, color = \"white\") +\n  labs(x = \"Residual\")\n\n\n\n\n\n\n\n\n\nFigure 16: Histogram of residuals from regression model for baby birth weight as predicted by mother’s age.\n\n\n\n\n\nThis histogram shows that we have more negative residuals than negative. Since the residual \\(y-\\widehat{y}\\) is negative when \\(y &lt; \\widehat{y}\\), it seems our regression model’s fitted baby birth weights (\\(\\widehat{y}\\)) tend to overestimate the birth weight \\(y\\). Furthermore, this histogram has a left-skew in that there is a longer tail on the left. This is another way to say the residuals exhibit a negative skew.\nIs this a problem? Again, there is a certain amount of subjectivity in the response. In my opinion, while there is a slight skew to the residuals, I don’t believe it is so bad that I would say this condition is violated. On the other hand, others might disagree with our assessment.\nLet’s present examples where the residuals clearly do and don’t follow a normal distribution in Figure 17. In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid.\n\n\n\n\n\n\n\n\nFigure 17: Example of clearly normal and clearly not normal residuals.\n\n\n\n\n\n\n\n7.1.5 Equality of variance\nThe fourth and final condition is that the residuals should exhibit Equal variance across all values of the explanatory variable \\(x\\). In other words, the value and spread of the residuals should not depend on the value of the explanatory variable \\(x\\).\nRecall the scatterplot in Figure 1: we had the explanatory variable \\(x\\) of mother’s age on the x-axis and the outcome variable \\(y\\) of baby’s birth weight on the y-axis. Instead, let’s create a scatterplot that has the same values on the x-axis, but now with the residual \\(y-\\widehat{y}\\) on the y-axis as seen in Figure 18.\n\nggplot(data = regression_points, \n       mapping = aes(x = mage, y = residual)) +\n  geom_point() +\n  labs(x = \"Mother's Age\", y = \"Residual\") +\n  geom_hline(yintercept = 0, col = \"blue\", size = 1)\n\n\n\n\n\n\n\n\n\nFigure 18: Plot of residuals over mother’s age.\n\n\n\n\n\nYou can think of Figure 18 a modified version of the plot with the regression line in Figure 1, but with the regression line flattened out to \\(y=0\\). Looking at this plot, would you say that the spread of the residuals around the line at \\(y=0\\) is constant across all values of the explanatory variable \\(x\\) of mother’s age? This question is rather qualitative and subjective in nature, thus different people may respond with different answers. For example, some people might say that there is slightly more variation in the residuals for ages between 20 and 40 than for ages below 20 and above 40. However, it can be argued that there isn’t a drastic non-constancy. Moreover, this could be an artifact of having smaller numbers of younger / older mothers in the sample.\nIn Figure 19 I present an example where the residuals clearly do not have equal variance across all values of the explanatory variable \\(x\\).\n\n\n\n\n\n\n\n\nFigure 19: Example of clearly non-equal variance.\n\n\n\n\n\nObserve how the spread of the residuals increases as the value of \\(x\\) increases. This is a situation known as heteroskedasticity. Any inference for regression based on a model yielding such a pattern in the residuals would not be valid.\n\n\n7.1.6 What’s the conclusion?\nLet’s list our four conditions for inference for regression again and indicate whether or not they were violated in our analysis:\n\nLinearity of relationship between variables: No\nIndependence of residuals: No\nNormality of residuals: Slight left-skew, but not too concerning\nEquality of variance: No\n\nSo what does this mean for the results of our confidence intervals and hypothesis tests in Section 4? Since none of the conditions were viola ted, we can have faith in the confidence intervals and \\(p\\)-values calculated before. If, however, any of these conditions were violated, we would need to pause and reconsider our model.\nWhen the Independence condition is violated, there exist dependencies between the observations in the dataset. To remedy this, you will need to use a statistical model which accounts for this dependency structure. One such technique is called hierarchical/multilevel modeling, which you may learn about in more advanced statistics courses.\nWhen conditions L, N, E are violated, it often means there is a shortcoming in our model. For example, it may be the case that using only a single explanatory variable is insufficient. We may need to incorporate more explanatory variables in a multiple regression model as we did in previously, or perhaps use a transformation of one or more of your variables, or use an entirely different modeling technique. To learn more about addressing such shortcomings, you’ll have to take a class on or read up on more advanced regression modeling methods.\nThe conditions for inference in regression problems are a key part of regression analysis that are of vital importance to the processes of constructing confidence intervals and conducting hypothesis tests. However, it is often the case with regression analysis in the real world that some of these conditions may be violated. Furthermore, as you saw, there is a level of subjectivity in the residual analyses to verify the L, N, and E conditions. So what can you do? As a statistics educator, I advocate for transparency in communicating all results. This lets the stakeholders of any analysis know about a model’s shortcomings or whether the model is “good enough.” So while this checking of assumptions has lead to some fuzzy “it depends” results, we decided as authors to show you these scenarios to help prepare you for difficult statistical decisions you may need to make down the road."
  },
  {
    "objectID": "weeks/chapters/week8-reading.html#footnotes",
    "href": "weeks/chapters/week8-reading.html#footnotes",
    "title": "Week 8 – Hypothesis Test for Slope & Inference Conditions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI prefer to use the term meaningful.↩︎\nmeaning they are bell shaped and symmetric↩︎\nTaking the mean of observations assumes they are inherently similar, so we shouldn’t calculate the mean of evaluation scores that are very different!↩︎"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7 – Inference for Regression",
    "section": "",
    "text": "1 Textbook Reading – Part 1\nRequired Reading: Exploring Sampling Variability\n\n\n\n\n\n\nReading Guide – Due Monday by the beginning of class\n\n\n\nDownload the Word Document\n\n\n\n\n2 Textbook Reading – Part 2\nRequired Reading: Confidence Intervals for the Slope\n\n\n\n\n\n\nReading Guide – Due Wednesday by the beginning of class (note the new time!)\n\n\n\nDownload the Word Document\n\n\n\n\n3 Concept Quiz – Due Wednesday by the beginning of class (note the new time!)\n1. Match each item to it’s respective analogy:\n\n\npoint estimate\nconfidence interval\n\n\n\nfishing with a net\nfishing with a spear\n\n\n2. To create a 95% confidence interval using the percentile method, what percentiles of the bootstrap distribution do you need to calculate?\n\n0th\n2.5th\n5th\n90th\n95th\n97.5th\n\n3. To create a 95% confidence interval using the standard error method, what standard error do you use?\n\nthe sample standard deviation\nthe bootstrap distribution standard deviation\na resample standard deviation\n1.96\n\n4. We almost never know if our confidence interval captured the true population parameter.\n\nTrue\nFalse\n\n5. What percentage of 99% confidence intervals do you expect to capture the true population parameter?\n6. The word “confident” in a confidence interval interpretation corresponds to what aspect of the interval?\n\nthe accuracy of the original sample\nthe reliability of the procedure for constructing confidence intervals\nthe precision of the bootstrap samples\n\n7. Which of the following are true?\n\nSmaller sample sizes tend to produce narrower confidence intervals.\nSmaller sample sizes tend to produce wider confidence intervals.\nLower confidence levels tend to produce wider confidence intervals.\nLower confidence levels tend to produce narrower confidence intervals.\n\n8. In a regression table, what does the “std_error” value associated with the slope represent?\n\nthe standard deviation of the sample\nthe standard deviation of the bootstrap distribution\nthe estimated standard deviation of the sampling distribution\nthe standard error of the sample\n\n9. In a regression table, how is the “std_error” value calculated?\n\na mathematical formula\nthe standard deviation of the sample\nthe standard deviation of the bootstrap distribution\n\n10. What percentage confidence interval is output in a regression table?\n\n99%\n95%\n90%\n\n\n\n4 R Tutorial – Due Wednesday by the beginning of class\nRequired Tutorial: Practice the infer Workflow",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week Four: Simple Linear Regression (Basic Regression)",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to start exploring statistical methods—linear regression. We will start with the “basic” case or simple linear regression, with a single quantitative explanatory variable and a quantitative response. We will review the concepts behind linear regression, learn how to visualize regression models, and practice obtaining the estimated regression lines in R.",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#learning-outcomes",
    "href": "weeks/week-4.html#learning-outcomes",
    "title": "Week Four: Simple Linear Regression (Basic Regression)",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe the difference between explanatory and predictive modeling\nproduce data visualizations for a simple linear regression\ndescribe how R decides on the “best” regression line\nfit a simple linear regression in R\nwrite out an estimated regression line\ninterpret the slope of an estimated regression line\ninterpret the intercept of an estimated regression line\ndescribe what an “offset” is in a simple linear regression\ninterpret the offsets of an estimated regression line\ncalculate a residual for an observation",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#textbook-reading",
    "href": "weeks/week-4.html#textbook-reading",
    "title": "Week Four: Simple Linear Regression (Basic Regression)",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\nChapter 5 (https://moderndive.com/5-regression.html)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nDon’t worry about Section 5.3.3, it goes a bit too in the weeds for what we are interested in.\n\n\n\nReading Guide\nDownload the Word Document\n\n\n\n\n\n\nDifferent colored answers\n\n\n\nPlease use a different color for your answers to the reading guide, so it is easier to find your responses! 😊",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#concept-quiz-due-monday-by-the-start-of-class",
    "href": "weeks/week-4.html#concept-quiz-due-monday-by-the-start-of-class",
    "title": "Week Four: Simple Linear Regression (Basic Regression)",
    "section": "1.2 Concept Quiz – Due Monday by the start of class",
    "text": "1.2 Concept Quiz – Due Monday by the start of class\n1. What does the correlation coefficient measure?\n\nstrength of a relationship between two variables\nstrength of a linear relationship between two variables\nstrength of a relationship between two numerical variables\nstrength of a linear relationship between two numerical variables\n\n2. How do you interpret a slope coefficient?\n\nFor any increase of \\(x\\), the slope is the expected increase in \\(y\\)\nFor any increase of \\(x\\), the slope is the expected increase in the mean of \\(y\\)\nFor a 1-unit increase in \\(x\\), the slope is the expected increase in \\(y\\)\nFor a 1-unit increase in \\(x\\), the slope is the expected increase in the mean of \\(y\\)\n\n3. How do you interpret an intercept coefficient?\n\nFor a 1-unit increase in \\(x\\), the intercept is the expected increase in \\(y\\)\nFor a 1-unit increase in \\(x\\), the intercept is the expected increase in the mean of \\(y\\)\nFor an \\(x\\) value of 0, the intercept is the expected value of \\(y\\)\nFor an \\(x\\) value of 0, the intercept is the expected value of the mean of \\(y\\)\n\n4. Match the explanatory and response variables to the correct variables in the lm() function syntax.\nlm(variable1 ~ variable2, data = &lt;NAME OF DATASET&gt;)\n5. What geom_ adds a linear regression line to a scatterplot?\n\ngeom_line()\ngeom_smoother()\ngeom_regression()\ngeom_smooth()",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-4.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "title": "Week Four: Simple Linear Regression (Basic Regression)",
    "section": "1.3 💻 R Tutorial – Due Wednesday by the start of class",
    "text": "1.3 💻 R Tutorial – Due Wednesday by the start of class\n\nRegression modeling: Simple linear regression\nRegression modeling: Interpreting regression models",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week One: Foundations of Statistics",
    "section": "",
    "text": "Welcome!\nIn this coursework, you’ll get a refresher on the foundational components of statistics and data, investigate how statistics is used in your major, and think critically about the philosophy of statistical inference.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#learning-outcomes",
    "href": "weeks/week-1.html#learning-outcomes",
    "title": "Week One: Foundations of Statistics",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe observations, variables, and data matrices\nexplain the different types of variables a study can have\nillustrate the difference between explanatory and response variables\ndelineate the difference between a population and a sample\ncompare and contrast different sampling methods\noutline what is necessary to make an experiment an “experiment”\ncharacterize the differences between observational studies and experiments",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#textbook-reading",
    "href": "weeks/week-1.html#textbook-reading",
    "title": "Week One: Foundations of Statistics",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n📖 Required Reading: Introduction to Modern Statistics – Hello Data\n📖 Required Reading: Introduction to Modern Statistics – Study Design\n\nReading Guide – Due Wednesday by noon\nDownload the Word Document\nNote: There is one combined reading guide for both chapters.\n\n\n\n\n\n\nSubmission (Due Wednesday, April 3 by the start of class)\n\n\n\nSubmit your completed reading guide to the Canvas assignment portal!",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#concept-quiz-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-1.html#concept-quiz-due-wednesday-by-the-start-of-class",
    "title": "Week One: Foundations of Statistics",
    "section": "1.2 Concept Quiz – Due Wednesday by the start of class",
    "text": "1.2 Concept Quiz – Due Wednesday by the start of class\n1. What are the different types of variables that can appear in a dataset? Select all that apply!\n\ndiscrete numerical\nordinal categorical\nnominal categorical\ncontinuous numerical\ndiscrete categorical\n\n2. Just because a variable has numeric values, does not mean it is a numeric variable. Which of the following variables appear numerical but behave like a categorical variable? Select all that apply!\n\nzip code\nGPA\nheight\nyear in school\n\n3. Which of the following statements are true about observational studies and experiments? Select all that apply!\n\nExperiments randomly assign the explanatory variable\nObservational studies randomly assign the explanatory variable\nObservational studies can make causal statements about the relationship between the explanatory and response variables\nExperiments can make causal statements about the relationship between the explanatory and response variables\n\n4. What are different methods for sampling from a population? Select all that apply!\n\nsimple random sampling\nstratified random sampling\ncluster sampling\nmultistage sampling\nconvenience sampling\n\n5. Cluster sampling and stratified sampling both rely on grouping observations, but have important differences. Match each method to how observations are randomly sampled.\n\n\nstratified sampling\ncluster sampling\n\ngroups of observations are created, groups are randomly selected, every observation in the selected group is sampled\ngroups of observations are created, observations within a group are randomly sampled",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#statistical-critique-step-0-due-monday-april-8-by-5pm",
    "href": "weeks/week-1.html#statistical-critique-step-0-due-monday-april-8-by-5pm",
    "title": "Week One: Foundations of Statistics",
    "section": "3.1 Statistical Critique Step 0 – Due Monday, April 8 by 5pm",
    "text": "3.1 Statistical Critique Step 0 – Due Monday, April 8 by 5pm\nUpload a PDF of your journal article to the Canvas assignment portal.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are going to continuing exploring data, through data summaries and visualizations, focusing specifically on numerical variables. We will be using the dplyr package in R to wrangle our data and the ggplot2 package to created data visualizations.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#learning-outcomes",
    "href": "weeks/week-2.html#learning-outcomes",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\noutline the differences between numerical and categorical variables\ndescribe what type of summary statistic is appropriate for a given distribution of a numerical variable\ndiscuss when it is / is not appropriate to summarize a variable with a mean\ncreate visualizations of one and two numerical variables\ndiscuss the benefits and shortcomings of different visualizations",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#textbook-reading-part-1",
    "href": "weeks/week-2.html#textbook-reading-part-1",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.1 Textbook Reading – Part 1",
    "text": "1.1 Textbook Reading – Part 1\n\n\n\n\n\n\n\n\n\n\n\nRequired Reading: Exploring Numerical Data\n\nReading Guide – Due Tuesday by the start of class\nDownload the Word Document\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit your completed reading guide to the Canvas assignment portal!",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#concept-quiz-due-tuesday-by-the-start-of-class",
    "href": "weeks/week-2.html#concept-quiz-due-tuesday-by-the-start-of-class",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.2 Concept Quiz – Due Tuesday by the start of class",
    "text": "1.2 Concept Quiz – Due Tuesday by the start of class\n\n\n\n\n\n\nNote\n\n\n\nThe two concept quizzes from each chapter have been combined into one concept quiz on Canvas.\n\n\n\nSuppose we have data on the departure delays of flights flying out of New York. What shape would you expect the distribution of departure delays to have?\n\n\nright skew\nleft skew\nbimodal\nmutimodal\nuniform\n\nHint: Think about how you would “typically” expect flight delays to behave.\n\nTo better decide what summary statistic we should use to summarize the departure delays it would be best to create a data visualization of the distribution of departure delays. What type of visualizations could we make? Select all that apply!\n\n\nboxplot\nhistogram\nbarplot\ndensity plot\nscatterplot",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#textbook-reading-part-2",
    "href": "weeks/week-2.html#textbook-reading-part-2",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.3 Textbook Reading – Part 2",
    "text": "1.3 Textbook Reading – Part 2\nRequired Reading: Data Visualization\n\n\n\n\n\n\n\n\n\n\n\n\nReading Guide – Due Wednesday by the start of class\nDownload the Word Document\n\n\n\n\n\n\nSubmission\n\n\n\nSubmit your completed reading guide to the Canvas assignment portal!",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#concept-quiz-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-2.html#concept-quiz-due-wednesday-by-the-start-of-class",
    "title": "Week Two: Summarizing & Visualizing Numerical Data",
    "section": "1.4 Concept Quiz – Due Wednesday by the start of class",
    "text": "1.4 Concept Quiz – Due Wednesday by the start of class\n\n\n\n\n\n\nNote\n\n\n\nThe two concept quizzes from each chapter have been combined into one concept quiz on Canvas.\n\n\n\nWhat aesthetics are being used in the following plot?\n\nHint: Think about what goes inside of the aes() function and what does not.\n\n\n\n\n\n\n\n\n\n\nx axis\ny axis\ncolor\nfacets\npoints\nlines\n\n\nWhat geometric objects are being used in the displayed visualization?\n\nHint: Think about what geoms you would use to make this plot!\n\npoints\nlines / smoothers\ncolors\nfacets\n\n\nWhat aspects of the distribution of departure delays can you see in the histogram that you could not see in the boxplot?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshape of distribution\nmedian\noutliers\nmode",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "",
    "text": "Welcome!\nIn this week’s coursework we are transitioning into our last topic of the quarter—comparing multiple means. These comparisons have a specific name, ANalysis of VAriance (ANOVA).\nWe are going to use all of the simulation-based methods we learned previously for this new context. An ANOVA relies on a new statistic, the \\(F\\)-statistic, to summarize how different 3 or more means are from each other.\nThe primary focus of an ANOVA is to detect if the means of 3 or more groups are different. Because of this we focus on hypothesis tests for a difference in the group means and do not use confidence intervals. We will generate permutation distributions of \\(F\\)-statistics that we could have observed if the null hypothesis was true (there is no difference in the group means).",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#learning-outcomes",
    "href": "weeks/week-9.html#learning-outcomes",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "0.1 Learning Outcomes",
    "text": "0.1 Learning Outcomes\nBy the end of this coursework you should be able to:\n\ndescribe what an ANOVA tests for\nuse a visualization to outline how the following are calculated:\n\ntotal sum of squares\ngroup sum of squares\nresidual sum of squares\n\ndescribe why the mean squares of groups is called the “between group variability”\ndescribe why the mean square error is called the “within group variability”\noutline how an F-statistic is calculated\nexplain what a “large” or a “small” F-statistic indicates\ndescribe the conditions for performing an ANOVA procedure\noutline when it is appropriate to use an \\(F\\)-distribution in an ANOVA\nexplain the similarities and differences between parametric (\\(F\\)-based) methods and non-parametric (simulation-based) methods\nuse R to:\n\ngenerate a permutation distribution for F-statistics\nvisualize the permutation distribution\ncalculate the observed F-statistic statistic\ncalculate a p-value for a hypothesis test",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#textbook-reading",
    "href": "weeks/week-9.html#textbook-reading",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "1.1 Textbook Reading",
    "text": "1.1 Textbook Reading\n\n\n\n\n\n\n\n\n\n\n\nRequired Reading: Inference for Comparing Many Means\n\nReading Guide – Due Monday by the start of class\nDownload the Word Document",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#concept-quiz-due-monday-by-the-start-of-class",
    "href": "weeks/week-9.html#concept-quiz-due-monday-by-the-start-of-class",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "1.2 Concept Quiz – Due Monday by the start of class",
    "text": "1.2 Concept Quiz – Due Monday by the start of class\n\nWhich of the following are true about the mean squares between groups? (Select all that apply)\n\n\nit is a standardized measure of the variability in responses between groups\nit compares the mean of each group to the overall mean across all groups\nit compares the observations within each group to the mean of that group\nit is used as the numerator in an F-statistic\nit is used as the denominator in an F-statistic\nit is found by dividing the sum of squares between groups by the number of groups minus 1 (\\(k\\) - 1)\nit is found by dividing the sum of squares between groups by the sample size minus the number of groups (\\(n - k\\))\n\n\nWhich of the following are true about the mean square errors? (Select all that apply)\n\n\nit is a standardized measure of the variability in responses within each group\nit compares the mean of each group to the overall mean across all groups\nit compares the observations within each group to the mean of that group\nit is used as the numerator in an F-statistic\nit is used as the denominator in an F-statistic\nit is found by dividing the sum of square errors by the number of groups minus 1 (\\(k\\) - 1)\nit is found by dividing the sum of square errors by the sample size minus the number of groups (\\(n - k\\))\n\n\nAn F-statistic uses which formula?\n\n\n\\(\\frac{MSG}{MSE}\\)\n\\(\\frac{SSG}{SSE}\\)\n\\(\\frac{MSE}{MSG}\\)\n\\(\\frac{SSE}{SSG}\\)\n\n\nIdeally, in an ANOVA we’d like to see… (select all that apply)\n\n\nlarge differences in the means of the groups\nsmall differences in the means of the groups\nlarge variability in the observations within each group\nsmall variability in the observations within each group\n\n\nIf the null hypothesis that the means of four groups are all the same is rejected using ANOVA at a 5% significance level, then… (select all that apply)\n\n\nwe can then conclude that all the means are different from one another.\nthe variability between groups is higher than the variability within groups.\nwe can then conclude that at least one mean is different from the others.\nthe pairwise analysis will identify at least one pair of means that are significantly different.\nan appropriate \\(\\alpha\\) to be used in pairwise comparisons is \\(\\frac{0.05}{4} = 0.0125\\) since there are four groups.\n\n\nTrue or false, as the total sample size increases, the degrees of freedom for the residuals increases.\nTrue or false, the constant variance condition can be somewhat relaxed when the sample sizes are large.\nTrue or false, the independence assumption can be relaxed when the total sample size is large.\nTrue or false, the normality condition is very important when the sample sizes of each group are small.",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week-9.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "href": "weeks/week-9.html#r-tutorial-due-wednesday-by-the-start-of-class",
    "title": "Week 9: Inference for Many Means (ANOVA)",
    "section": "1.3 R Tutorial – Due Wednesday by the start of class",
    "text": "1.3 R Tutorial – Due Wednesday by the start of class\nRequired Tutorial: Comparing many means with ANOVA",
    "crumbs": [
      "Weekly materials",
      "Week 9"
    ]
  },
  {
    "objectID": "activity/week-5-model-justification.html",
    "href": "activity/week-5-model-justification.html",
    "title": "Model Justification",
    "section": "",
    "text": "This study seeks to investigate how the relationship between average SAT math scores and the percentage of students who are economically disadvantaged differs based on the size of the school. As seen in the plot below, the slope for this relationship was found to be similar across the different levels of schools. Therefore, this study used a parallel slopes (additive) multiple linear regression model.\n\n\n\n\n\n\n\n\n\nWhat do you feel is missing from this model justification?"
  },
  {
    "objectID": "activity/week-6-model-selection.html",
    "href": "activity/week-6-model-selection.html",
    "title": "STAT 313 / 513",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(moderndive)"
  },
  {
    "objectID": "activity/week-6-model-selection.html#full-model-all-explanatory-variables-included",
    "href": "activity/week-6-model-selection.html#full-model-all-explanatory-variables-included",
    "title": "STAT 313 / 513",
    "section": "Full Model – ALL Explanatory Variables Included",
    "text": "Full Model – ALL Explanatory Variables Included\n\nlm(body_mass_g ~ . , data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.877         0.873 79631.  282.  287.      255.       0     9   333\n\n\n\n\n\n\n\n\nStarting adjusted \\(R^2\\)\n\n\n\nNote the adjusted \\(R^2\\) value you are starting with, as this will influence your decisions."
  },
  {
    "objectID": "activity/week-6-model-selection.html#variable-selection",
    "href": "activity/week-6-model-selection.html#variable-selection",
    "title": "STAT 313 / 513",
    "section": "Variable Selection",
    "text": "Variable Selection\nNow, starting with our full model, we will use backwards selection to decide what variable(s) should be removed from the model.\nYou can only delete a variable if it increases adjusted \\(R^2\\).\n\n\n\n\n\n\nNote\n\n\n\nNote we are not saying how much adjusted \\(R^2\\) needs to be increased, simply that it must be bigger."
  },
  {
    "objectID": "activity/week-6-model-selection.html#models-deleting-one-explanatory-variable",
    "href": "activity/week-6-model-selection.html#models-deleting-one-explanatory-variable",
    "title": "STAT 313 / 513",
    "section": "Models Deleting One Explanatory Variable",
    "text": "Models Deleting One Explanatory Variable\n\nlm(body_mass_g ~ . -year, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.875         0.872 80659.  284.  288.      284.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -sex, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.853         0.849 94951.  308.  312.      235.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -flipper_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.863          0.86 88256.  297.  301.      256.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -bill_depth_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.873          0.87 81908.  286.  290.      279.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -bill_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.871 81384.  285.  289.      281.       0     8   333\n\n\n\n\nlm(body_mass_g ~ . -island, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.876         0.874 79869.  283.  286.      329.       0     7   333\n\n\n\n\nlm(body_mass_g ~ . -species, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.847         0.843 99062.  315.  319.      257.       0     7   333\n\n\n\n\n\n\n\n\n\nWhich variable should be deleted?\n\n\n\nBased on the adjusted \\(R^2\\) values, which variable should be deleted from the model.\nRemember: You are looking for the model that has a higher adjusted \\(R^2\\) than what you started with!"
  },
  {
    "objectID": "activity/week-6-model-selection.html#models-deleting-two-explanatory-variables",
    "href": "activity/week-6-model-selection.html#models-deleting-two-explanatory-variables",
    "title": "STAT 313 / 513",
    "section": "Models Deleting Two Explanatory Variables",
    "text": "Models Deleting Two Explanatory Variables\n\nlm(body_mass_g ~ . - island - year, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.875         0.873 80828.  284.  287.      380.       0     6   333\n\n\n\nlm(body_mass_g ~ . - island - sex, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.852         0.849 95580.  309.  312.      313.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - flipper_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.863         0.861 88278.  297.  300.      344.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.873         0.871 82154.  287.  290.      373.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_length_mm, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.874         0.871 81636.  286.  289.      376.       0     6   333\n\n\n\n\nlm(body_mass_g ~ . - island - species, data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.831         0.829 108979.  330.  333.      323.       0     5   333\n\n\n\n\n\n\n\n\n\n\nWhich variable should be deleted?\n\n\n\nBased on the adjusted \\(R^2\\) values, which variable should be deleted from the model.\nRemember: You are looking for the model that has a higher adjusted \\(R^2\\) than what you started with!"
  },
  {
    "objectID": "activity/week-6-model-selection.html#new-rules",
    "href": "activity/week-6-model-selection.html#new-rules",
    "title": "STAT 313 / 513",
    "section": "New Rules",
    "text": "New Rules\nNow, choose the simplest model that is within 1% of the best adjusted \\(R^2\\) you obtained.\n\n\n\n\n\n\nNote\n\n\n\nNote, when I say “simplest model” I mean the model with the fewest variables included.\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - bill_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.869         0.867 84623.  291.  294.      434.       0     5   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.871         0.869 83702.  289.  292.      440.       0     5   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - species, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.818         0.815 117911.  343.  346.      368.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - flipper_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.856         0.853 93353.  306.  308.      387.       0     5   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - sex, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.832         0.829 108823.  330.  333.      323.       0     5   333\n\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.867         0.865 86047.  293.  296.      534.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - flipper_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.856         0.854 93385.  306.  308.      486.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - sex, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.824         0.822 113574.  337.  340.      385.       0     4   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - species, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.807         0.805 125076.  354.  356.      457.       0     3   333\n\n\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm - sex, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.787         0.785 137667.  371.  373.      405.       0     3   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm - species, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared     mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.806         0.805 125512.  354.  356.      685.       0     2   333\n\n\n\n\nlm(body_mass_g ~ . - island - bill_depth_mm - year - bill_length_mm - flipper_length_mm, \n   data = penguins) %&gt;% \n  get_regression_summaries()\n\n# A tibble: 1 × 9\n  r_squared adj_r_squared    mse  rmse sigma statistic p_value    df  nobs\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.847         0.845 99037.  315.  317.      606.       0     3   333\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is your best model?\n\n\n\nWhat variables are included in the final model you chose?"
  },
  {
    "objectID": "activity/week-1-quarto.html",
    "href": "activity/week-1-quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Quarto is possibly my favorite thing that happened in 2023! It is an easy-to-use tool for creating reproducible data analyses. Our weekly lab assignments will be done in Quarto, so this week each of you is going to complete your own lab assignment, so you can get to know Quarto a bit on your own.\n\nAccessing Lab 1\nWe will be using Posit Cloud to work with R, so there is no need for you to download any software. However, you will need to create an account for you to be able to access the weekly labs.\nHere are the steps you need to complete for you to access this week’s (and every week’s) lab assignment:\n\nClick on the Posit Cloud link below the “Lab” assignment module on Canvas\nCreate a log-in for Posit Cloud\n\n\n\n\nWhat you will see when you first click on the link to join the Posit Cloud workspace for STAT 313 / 513\n\n\n\nClick “Yes” to join the STAT 313 / 513 workspace\n\n\n\n\nPrompt you should see after you make an account, asking if you want to join the workspace – you should click “Yes”\n\n\n\nIf you successfully joined the workspace, you should see a page that looks like this:\n\n\n\n\nThe “Welcome to Stat 313 / 513 Winter 2024” welcome message you should see if you successfully joined the workspace\n\n\n\n\n\n\n\n\nYour welcome page should reflect the course you are registered for\n\n\n\nIn the image above, the message says “Welcome to Stat 513 Winter 2024”. If you are registered for Stat 313, your welcome message should say “Welcome to Stat 313 Winter 2024”.\n\n\n\nOnce you are in the workspace, you need to access the Content tab, which is where the lab assignments will be listed\n\n\n\n\nWhat you should see if you click on the “Content” tab in the Stat 313 / 513 workspace\n\n\n\nClick on the Lab 1 project to open this week’s lab assignment\n\n 7. Once you are in the Lab 1 project, the final step is to open the Lab 1 Quarto document. To do this, you need to click on the lab-1.qmd document, located in the lower right pane.\n\n\n\nWhere you need to click to open the Lab 1 assignment (lab-1.qmd) and what you should see pop up once you open the document\n\n\nNow that you have Lab 1 open, you are ready to get started! Use the resources below to learn more about working in a Quarto document.\n\n\nLearning About Quarto\n\n🎥 Video Guide of Quarto\nLink to video: https://www.youtube.com/watch?v=_f3latmOhew\n\n\n\n\n\n\nWatch the first 10-minutes!\n\n\n\n\n\n\n\n\n📖 Textbook Guide of Quarto\nLink to textbook chapter: https://r4ds.hadley.nz/quarto.html\n\n\n\n\n\n\nRead sectios 28.1 through 28.5\n\n\n\n\n\n\n\n\n💻 Tutorial of Quarto\nLink to tutorial: https://quarto.org/docs/get-started/hello/rstudio.html\n\n\n\n\n\n\nI’d recommend also watching the video!"
  },
  {
    "objectID": "resources/week-2.html",
    "href": "resources/week-2.html",
    "title": "Week 2 – Exploring Numerical Variables",
    "section": "",
    "text": "Tip\n\n\n\nWherever you see &lt; and &gt; characters, these need to be replaced with information from your dataset. For example,\n\n\n\n\n\n\n\n\nLoading in Packages\nlibrary(tidyverse)\nLoads a package into the R workspace, so you can use the functions and data it contains\n\n\n\n\n\n\n\nReading in Data\nIPEDS &lt;- read_csv(here::here(“data”,\n                             “&lt;NAME OF DATASET.csv&gt;”)\n                  )\nNote: The name of the dataset will change, but it will always need to have the .csv at the end of its name!\n\n\nAssignment Arrow\npenguins_2007 &lt;- filter(penguins, year == 2007)\nAssigns a value (e.g., dataframe) to the name of a variable\n\n\nFiltering a Dataset\nlarge_adelie_2008 &lt;- filter(penguins,\n                            species == “Adelie”,\n                            body_mass_g &gt; 3000,\n                            year == 2008)\nFilters observations (rows) out of / into a dataframe, where the inputs (arguments) are the conditions to be satisfied in the data that are kept\nNote: It makes your code more readable if you put each filter on a new line (hit enter after each comma)!\n\n\nMutating a Dataset\npenguins_large &lt;- mutate(penguins,\n    body_mass_kg = body_mass_g / 1000)\nCreates new variables or modifies existing variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Summary Statistics for Numerical Variables\nsummarize(&lt;NAME OF DATASET&gt;,\n          &lt;NAME OF STAT&gt; = &lt;STAT FUNCTION&gt;(&lt;NAME OF VARIABLE&gt;)\n         )\nFor example, to calculate the mean and median of the dep_delay variable from the nycflights dataset we have:\nsummarize(nycflights,\n          mean_dep_delay = mean(dep_delay),\n          median_dep_delay = median(dep_delay)\n          )\n\n\nHistogram\nggplot(data = &lt;NAME OF DATASET&gt;,\n       mapping = aes(x = &lt;NAME OF VARIABLE&gt;)) +\n  geom_histogram(binwidth = &lt;WIDTH OF BINS&gt;) +\n  labs(x = “&lt;TITLE FOR THE X-AXIS&gt;”)\nNote: A histogram must have the variable on the x-axis!\n\n\nBoxplot\nggplot(data = &lt;NAME OF DATASET&gt;,\n       mapping = aes(x = &lt;NAME OF VARIABLE&gt;)) +\n  geom_boxplot() +\n  labs(x = “&lt;TITLE FOR THE X-AXIS&gt;”)\nNote: This boxplot is horizontal. If you want for your boxplot to be vertical, you use y = instead of x = . Keep in mind you will need to change the location of you axis label, too!\n\n\nScatterplot\nggplot(data = &lt;NAME OF DATASET&gt;,\n       mapping = aes(x = &lt;NAME OF X-VARIABLE&gt;,\n                     y = &lt;NAME OF Y-VARIABLE&gt;)\n       ) +\n  geom_point() +\n  labs(x = “&lt;TITLE FOR THE X-AXIS&gt;”,\n       y = \"&lt;TITLE FOR THE Y-AXIS&gt;\")"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "This course uses Posit Cloud for lab assignments and projects. There is a link to join the STAT 313 Posit Cloud Workspace at the bottom of the Week 1 Module on Canvas.\nIf you have worked with R & RStudio before, I do not want you using the local installation of RStudio to work on your Labs / Projects. I cannot help you on your code if things go sideways! If you use Posit Cloud, however, I can “peek in” to your project and help you debug your code. :)",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 313: Applied Experimental Design and Regression Models",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the quarter. Note that this schedule will be updated as the quarter progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nLecture Slides\nActivity\nLab\nCritique\nProject\n\n\n\n\n0\nSun, March 31\nWelcome to Stat 313 Course Set-up\n📖\n\n\n\n\n\n\n\n1\nMon, April 1 (no class)\nCourse Structure & Statistics Review\n📖\n🖥\n\n\n\n\n\n\n\nWed, April 3\nIntroduction to Data\n\n🎥\n📋\n💻  Due April 8  Lab 1 Feedback\n\n\n\n\n2\nMon, April 8\nVisualizing & Summarizing Numerical Variables\n📖\n🖥\n📋\n\n\n\n\n\n\nWed, April 10\nThe Flaws of Averages\n\n🖥\n\n💻  Due April 15  Lab 2 Feedback\n\n\n\n\n3\nMon, April 15\nIncorporating Categorical Variables\n📖\n🖥\n\n\n\n\n\n\n\nWed, April 17\nEthics & Categorical Variables\n\n🖥\n\n💻  Due April 22  Lab 3 Feedback\n\n\n\n\n\nMon, April 22\nStatistical Critique Due\n\n\n\n\n✍\n Statistical Critique 1 Feedback\n\n\n\n4\nMon, April 22\nIntroduction to Linear Regression\n📖\n🖥\n\n\n\n\n\n\n\nWed, April 24\nThe Ugly History of Linear Regression\n\n🖥\n\n💻  Due April 29 \n\n\n\n\n\nMon, April 29\nMidterm Project Proposal\n\n\n\n\n\n📁 STAT 313\n\n\n5\nMon, April 29\nIntroduction to Multiple Linear Regression\n📖\n🖥\n📋\n\n\n\n\n\n\nWed, May 1\nWork Day – Coding a Multiple Linear Regression\n\n🖥\n📋\n\n\n\n\n\n\nSun, May 5\nMidterm Project First Draft Due\n\n\n\n\n\nGrading Rubric: ✅\n\n\n6\nMon, May 6\nVariable Selection in Multiple Linear Regression\n📖\n🖥\n📋\n\n\n\n\n\n\nWed, May 8\nMachine Learning\n\n🖥\n\n💻  Due May 13 \n\n\n\n\n\nSun, May 12\nMidterm Project Final Version Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\n7\nMon, May 13\nSampling Variability\n📖\n🖥\n📋\n\n\n\n\n\n\nWed, May 15\nConfidence Intervals\n\n🖥\n\n💻  Due May 20 \n\n\n\n\n8\nMon, May 20\np-values & Hypothesis Tests\n📖\n🖥\n📋\n\n\n\n\n\n\nWed, May 22\nSimulation-based Methods vs. Theory-based Methods\n\n🖥\n\n💻  Due May 27 \n\n\n\n\n\nMon, May 27\nStatistical Critique 2 Due\n\n\n\n\n✍\n\n\n\n9\nMon, May 27\nOne-Way Analysis of Variance (OWA)\n📖\n🖥\n\n\n\n\n\n\n\n\nWed, May 29\nInference for OWA & Model Selection\n\n🖥\nAssessing Independence: 📋\n\n\n\n\n\n\nSun, June 2\nFinal Project First Draft Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\n10\nMon, June 3\nTwo-Way ANOVA (TWA) Models\n📖\n🖥\nOne-way to Two-way Model Selection Process: 📋\n\n\n\n\n\n\nWed, June 5\nLast Day of STAT 313\n\n🖥\n\n\n\nWhat should not be in your project: 📋\n\n\n\nSun, June 9\nFinal Project Final Version Due\n\n\n\n\n\nInstructions: 📁  Grading Rubric: ✅\n\n\nFinals Week\n\nFinal Presentations\n\n\n\n\n\nPresentation Instructions:📁\n\n\n\nSTAT 313-01\nWed, June 12 10:10am - 1:00pm\n\n\n\n\n\n\n\n\n\nSTAT 313-02\nTues, June 11 7:10am - 10:00am\nYes, your final is at 7:10am. 😫",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/week9-day2.html#upcoming-deadlines",
    "href": "slides/week9-day2.html#upcoming-deadlines",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Upcoming Deadlines",
    "text": "Upcoming Deadlines\n\nLab 7 revisions are due today (by midnight)\nLab 8 revisions are due next Thursday\nStatistical Critique 2 will be graded by Friday, revisions will be due next Thursday\nFinal revisions on all assignments will be accepted until Sunday, March 17\n\n\n\n\n\n\n\nDon’t forget reflections!"
  },
  {
    "objectID": "slides/week9-day2.html#common-mistakes",
    "href": "slides/week9-day2.html#common-mistakes",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\n\nQuestions 1 & 2 – Your axis labels should contain units and indicate if your variables were transformed!\n\n\nQuestion 6 – Equal variance is not about equal points above and below the line!\n\n\n\nQuestion 11 – Your hypothesis test decision should state what \\(\\alpha\\) was used!\n\n\n\nQuestion 14 – The \\(t\\)-distribution approximates the permutation distribution!\n\n\n\nQuestion 15 – Both theory-based and simulation-based methods have conditions and the reliability of a p-value depends on those conditions."
  },
  {
    "objectID": "slides/week9-day2.html#visualizations-density-ridges",
    "href": "slides/week9-day2.html#visualizations-density-ridges",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Visualizations – Density Ridges",
    "text": "Visualizations – Density Ridges\n\nggplot(data = &lt;NAME OF DATASET&gt;, \n       mapping = aes(x = &lt;NAME OF NUMERICAL VARIABLE&gt;, \n                     y = &lt;NAME OF CATEGORICAL VARIABLE&gt;)\n       ) + \n  geom_density_ridges()+\n  labs(x = \"&lt;TITLE FOR THE X-AXIS&gt;\", \n       y = \"&lt;TITLE FOR THE Y-AXIS&gt;\")\n\n\n\n\n\n\n\nRevisit the code you wrote for Question 8 of Lab 3"
  },
  {
    "objectID": "slides/week9-day2.html#choosing-a-statistical-model",
    "href": "slides/week9-day2.html#choosing-a-statistical-model",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Choosing a Statistical Model",
    "text": "Choosing a Statistical Model\n\nEveryone will fit two one-way ANOVA models — one model for each explanatory variable!\n\n\n\nYou will need to choose whether to use a theory-based method or a simulation-based method.\nYour choice is based on the model conditions — specifically the normality condition.\n\nThe density ridge plots can be used to check the normality and equal variance conditions.\nThe study design should be used to check the independence condition."
  },
  {
    "objectID": "slides/week9-day2.html#theory-based-method",
    "href": "slides/week9-day2.html#theory-based-method",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Theory-based Method",
    "text": "Theory-based Method\n\nTo fit a theory-based ANOVA, you use the following code:\n\nmy_model &lt;- aov(&lt;NAME OF RESPONSE VARIABLE&gt; ~ &lt;NAME OF EXPLANATORY VARIABLE&gt;, \n                data = &lt;NAME OF DATASET&gt;)\n\n\n\n\n\n\n\n\n\nOnly use if normality is not violated!\n\n\nIf you look at your density ridge plots and you believe the normality condition is violated, you should not use a theory-based method!"
  },
  {
    "objectID": "slides/week9-day2.html#simulation-based-method",
    "href": "slides/week9-day2.html#simulation-based-method",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Simulation-based Method",
    "text": "Simulation-based Method\n\nTo fit a simulation-based ANOVA, you need to carry out the following steps:\n\nfind the observed statistic\nsimulate statistics that could have happened if the null was true\nvisualize the distribution of simulated statistics (your permutation distribution)\ncalculate the p-value for your observed statistic\n\n\n\n\n\n\n\n\n2 levels versus 3 levels\n\n\nIf you categorical variable has two levels, you need to use a \"diff in means\" statistic, not an F-statistic!"
  },
  {
    "objectID": "slides/week9-day2.html#model-validity",
    "href": "slides/week9-day2.html#model-validity",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Model Validity",
    "text": "Model Validity\n\n\n\n\n\n\nDelete the code templates I provided!\n\n\n\n\n\n\n\nYou can assess the normality and equal variance conditions based on the density ridge plots you make!"
  },
  {
    "objectID": "slides/week9-day2.html#two-way-anova",
    "href": "slides/week9-day2.html#two-way-anova",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA"
  },
  {
    "objectID": "slides/week9-day2.html#code-for-plot",
    "href": "slides/week9-day2.html#code-for-plot",
    "title": "Coding a One-way ANOVA – Final Project Work Day",
    "section": "Code for Plot",
    "text": "Code for Plot\n\nggplot(data = evals_small, \n       mapping = aes(y = age_cat, x = min_eval, fill = gender)) + \n  geom_density_ridges(alpha = 0.5) + \n  scale_fill_brewer(palette = \"PRGn\") +\n  labs(y = \"\",\n       title = \"Gaps between male and female faculty evaluation scores become more pronounced with age\", \n       x = \"\", \n       fill = \"Professor's Sex\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nLegend Positions\n\n\n\nThere are four options for legend.position\n\n\"none\" (removes the legend)\n\"left\" (difficult to read with the y-axis label)\n\"right\" (the default position)\n\"bottom\" (difficult to read with the x-axis label)\n\"top\" (generally my preference)"
  },
  {
    "objectID": "slides/week2-day1.html#section",
    "href": "slides/week2-day1.html#section",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "5-minutes\n\n\n\nIf you would like to participate\n\nComplete consent form (say yes)\n\n\n\n\nIf you would not like to participate\n\nComplete consent form (say no)"
  },
  {
    "objectID": "slides/week2-day1.html#warm-up",
    "href": "slides/week2-day1.html#warm-up",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Warm-up",
    "text": "Warm-up\n\n\nWhere are variables being included in this plot?\nWhat objects are being used to represent the data?"
  },
  {
    "objectID": "slides/week2-day1.html#todays-layout",
    "href": "slides/week2-day1.html#todays-layout",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Today’s Layout",
    "text": "Today’s Layout\n\nReview visualizations for numerical variables\nDiscuss the pros / cons of each visualization"
  },
  {
    "objectID": "slides/week2-day1.html#section-1",
    "href": "slides/week2-day1.html#section-1",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Univariate (One Variable) Visualizations – For Numerical Data\n\n\n\n\nHistogram (or Dotplot)\nBoxplot\nDensity Plot"
  },
  {
    "objectID": "slides/week2-day1.html#histogram",
    "href": "slides/week2-day1.html#histogram",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Histogram",
    "text": "Histogram\n\n\n\nIs count a variable in the dataset?\nHow did ggplot decide how tall each bar should be?"
  },
  {
    "objectID": "slides/week2-day1.html#section-2",
    "href": "slides/week2-day1.html#section-2",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are the strengths of a histogram?\n\n\nInspect shape of a distribution (skewed or symmetric)\nIdentify modes (most common values)\n\n\n\nWhat are the weaknesses of a histogram?\n\n\nDo not plot raw data, plot summaries (counts) of the data!\nSensitive to the width of the bins (binwidth)"
  },
  {
    "objectID": "slides/week2-day1.html#boxplot",
    "href": "slides/week2-day1.html#boxplot",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Boxplot",
    "text": "Boxplot\n\n\n\n\n\n\n\n\n\n\n\nWhat calculations are necessary for creating a boxplot?"
  },
  {
    "objectID": "slides/week2-day1.html#section-3",
    "href": "slides/week2-day1.html#section-3",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are strengths of a boxplot?\n\n\nEasy to flag unusual observations\nEasy to see the median\n\n\n\nWhat are weaknesses of a boxplot?\n\n\nDon’t plot raw data\nOnly plot summary statistics\nHide multiple modes"
  },
  {
    "objectID": "slides/week2-day1.html#density-plot",
    "href": "slides/week2-day1.html#density-plot",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Density Plot",
    "text": "Density Plot\n\n\n\nA smooth approximation to a variable’s distribution\nPlots density (as a proportion) on the y-axis"
  },
  {
    "objectID": "slides/week2-day1.html#section-5",
    "href": "slides/week2-day1.html#section-5",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "What are strengths of a density plot?\n\n\nInspect shape of a distribution (skewed or symmetric)\nIdentify modes (most common values)\nLess jagged than a histogram\n\n\n\nWhat are weaknesses of a density plot?\n\n\nDo not plot raw data, plot summaries (proportions) of the data!\ny-axis is difficult to interpret\nCan over smooth and hide interesting shapes"
  },
  {
    "objectID": "slides/week2-day1.html#section-6",
    "href": "slides/week2-day1.html#section-6",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Bivariate (Two Variables) Visualizations – For Numerical Data\n\n\n\n\nScatterplots (Week 2)\nFaceted Histograms (Week 3)\nSide-by-Side Boxplots (Week 3)\nStacked Density Plots (Week 3)"
  },
  {
    "objectID": "slides/week2-day1.html#scatterplots",
    "href": "slides/week2-day1.html#scatterplots",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nWhat are the geometric objects being plotted in a scatterplot?"
  },
  {
    "objectID": "slides/week2-day1.html#section-7",
    "href": "slides/week2-day1.html#section-7",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "“The scatterplot the most generally useful invention in the history of statistical graphics.”\n\n\n\n\n\nWhat are strengths of a scatterplot?\n\n\nPlots the raw data!\nInspect form, strength, and direction of a relationship\nIdentify unusual values\n\n\n\nWhat are weaknesses of a scatterplot?\n\n\nCan get “busy” when there are lots of observations (points)"
  },
  {
    "objectID": "slides/week2-day1.html#section-8",
    "href": "slides/week2-day1.html#section-8",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "",
    "text": "Multivariate (3+ Variables) Plots\n\n\nThere are two main methods for adding a third (or fourth) variable into a data visualization:\n\n\n\n\nColors\n\n\ncreates colors for every level of a categorical variable\ncreates a gradient for different values of a quantitative variable\n\n\n\n\n\nFacets\n\n\ncreates subplots for every level of a categorical variable\nlabels each sub-plot with the value of the variable"
  },
  {
    "objectID": "slides/week2-day1.html#colors-in-scatterplots-numerical-variable",
    "href": "slides/week2-day1.html#colors-in-scatterplots-numerical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Colors in Scatterplots – Numerical Variable",
    "text": "Colors in Scatterplots – Numerical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#colors-in-scatterplots-categorical-variable",
    "href": "slides/week2-day1.html#colors-in-scatterplots-categorical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Colors in Scatterplots – Categorical Variable",
    "text": "Colors in Scatterplots – Categorical Variable"
  },
  {
    "objectID": "slides/week2-day1.html#why-not-use-facets-with-a-numerical-variable",
    "href": "slides/week2-day1.html#why-not-use-facets-with-a-numerical-variable",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Why not use facets with a numerical variable???",
    "text": "Why not use facets with a numerical variable???"
  },
  {
    "objectID": "slides/week2-day1.html#due-by-wednesday",
    "href": "slides/week2-day1.html#due-by-wednesday",
    "title": "Visualizing & Summarizing Numerical Data",
    "section": "Due by Wednesday",
    "text": "Due by Wednesday\n\nVisualizing Numerical Data – Reading Guide, Part 2\nVisualizing & Summarizing Numerical Data – Concept Quiz\nExploring Numerical Data – R Tutorial, Part 1\nSummarizing with Statistics – R Tutorial, Part 2\n\n\n\n\n\n\n\n\nDeadline Policy\n\n\nIf you cannot submit an assignment by the deadline, you are required to submit a deadline extension request before the assignment deadline has passed."
  },
  {
    "objectID": "slides/week8-day1.html#weeks-8-9-10",
    "href": "slides/week8-day1.html#weeks-8-9-10",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Weeks 8, 9, & 10",
    "text": "Weeks 8, 9, & 10\n\n\n\nWeek 8\n\nLearn about hypothesis testing for SLR & MLR\nCompare “visual” model selection with p-value model selection\n\n\n\n\nWeek 9\n\nLearn about one-way ANOVA\nGet started on first portion of Final Project\n\n\n\n\nWeek 10\n\nLearn about two-way ANOVA\nFinish Final Project"
  },
  {
    "objectID": "slides/week8-day1.html#upcoming-deadlines",
    "href": "slides/week8-day1.html#upcoming-deadlines",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Upcoming Deadlines",
    "text": "Upcoming Deadlines\n\nLab 6 revisions are due on Thursday (by midnight)\nStatistical Critique 2 is due next Monday (at 5pm)\nFinal revisions on all assignments will be accepted until Sunday, March 17\n\n\n\n\n\n\n\nRevision Deadlines\n\n\nIf you did not submit revisions by the deadline (or forgot to include your reflections), your assignment is not eligible for additional revisions."
  },
  {
    "objectID": "slides/week8-day1.html#section",
    "href": "slides/week8-day1.html#section",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "you…\n\n\n\nunderstand the importance of sampling variability\nknow about using confidence intervals to estimate a range of plausible values for the population parameter\nwant to know how p-values fit in"
  },
  {
    "objectID": "slides/week8-day1.html#section-1",
    "href": "slides/week8-day1.html#section-1",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "What if I want to know if the population parameter differs from a specific value?\n\n\n\n\nHypothesis test!"
  },
  {
    "objectID": "slides/week8-day1.html#section-2",
    "href": "slides/week8-day1.html#section-2",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Hypothesis test goal:\n\n\nAssess how different what we saw in our data is from what could have happened if the null hypothesis was true*\n\n\n\n*For hypothesis tests, we live in an alternative universe where \\(H_0\\) is true"
  },
  {
    "objectID": "slides/week8-day1.html#how-can-we-approximate-what-could-have-happened-if-the-null-was-true",
    "href": "slides/week8-day1.html#how-can-we-approximate-what-could-have-happened-if-the-null-was-true",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "How can we approximate what could have happened if the null was true?",
    "text": "How can we approximate what could have happened if the null was true?\n\n\nPermutation!"
  },
  {
    "objectID": "slides/week8-day1.html#a-permutation-resample",
    "href": "slides/week8-day1.html#a-permutation-resample",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "A Permutation Resample",
    "text": "A Permutation Resample\n\n\nAssumes the original sample is “representative” of observations in the population\nUses the original sample to generate new samples that might have occurred if the null hypothesis was true.\n\n\n\n\nWe can use statistics from these resamples to approximate the true sampling distribution under the null!"
  },
  {
    "objectID": "slides/week8-day1.html#testing-a-population-parameter",
    "href": "slides/week8-day1.html#testing-a-population-parameter",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Testing a Population Parameter",
    "text": "Testing a Population Parameter\n\n\nLike before, we are interested in knowing how a statistic varies from sample to sample.\nKnowing a statistic’s behavior helps us make better / more informed decisions!\nThis helps us know what statistics are more or less likely to occur if the null hypothesis is true."
  },
  {
    "objectID": "slides/week8-day1.html#how-do-i-get-a-p-value",
    "href": "slides/week8-day1.html#how-do-i-get-a-p-value",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "How do I get a p-value?",
    "text": "How do I get a p-value?\n\n\n\nPermuting!\n\n\n\n\n\nFrom your original sample, separate the \\(x\\) values from the \\(y\\) values.\nCreate new ordered pairs by randomly pairing \\(x\\) values with \\(y\\) values (permuting the labels).\n\n\n\n\n\n\nThis is your permuted resample.\n\n\n\n\n\n\nRepeat this process many, many times.\n\n\n\n\n\n\nCalculate a numerical summary (e.g., slope) for each permutation resample.\n\n\n\n\n\n\nThese are your permuted statistics."
  },
  {
    "objectID": "slides/week8-day1.html#permutation-distribution",
    "href": "slides/week8-day1.html#permutation-distribution",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Permutation Distribution",
    "text": "Permutation Distribution\n\n\ndefinition: a distribution of the permuted statistics from every permuted resample\n\n\n\nDisplays the variability in the statistic that could have happened with repeated sampling, if the null hypothesis was true.\n\n\n\nApproximates the true sampling distribution under the null!"
  },
  {
    "objectID": "slides/week8-day1.html#section-3",
    "href": "slides/week8-day1.html#section-3",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "How do I get my p-value?\n\n\nCompare the observed statistic with the statistics produced assuming the null hypothesis was true.\n\n\n\n\nA p-value summarizes the probability of obtaining a sample statistic as or more extreme than what we observed, if the null hypothesis was true."
  },
  {
    "objectID": "slides/week8-day1.html#section-4",
    "href": "slides/week8-day1.html#section-4",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "Your turn!\n\n\nWhat is one similarity and one difference between\n\n\n\n\na permutation distribution\n\n\n\n\n\na bootstrap distribution"
  },
  {
    "objectID": "slides/week8-day1.html#exploring-the-hbr_maples-dataset",
    "href": "slides/week8-day1.html#exploring-the-hbr_maples-dataset",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Exploring the hbr_maples dataset!",
    "text": "Exploring the hbr_maples dataset!\n\n\n\n\n\n\nstem_length: a number denoting the height of the seedling in millimeters\nstem_dry_mass: a number denoting the dry mass of the stem in grams"
  },
  {
    "objectID": "slides/week8-day1.html#section-5",
    "href": "slides/week8-day1.html#section-5",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "",
    "text": "What condition do we need to be worried about?"
  },
  {
    "objectID": "slides/week8-day1.html#in-this-sample-of-359-sugar-maples",
    "href": "slides/week8-day1.html#in-this-sample-of-359-sugar-maples",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "In this sample of 359 sugar maples…",
    "text": "In this sample of 359 sugar maples…\n\\[\\widehat{\\text{stem dry mass}} = -0.043 + 0.001 \\times \\text{stem length}\\]\n\n\n\nWhat slope could have happened if there was no relationship between stem length and stem dry mass?"
  },
  {
    "objectID": "slides/week8-day1.html#generating-a-permuted-resample-and-calculating-permuted-statistics",
    "href": "slides/week8-day1.html#generating-a-permuted-resample-and-calculating-permuted-statistics",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Generating a permuted resample and calculating permuted statistics",
    "text": "Generating a permuted resample and calculating permuted statistics\n\n\nStep 1: specify() your response and explanatory variables\n\n\nStep 2: hypothesize() what would happen under the null\n\n\nStep 3: generate() permuted resamples\n\n\nStep 4: calculate() the statistic of interest"
  },
  {
    "objectID": "slides/week8-day1.html#step-1-specify-your-variables",
    "href": "slides/week8-day1.html#step-1-specify-your-variables",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Step 1: Specify your variables!",
    "text": "Step 1: Specify your variables!\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length)"
  },
  {
    "objectID": "slides/week8-day1.html#step-2-state-your-hypothesis",
    "href": "slides/week8-day1.html#step-2-state-your-hypothesis",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Step 2: State your hypothesis!",
    "text": "Step 2: State your hypothesis!\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  hypothesize(null = \"independence\")\n\n\n\"independence\" – the assumed relationship between the explanatory and response variables under the null hypothesis\n\n\n\n\n\n\n\nIndependence of variables\n\n\nNote! This is different from assuming your observations are independent!"
  },
  {
    "objectID": "slides/week8-day1.html#step-3-generate-your-resamples",
    "href": "slides/week8-day1.html#step-3-generate-your-resamples",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Step 3: Generate your resamples!",
    "text": "Step 3: Generate your resamples!\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\")\n\n\n\n\n\nreps – the number of resamples you want to generate\n\n\n\n\"permute\" – the method that should be used to generate the new samples"
  },
  {
    "objectID": "slides/week8-day1.html#dont-worry",
    "href": "slides/week8-day1.html#dont-worry",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Don’t worry,",
    "text": "Don’t worry,\ninfer will let you know if you missed something!\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  generate(reps = 1000, type = \"permute\")\n\n\nError: Permuting should be done only when doing independence hypothesis test. See `hypothesize()`.\nIn addition: Warning message:\nYou have given `type = \"permute\"`, but `type` is expected to be `\"bootstrap\"`.\nThis workflow is untested and the results may not mean what you think they mean."
  },
  {
    "objectID": "slides/week8-day1.html#step-4-calculate-your-statistics",
    "href": "slides/week8-day1.html#step-4-calculate-your-statistics",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Step 4: Calculate your statistics!",
    "text": "Step 4: Calculate your statistics!\n\nhbr_maples %&gt;% \n  specify(response = stem_dry_mass,\n          explanatory = stem_length) %&gt;% \n  hypothesize(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"slope\")"
  },
  {
    "objectID": "slides/week8-day1.html#your-turn",
    "href": "slides/week8-day1.html#your-turn",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Your turn!",
    "text": "Your turn!\nWhy is the hypothesize() function used to make a null distribution but not for a bootstrap distribution?\n\nWhat does the null = \"independence\" input in hypothesize() mean? What is it assuming about the variables declared in the specify() step?"
  },
  {
    "objectID": "slides/week8-day1.html#the-final-product",
    "href": "slides/week8-day1.html#the-final-product",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "The final product",
    "text": "The final product\n\nvisualise(null_dist) + \n  labs(x = \"Permuted Slope Statistic\")"
  },
  {
    "objectID": "slides/week8-day1.html#is-our-observed-statistic-unlikely-if-the-null-hypothesis-was-true",
    "href": "slides/week8-day1.html#is-our-observed-statistic-unlikely-if-the-null-hypothesis-was-true",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Is our observed statistic unlikely if the null hypothesis was true?",
    "text": "Is our observed statistic unlikely if the null hypothesis was true?\n\nvisualise(null_dist) +\n  shade_p_value(obs_stat = obs_slope, \n                direction = \"two-sided\") +\n  labs(x = \"Permuted Slope Statistic\")"
  },
  {
    "objectID": "slides/week8-day1.html#the-p-value-is",
    "href": "slides/week8-day1.html#the-p-value-is",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "The p-value is…",
    "text": "The p-value is…\n\nget_p_value(null_dist, \n            obs_stat = obs_slope, \n            direction = \"two-sided\") \n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\n\n\nWhy did we get a warning?"
  },
  {
    "objectID": "slides/week8-day1.html#how-do-we-interpret-a-p-value",
    "href": "slides/week8-day1.html#how-do-we-interpret-a-p-value",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "How do we interpret a p-value?",
    "text": "How do we interpret a p-value?\n\nNeed:\n\nprobability of what we saw in the data\nassuming the null hypothesis is true\n\n\n\n\nThe probability of observing a slope statistic (for the relationship between stem length and stem dry mass) as or more extreme than what was observed is less than 1 in 1000, if there was no relationship between a sugar maple’s stem length and stem dry mass."
  },
  {
    "objectID": "slides/week8-day1.html#classic-interpretation-mistakes",
    "href": "slides/week8-day1.html#classic-interpretation-mistakes",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Classic interpretation mistakes",
    "text": "Classic interpretation mistakes\n\n\n“The probability that the null hypothesis is true is about 0%.”\n\n\n\nYou assume the null is true when you calculate a p-value!"
  },
  {
    "objectID": "slides/week8-day1.html#midterm-projects-study-limitations",
    "href": "slides/week8-day1.html#midterm-projects-study-limitations",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Midterm Projects – Study Limitations",
    "text": "Midterm Projects – Study Limitations\n\nYou must justify why you believe you can infer onto a population larger than your sample!\n\n\nCausal statements do not come from random / representative samples!"
  },
  {
    "objectID": "slides/week8-day1.html#midterm-project-grades",
    "href": "slides/week8-day1.html#midterm-project-grades",
    "title": "🧑🏽‍🔬 P-values & Hypothesis Tests",
    "section": "Midterm Project Grades",
    "text": "Midterm Project Grades\n\n\n\n\n\n\n\n\n\n\nExcellent Project\nSatisfactory Project\nProgressing Project\nNo Credit\n\n\n\n\nAt most one section is marked “Satisfactory” the remainder are marked “Excellent”\nAt most one section is marked “Progressing” the remainder are marked “Satisfactory” or “Excellent”\nAt most two sections are marked “Progressing” the remainder are marked “Satisfactory”\nAt most one section is marked “Satisfactory” or “Excellent” the remainder are marked “Progressing”"
  },
  {
    "objectID": "slides/week3-day2.html#revisions-due-tonight",
    "href": "slides/week3-day2.html#revisions-due-tonight",
    "title": "Week 3 Day 2",
    "section": "",
    "text": "If you submitted revisions and your grade is still “Incomplete” – you forgot to submit reflections! Please add these as a comment to your assignment by the end of the day."
  },
  {
    "objectID": "slides/week3-day2.html#a-grading-reminder",
    "href": "slides/week3-day2.html#a-grading-reminder",
    "title": "Week 3 Day 2",
    "section": "A Grading Reminder",
    "text": "A Grading Reminder\n\n\n\n\n“Complete” = Satisfactory\n\n\nYour group obtained a “Success” on every question\n\n\n\n\n“Incomplete” = Growing\n\n\nYour group received a “Growing” on at least one question"
  },
  {
    "objectID": "slides/week3-day2.html#common-mistakes",
    "href": "slides/week3-day2.html#common-mistakes",
    "title": "Week 3 Day 2",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\n\nUnits in axis labels (Q2 & Q9)\n\nWhat unit were the departure / arrival delays measured in?\n\nJustifying why I should expect to be early / late (Q8 & Q10)\n\nWhy is the mean / median a reasonable estimate of the “typical” delay?\nWhat aspect(s) of the distribution did you use to decide what a “typical” delay is?"
  },
  {
    "objectID": "slides/week3-day2.html#copying-the-lab-last-weeks-recorder",
    "href": "slides/week3-day2.html#copying-the-lab-last-weeks-recorder",
    "title": "Week 3 Day 2",
    "section": "Copying the Lab – Last Week’s Recorder",
    "text": "Copying the Lab – Last Week’s Recorder\nThe person who typed your lab needs to make their project “public”\n\nOpen Posit Cloud\nGo to the STAT 313 workspace\nClick on “Your Content”\nOpen the settings for your Lab 2 project"
  },
  {
    "objectID": "slides/week3-day2.html#copying-the-lab-last-weeks-recorder-1",
    "href": "slides/week3-day2.html#copying-the-lab-last-weeks-recorder-1",
    "title": "Week 3 Day 2",
    "section": "Copying the Lab – Last Week’s Recorder",
    "text": "Copying the Lab – Last Week’s Recorder\n\nChange the access for your project to “Space Members”"
  },
  {
    "objectID": "slides/week3-day2.html#copying-the-lab-everyone-else",
    "href": "slides/week3-day2.html#copying-the-lab-everyone-else",
    "title": "Week 3 Day 2",
    "section": "Copying the Lab – Everyone Else",
    "text": "Copying the Lab – Everyone Else\n\n\nFind your group member’s lab (you can use the search bar to search for their name)\n\n\n\n\n\nOpen their Lab 2 project\nSelect “Save a Permanent Copy”"
  },
  {
    "objectID": "slides/week3-day2.html#completing-revisions",
    "href": "slides/week3-day2.html#completing-revisions",
    "title": "Week 3 Day 2",
    "section": "Completing Revisions",
    "text": "Completing Revisions\nLab 2 revisions are due by Wednesday, April 24.\n\n\nRead comments on Canvas\nCopy your group’s lab assignment\nComplete your revisions\nRender your revised Lab 2\nDownload your revised HTML\nSubmit your revisions to the original Lab 2 assignment\n\n\n\n\n\n\n\n\nReflections\n\n\n\n\nRevisions are required to be accompanied with reflections on what you learned while completing your revisions. These can be written in your Lab 2 Quarto file (next to the problems you revised), in a Word document, in the comment box on Canvas."
  },
  {
    "objectID": "slides/week3-day2.html#todays-data",
    "href": "slides/week3-day2.html#todays-data",
    "title": "Week 3 Day 2",
    "section": "Today’s Data",
    "text": "Today’s Data\n\n\nThe and_vertebrates dataset contains length and weight observations for Coastal Cutthroat Trout and two salamander species (Coastal Giant Salamander, and Cascade Torrent Salamander) in previously clear cut (c. 1963) and old growth coniferous forest sections of Mack Creek in HJ Andrews Experimental Forest, Willamette National Forest, Oregon."
  },
  {
    "objectID": "slides/week3-day2.html#research-question",
    "href": "slides/week3-day2.html#research-question",
    "title": "Week 3 Day 2",
    "section": "Research Question",
    "text": "Research Question\n \n\n\nAre there differences in fish biomass between clear cut and old growth sections of the HJ Andrews Forest?"
  },
  {
    "objectID": "slides/week3-day2.html#data-layout",
    "href": "slides/week3-day2.html#data-layout",
    "title": "Week 3 Day 2",
    "section": "Data Layout",
    "text": "Data Layout\n\n\n\n\n\n\n\n\nyear\nsitecode\nsection\nreach\npass\nunitnum\nunittype\nvert_index\npitnumber\nspecies\nlength_1_mm\nlength_2_mm\nweight_g\nclip\nsampledate\nnotes\n\n\n\n\n2003\nMACKOG-U\nOG\nU\n1\n18\nP\n21\nNA\nCascade torrent salamander\n42\n75\nNA\nNONE\n2003-09-03\nNA\n\n\n2017\nMACKOG-M\nOG\nM\n1\n8\nSC\n20\nNA\nCascade torrent salamander\n30\n51\n0.65\nNONE\n2017-09-07\nNA\n\n\n2019\nMACKCC-L\nCC\nL\n1\n1\nC\n16\n1044070\nCoastal giant salamander\n63\n107\n10.10\nNONE\n2019-09-03\nCut tail\n\n\n2013\nMACKCC-U\nCC\nU\n1\n7\nC\n112\n19093013\nCoastal giant salamander\n69\n126\n12.00\nNONE\n2013-09-03\nNA\n\n\n2005\nMACKOG-U\nOG\nU\n1\n15\nC\n12\nNA\nCutthroat trout\n140\nNA\nNA\nLV\n2005-09-01\nNA\n\n\n2017\nMACKCC-U\nCC\nU\n2\n9\nC\n10\nNA\nCutthroat trout\n38\nNA\n0.49\nNONE\n2017-09-06\nNA\n\n\n1990\nMACKCC-L\nCC\nL\n1\n3\nSC\n1\nNA\nNA\nNA\nNA\nNA\nNONE\n1990-08-15\nnot sampled\n\n\n1995\nMACKOG-M\nOG\nM\n1\n10\nS\n1\nNA\nNA\nNA\nNA\nNA\nNONE\n1995-08-29\nJAM\n\n\n\n\n\n\n\n. . .\nWhat are the observations / rows in this dataset?"
  },
  {
    "objectID": "slides/week3-day2.html#if-you-wanted-to-find-the-mean-mass-for-cutthroat-trout-in-each-section-what-would-you-do",
    "href": "slides/week3-day2.html#if-you-wanted-to-find-the-mean-mass-for-cutthroat-trout-in-each-section-what-would-you-do",
    "title": "Week 3 Day 2",
    "section": "If you wanted to find the mean mass for Cutthroat trout in each section, what would you do?",
    "text": "If you wanted to find the mean mass for Cutthroat trout in each section, what would you do?"
  },
  {
    "objectID": "slides/week3-day2.html#step-1-remove-salamanders",
    "href": "slides/week3-day2.html#step-1-remove-salamanders",
    "title": "Week 3 Day 2",
    "section": "Step 1: Remove Salamanders",
    "text": "Step 1: Remove Salamanders\n \n\ntrout &lt;- filter(and_vertebrates, \n                species == \"Cutthroat trout\")"
  },
  {
    "objectID": "slides/week3-day2.html#step-2-group_by-summarize",
    "href": "slides/week3-day2.html#step-2-group_by-summarize",
    "title": "Week 3 Day 2",
    "section": "Step 2: group_by() + summarize()",
    "text": "Step 2: group_by() + summarize()\n\ntrout %&gt;% \n  group_by(section) %&gt;% \n  summarize(\n    mean_mass = mean(weight_g,\n                     na.rm = TRUE)\n            )\n\n# A tibble: 2 × 2\n  section    mean_mass\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Clear cut       9.38\n2 Old growth      8.21\n\n\n. . .\n\n\n\n\n\n\n\nRemember na.rm = TRUE is important if there are missing values!"
  },
  {
    "objectID": "slides/week3-day2.html#are-they-different",
    "href": "slides/week3-day2.html#are-they-different",
    "title": "Week 3 Day 2",
    "section": "Are they different?",
    "text": "Are they different?\n\n\n\n\n\n\n\n\n\nForest Section\nMean Biomass\n\n\n\n\nClear cut\n9.380571\n\n\nOld growth\n8.213796\n\n\n\n\n\n\n\n\nWould you conclude there is a difference in fish biomass between clear cut and old growth sections of the HJ Andrews Forest?"
  },
  {
    "objectID": "slides/week3-day2.html#lets-make-a-visualization",
    "href": "slides/week3-day2.html#lets-make-a-visualization",
    "title": "Week 3 Day 2",
    "section": "Let’s Make a Visualization!",
    "text": "Let’s Make a Visualization!\n\nggplot(data = trout, \n       mapping = aes(x = weight_g, \n                     y = section)\n       ) +\n  geom_density_ridges() +\n  labs(x = \"Weight of Cutthroat Trout (g)\", \n       y = \"Section of HJ Andrews Experimental Forest\")"
  },
  {
    "objectID": "slides/week3-day2.html#are-they-different-1",
    "href": "slides/week3-day2.html#are-they-different-1",
    "title": "Week 3 Day 2",
    "section": "Are they different?",
    "text": "Are they different?\n\n\n\n\n\n\n\n\n\nWould you conclude there is a difference in fish biomass between clear cut and old growth sections of the HJ Andrews Forest?"
  },
  {
    "objectID": "slides/week3-day2.html#lets-add-in-the-type-of-channel",
    "href": "slides/week3-day2.html#lets-add-in-the-type-of-channel",
    "title": "Week 3 Day 2",
    "section": "Let’s Add in the Type of Channel",
    "text": "Let’s Add in the Type of Channel\n\nThe channels (unittype) of the Mack Creek which were sampled were classified into the following groups:\n\n\n\n\"C\"\n\"I\"\n\"IP\"\n\"P\"\n\"R\"\n\"S\"\n\"SC\"\nNA\n\n\n\ncascade\nriffle\nisolated pool\npool\nrapid\nstep (small falls)\nside channel\nnot sampled by unit"
  },
  {
    "objectID": "slides/week3-day2.html#if-you-wanted-to-find-the-mean-mass-for-cutthroat-trout-for-each-type-of-channel-in-each-section-what-would-you-do",
    "href": "slides/week3-day2.html#if-you-wanted-to-find-the-mean-mass-for-cutthroat-trout-for-each-type-of-channel-in-each-section-what-would-you-do",
    "title": "Week 3 Day 2",
    "section": "If you wanted to find the mean mass for Cutthroat trout for each type of channel in each section, what would you do?",
    "text": "If you wanted to find the mean mass for Cutthroat trout for each type of channel in each section, what would you do?"
  },
  {
    "objectID": "slides/week3-day2.html#group_by-summarize",
    "href": "slides/week3-day2.html#group_by-summarize",
    "title": "Week 3 Day 2",
    "section": "group_by() + summarize()",
    "text": "group_by() + summarize()\n\ntrout %&gt;% \n  group_by(section, unittype) %&gt;% \n  summarize(mean_mass = mean(weight_g, \n                             na.rm = TRUE)\n            )\n\n\n\n# A tibble: 13 × 3\n# Groups:   section [2]\n   section    unittype mean_mass\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n 1 Clear cut  C             8.47\n 2 Clear cut  P            13.4 \n 3 Clear cut  R             8.06\n 4 Clear cut  S             4.34\n 5 Clear cut  SC            4.80\n 6 Clear cut  &lt;NA&gt;         16.3 \n 7 Old growth C             7.58\n 8 Old growth I             9.81\n 9 Old growth IP            1.39\n10 Old growth P            10.4 \n11 Old growth R             6.66\n12 Old growth SC            5.38\n13 Old growth &lt;NA&gt;         13.4"
  },
  {
    "objectID": "slides/week3-day2.html#visualization-2.0-incorporating-color",
    "href": "slides/week3-day2.html#visualization-2.0-incorporating-color",
    "title": "Week 3 Day 2",
    "section": "Visualization 2.0 – Incorporating Color",
    "text": "Visualization 2.0 – Incorporating Color\n\nggplot(data = trout, \n       mapping = aes(x = weight_g, \n                     fill = section, \n                     y = unittype)\n       ) +\n  geom_density_ridges(alpha = 0.5) +\n  labs(x = \"Weight of Cutthroat Trout (g)\", \n       fill = \"Section of HJ Andrews \\nExperimental Forest\", \n       y = \"Type of Channel\")\n\n\n\n\n\n\n\n\n. . .\n\nWhy put unittype on the y-axis instead of section???"
  },
  {
    "objectID": "slides/week3-day2.html#visualization-2.0-incorporating-color-output",
    "href": "slides/week3-day2.html#visualization-2.0-incorporating-color-output",
    "title": "Week 3 Day 2",
    "section": "Visualization 2.0 – Incorporating Color",
    "text": "Visualization 2.0 – Incorporating Color"
  },
  {
    "objectID": "slides/week3-day2.html#visualization-2.0-incorporating-facets",
    "href": "slides/week3-day2.html#visualization-2.0-incorporating-facets",
    "title": "Week 3 Day 2",
    "section": "Visualization 2.0 – Incorporating Facets",
    "text": "Visualization 2.0 – Incorporating Facets\n\nggplot(data = trout, \n       mapping = aes(x = weight_g, \n                     y = unittype)) +\n  geom_density_ridges() +\n  labs(x = \"Weight of Cutthroat Trout (g)\", \n       y = \"Section of HJ Andrews Experimental Forest\") +\n  facet_wrap(~section)"
  },
  {
    "objectID": "slides/week3-day2.html#visualization-2.0-incorporating-facets-output",
    "href": "slides/week3-day2.html#visualization-2.0-incorporating-facets-output",
    "title": "Week 3 Day 2",
    "section": "Visualization 2.0 – Incorporating Facets",
    "text": "Visualization 2.0 – Incorporating Facets"
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure",
    "href": "slides/week3-day2.html#choosing-what-to-measure",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure",
    "text": "Choosing What to Measure\nKeller et al. (2017) designed a study to examine whether a community-based suicide prevention project could increase willingness to seek professional help for suicidal ideation among eastern Montana youth.\n\n\nStudents attending the Let’s Talk theater workshop, were asked to report their gender, race, and age."
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure-1",
    "href": "slides/week3-day2.html#choosing-what-to-measure-1",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure",
    "text": "Choosing What to Measure\nResearchers provided students with the following question:\n\nWhat is your gender?\nMale, Female, Other\n\n\n \n\nWhat information are the researchers missing?"
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure-2",
    "href": "slides/week3-day2.html#choosing-what-to-measure-2",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure",
    "text": "Choosing What to Measure\n\\(\\beta\\) blockers have been shown to improve survival in patients with congestive heart failure. These medicines block the effects of the hormone epinephrine (adrenaline). Research suggests that beta blockers have a differing effect for individuals with high levels of estrogen (Khan and Movahed 2000)."
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure-3",
    "href": "slides/week3-day2.html#choosing-what-to-measure-3",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure",
    "text": "Choosing What to Measure\nThe intake form for a local heart clinic asks the following question:\n\nWhat is sex were you assigned at birth?\nMale, Female, Other\n\n\n \n\nWhat information are the doctors missing?"
  },
  {
    "objectID": "slides/week3-day2.html#data-feminism",
    "href": "slides/week3-day2.html#data-feminism",
    "title": "Week 3 Day 2",
    "section": "Data Feminism",
    "text": "Data Feminism\n\n\n\n\n\n\n\n\nData science by whom?\nData science for whom?\nData sets about whom?\nData science with whose values?"
  },
  {
    "objectID": "slides/week3-day2.html#principles-of-data-feminism",
    "href": "slides/week3-day2.html#principles-of-data-feminism",
    "title": "Week 3 Day 2",
    "section": "Principles of Data Feminism",
    "text": "Principles of Data Feminism"
  },
  {
    "objectID": "slides/week3-day2.html#references",
    "href": "slides/week3-day2.html#references",
    "title": "Week 3 Day 2",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nKeller, Sarah N., and Timothy Wilkinson. 2017. “Preventing Suicide in Montana: A Community-Based Theatre Intervention.” Journal of Social Marketing 7 (4): 423–40. https://doi.org/10.1108/jsocm-12-2016-0086.\n\n\nKhan, Nazim Uddin Azam, and Assad Movahed. 2000. “Role of ß Blockers in Congestive Heart Failure.” Congestive Heart Failure 6 (6): 299–312. https://doi.org/10.1111/j.1527-5299.2000.80176.x."
  },
  {
    "objectID": "slides/week10-day1.html#wrapping-up-revisions",
    "href": "slides/week10-day1.html#wrapping-up-revisions",
    "title": "Week 10: Two-way ANOVA",
    "section": "Wrapping Up Revisions",
    "text": "Wrapping Up Revisions\n\n\nStatistical Critique 2 revisions are due by Thursday\nLab 8 revisions are due by Thursday\nFinal revisions on all assignments will be accepted until this Sunday, March 17\n\n\n\n\n\n\n\n\n\nOne round of revisions\n\n\nYou will only have time for one round of revisions on Lab 8 and Statistical Critique 2, so make sure you feel confident about your revisions."
  },
  {
    "objectID": "slides/week10-day1.html#final-project",
    "href": "slides/week10-day1.html#final-project",
    "title": "Week 10: Two-way ANOVA",
    "section": "Final Project",
    "text": "Final Project\n\nFeedback (from me) will be provided no later than Thursday evening\nPeer feedback on Thursday\n\nPrint your report!"
  },
  {
    "objectID": "slides/week10-day1.html#section",
    "href": "slides/week10-day1.html#section",
    "title": "Week 10: Two-way ANOVA",
    "section": "",
    "text": "Two-way ANOVA\n\n\n\nGoal:\n\nAssess if multiple categorical variables have a relationship with the response."
  },
  {
    "objectID": "slides/week10-day1.html#modeling-options",
    "href": "slides/week10-day1.html#modeling-options",
    "title": "Week 10: Two-way ANOVA",
    "section": "Modeling Options",
    "text": "Modeling Options\n\n\n\nAdditive Model\n\n\nAssess if each explanatory variable has a meaningful relationship with the response, conditional on the variable(s) included in the model.\n\n\n\nInteraction Model\n\n\nAssess if the relationship between one categorical explanatory variable and the response differs based on the values of another categorical variable."
  },
  {
    "objectID": "slides/week10-day1.html#what-are-we-looking-for",
    "href": "slides/week10-day1.html#what-are-we-looking-for",
    "title": "Week 10: Two-way ANOVA",
    "section": "What are we looking for?",
    "text": "What are we looking for?"
  },
  {
    "objectID": "slides/week10-day1.html#another-way-to-think-about-it",
    "href": "slides/week10-day1.html#another-way-to-think-about-it",
    "title": "Week 10: Two-way ANOVA",
    "section": "Another way to think about it…",
    "text": "Another way to think about it…"
  },
  {
    "objectID": "slides/week10-day1.html#research-question",
    "href": "slides/week10-day1.html#research-question",
    "title": "Week 10: Two-way ANOVA",
    "section": "Research Question",
    "text": "Research Question\n\nDoes the relationship between stem dry mass and calcium treatment for sugar maples differ based on the year the treatment was applied?\n\n\n\nOr, because the study was an experiment…\n\nDoes the effect of calcium treatment on the stem dry mass of sugar maples differ based on the year of the treatment?"
  },
  {
    "objectID": "slides/week10-day1.html#conditions",
    "href": "slides/week10-day1.html#conditions",
    "title": "Week 10: Two-way ANOVA",
    "section": "Conditions",
    "text": "Conditions\n\nIndependence of observations\n\n\n\nObservations are independent within groups and between groups\n\n\n\n\nEqual variability of the groups\n\n\n\nThe spread of the distributions are similar across groups\n\n\n\n\n\nNormality of the residuals\n\n\n\nThe distribution of residuals for each group is approximately normal"
  },
  {
    "objectID": "slides/week10-day1.html#theory-based-two-way-anova",
    "href": "slides/week10-day1.html#theory-based-two-way-anova",
    "title": "Week 10: Two-way ANOVA",
    "section": "Theory-based Two-Way ANOVA",
    "text": "Theory-based Two-Way ANOVA\n\naov(stem_dry_mass ~ watershed * year_cat, \n    data = hbr_maples_small)\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nwatershed\n1\n0.016057515\n0.0160575147\n71.86801\n3.285643e-15\n\n\nyear_cat\n1\n0.118720475\n0.1187204750\n531.35277\n1.034508e-60\n\n\nwatershed:year_cat\n1\n0.003445855\n0.0034458550\n15.42248\n1.148968e-04\n\n\nResiduals\n221\n0.049378166\n0.0002234306\nNA\nNA\n\n\n\n\n\n\n\n\n\nThe watershed:year_cat line is testing if the relationship between the calcium treatment (watershed) and stem dry mass differs between 2003 and 2004.\n\n\n\n\n\nDoes it?"
  },
  {
    "objectID": "slides/week10-day1.html#how-are-those-p-values-calculated",
    "href": "slides/week10-day1.html#how-are-those-p-values-calculated",
    "title": "Week 10: Two-way ANOVA",
    "section": "How are those p-values calculated?",
    "text": "How are those p-values calculated?\nThe p-values in the previous table use Type I sums of squares.\n\nType I sums of squares are “sequential,” meaning variables are tested in the order they are listed.\n\n\n\nSo, the p-value for watershed:year_cat is conditional on including watershed and year_cat as explanatory variables.\n\n\n\nIs that what we want????"
  },
  {
    "objectID": "slides/week10-day1.html#testing-main-effects",
    "href": "slides/week10-day1.html#testing-main-effects",
    "title": "Week 10: Two-way ANOVA",
    "section": "Testing “main effects”",
    "text": "Testing “main effects”\nIf there is evidence of an interaction, we do not test if the main effects are “significant.”\n\n\nWhy?\n\n\n\nThe interactions depend on these variables, so they should be included in the model!"
  },
  {
    "objectID": "slides/week10-day1.html#interpreting-main-effects",
    "href": "slides/week10-day1.html#interpreting-main-effects",
    "title": "Week 10: Two-way ANOVA",
    "section": "Interpreting “main effects”",
    "text": "Interpreting “main effects”\nWhen interaction effects are present, the interpretation of the main effects is incomplete or misleading"
  },
  {
    "objectID": "slides/week10-day1.html#what-if-our-analysis-found-no-evidence-of-an-interaction",
    "href": "slides/week10-day1.html#what-if-our-analysis-found-no-evidence-of-an-interaction",
    "title": "Week 10: Two-way ANOVA",
    "section": "What if our analysis found no evidence of an interaction?",
    "text": "What if our analysis found no evidence of an interaction?"
  },
  {
    "objectID": "slides/week10-day1.html#testing-for-a-relationship-for-each-variable",
    "href": "slides/week10-day1.html#testing-for-a-relationship-for-each-variable",
    "title": "Week 10: Two-way ANOVA",
    "section": "Testing for a relationship for each variable",
    "text": "Testing for a relationship for each variable\n\naov(stem_dry_mass ~ elevation + watershed, \n    data = hbr_maples_small) %&gt;% \n  tidy()\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nelevation\n1\n0.0005563435\n5.563435e-04\n7.118603\n8.479264e-03\n\n\nwatershed\n1\n0.0029220631\n2.922063e-03\n37.388783\n8.206184e-09\n\n\nResiduals\n148\n0.0115667133\n7.815347e-05\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nDo you think it matters which variable comes first?"
  },
  {
    "objectID": "slides/week10-day1.html#lets-see",
    "href": "slides/week10-day1.html#lets-see",
    "title": "Week 10: Two-way ANOVA",
    "section": "Let’s see…",
    "text": "Let’s see…\n\naov(stem_dry_mass ~ watershed + elevation, \n    data = hbr_maples) %&gt;% \n  tidy()\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nwatershed\n1\n0.0065062507\n6.506251e-03\n86.658504\n9.073535e-18\n\n\nelevation\n1\n0.0005821935\n5.821935e-04\n7.754392\n5.791052e-03\n\n\nResiduals\n237\n0.0177937692\n7.507919e-05\nNA\nNA\n\n\n\n\n\n\n\n\nDid we get the same p-values as before?"
  },
  {
    "objectID": "slides/week10-day1.html#sequential-versus-partial-sums-of-squares",
    "href": "slides/week10-day1.html#sequential-versus-partial-sums-of-squares",
    "title": "Week 10: Two-way ANOVA",
    "section": "Sequential Versus Partial Sums of Squares",
    "text": "Sequential Versus Partial Sums of Squares\nSimilar to before, the p-values in the ANOVA table use Type I (sequential) sums of squares.\n\n\n\nThe p-value for each variable is conditional on the variable(s) that came before it.\nThe p-value for elevation is conditional on watershed being included in the model\nThe p-value for watershed is conditional on…nothing.\n\n\n\n\n\nIf we want the p-value for each explanatory variable to be conditional on every variable included in the model, then we need to use a different type of sums of squares!"
  },
  {
    "objectID": "slides/week10-day1.html#partial-sums-of-squares",
    "href": "slides/week10-day1.html#partial-sums-of-squares",
    "title": "Week 10: Two-way ANOVA",
    "section": "Partial Sums of Squares",
    "text": "Partial Sums of Squares\n\nType III sums of squares are “partial,” meaning every term in the model is tested in light of the other terms in the model.\n\n\n\n\nThe p-value for elevation is conditional on watershed being included in the model\nThe p-value for watershed is conditional on elevation being included in the model\n\n\n\n\n\n\n\n\n\n\nOnly different for variables that were not first\n\n\nWe could have used Type III sums of squares for the interaction model and would have gotten the same p-value!"
  },
  {
    "objectID": "slides/week10-day1.html#getting-the-conditional-tests-for-every-variable",
    "href": "slides/week10-day1.html#getting-the-conditional-tests-for-every-variable",
    "title": "Week 10: Two-way ANOVA",
    "section": "Getting the Conditional Tests for Every Variable",
    "text": "Getting the Conditional Tests for Every Variable\n\nlibrary(car)\n\nwater_elev_lm &lt;- lm(stem_dry_mass ~ watershed + elevation, \n    data = hbr_maples_small) \n\nAnova(water_elev_lm, type = \"III\")\n\n\n\n\n\n\n\nLoad in the car package!"
  },
  {
    "objectID": "slides/week10-day1.html#additive-model-hypothesis-tests",
    "href": "slides/week10-day1.html#additive-model-hypothesis-tests",
    "title": "Week 10: Two-way ANOVA",
    "section": "Additive Model Hypothesis Tests",
    "text": "Additive Model Hypothesis Tests\n\n\n\n\n\n\n\n\nterm\nsumsq\ndf\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0376934983\n1\n482.301032\n2.023579e-48\n\n\nwatershed\n0.0029220631\n1\n37.388783\n8.206184e-09\n\n\nelevation\n0.0004471432\n1\n5.721348\n1.801533e-02\n\n\nResiduals\n0.0115667133\n148\nNA\nNA\n\n\n\n\n\n\n\n\n\nWhat do you think the is the elevation line testing?\n\nWhat would you decide?"
  },
  {
    "objectID": "slides/week10-day1.html#keeping-non-significant-variables",
    "href": "slides/week10-day1.html#keeping-non-significant-variables",
    "title": "Week 10: Two-way ANOVA",
    "section": "Keeping “Non-significant” Variables",
    "text": "Keeping “Non-significant” Variables\n\nShould you always remove variables with “large” p-values from an ANOVA?\n\n\nNo!\nEven “non-significant” variables explain some amount of the variation in the response. Which makes your estimates of a treatment effect more precise!"
  },
  {
    "objectID": "slides/week10-day1.html#hypothesis-test-steps",
    "href": "slides/week10-day1.html#hypothesis-test-steps",
    "title": "Week 10: Two-way ANOVA",
    "section": "Hypothesis Test Steps",
    "text": "Hypothesis Test Steps\n\n\n\nStep 1: Fit a one-way ANOVA model for each categorical variable\n\n\n\n\n\n\nStep 2: Decide if each explanatory variable has a meaningful relationship with the response variable\n\n\n\n\nIf yes, then go to Step 3!\nIf no, then report which variable (if any) has the strongest relationship with the response."
  },
  {
    "objectID": "slides/week10-day1.html#step-3-fit-an-additive-two-way-anova",
    "href": "slides/week10-day1.html#step-3-fit-an-additive-two-way-anova",
    "title": "Week 10: Two-way ANOVA",
    "section": "Step 3 – Fit an Additive Two-way ANOVA",
    "text": "Step 3 – Fit an Additive Two-way ANOVA\nIf there is evidence that both variables have a relationship with the response variable, then you fit an additive two-way ANOVA.\n\nlibrary(car) \n\nmy_model &lt;- lm(&lt;NAME OF RESPONSE VARIABLE&gt; ~ &lt;NAME OF EXPLANATORY VARIABLE 1&gt; + &lt;NAME OF EXPLANATORY VARIABLE 2&gt;,\n               data = &lt;NAME OF DATASET&gt;) \n\nAnova(my_model, type = “III”) %&gt;% \n  tidy()\n\n\n\n\n\n\n\nDon’t forget to load in the car package!"
  },
  {
    "objectID": "slides/week10-day1.html#what-about-interaction-models",
    "href": "slides/week10-day1.html#what-about-interaction-models",
    "title": "Week 10: Two-way ANOVA",
    "section": "What about interaction models?",
    "text": "What about interaction models?\n \nFor the sake of time, we are not fitting interaction models for the Final Project."
  },
  {
    "objectID": "slides/week10-day1.html#your-options",
    "href": "slides/week10-day1.html#your-options",
    "title": "Week 10: Two-way ANOVA",
    "section": "Your Options",
    "text": "Your Options\n\nComplete your revisions on Lab 8\nComplete your revisions on Statistical Critique 2\nFit your two-way ANOVA model for your Final Project and interpret the results\nFinish any remaining revisions on lab or statistical critiques"
  },
  {
    "objectID": "slides/week5-day1.html#plan-for-today",
    "href": "slides/week5-day1.html#plan-for-today",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Plan for Today",
    "text": "Plan for Today\n\nReview different types of multiple linear regression models\nComplete an activity on sample selection\nStart Midterm Project write-up"
  },
  {
    "objectID": "slides/week5-day1.html#plan-for-thursday",
    "href": "slides/week5-day1.html#plan-for-thursday",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Plan for Thursday",
    "text": "Plan for Thursday\nNo lab – focus on getting all the coding accomplished for the Midterm Project"
  },
  {
    "objectID": "slides/week5-day1.html#draft-due-by-sunday",
    "href": "slides/week5-day1.html#draft-due-by-sunday",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Draft Due by Sunday",
    "text": "Draft Due by Sunday\nTo get everyone feedback on their drafts in a timely manner, the first drafts are due by Sunday.\n\n\n\n\n\n\nDeadline Extension\n\n\nA deadline extension is permitted for the first draft. Deadline extensions are not permitted for the final version (due next week)."
  },
  {
    "objectID": "slides/week5-day1.html#section",
    "href": "slides/week5-day1.html#section",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Before…"
  },
  {
    "objectID": "slides/week5-day1.html#section-1",
    "href": "slides/week5-day1.html#section-1",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Now…"
  },
  {
    "objectID": "slides/week5-day1.html#section-2",
    "href": "slides/week5-day1.html#section-2",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "How?"
  },
  {
    "objectID": "slides/week5-day1.html#section-3",
    "href": "slides/week5-day1.html#section-3",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Offsets!\n\n\nsmoke_lm &lt;- lm(weight ~ weeks * habit, data = ncbirths)\n\nget_regression_table(smoke_lm)\n\n\n\n# A tibble: 4 × 3\n  term              estimate std_error\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept           -5.94      0.484\n2 weeks                0.341     0.013\n3 habit: smoker       -1.86      1.63 \n4 weeks:habitsmoker    0.039     0.042\n\n\n\n\n\n\n\n\nInteraction Model\n\n\nThe * means the variables are interacting!"
  },
  {
    "objectID": "slides/week5-day1.html#estimated-regression-equations",
    "href": "slides/week5-day1.html#estimated-regression-equations",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Estimated Regression Equations",
    "text": "Estimated Regression Equations\n\n\n# A tibble: 4 × 3\n  term              estimate std_error\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept           -5.94      0.484\n2 weeks                0.341     0.013\n3 habit: smoker       -1.86      1.63 \n4 weeks:habitsmoker    0.039     0.042\n\n\n\nWhat is the regression equation for non-smoker mothers?\n\n\n\nWhat is the regression equation for smoker mothers?"
  },
  {
    "objectID": "slides/week5-day1.html#section-4",
    "href": "slides/week5-day1.html#section-4",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "What if we have a second numerical explanatory variable?"
  },
  {
    "objectID": "slides/week5-day1.html#section-6",
    "href": "slides/week5-day1.html#section-6",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Multiple slopes\n\n\nage_lm &lt;- lm(weight ~ weeks + mage, data = ncbirths)\n\nget_regression_table(age_lm)\n\n\n\n# A tibble: 3 × 3\n  term      estimate std_error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept   -6.68      0.492\n2 weeks        0.346     0.012\n3 mage         0.02      0.006\n\n\n\n\nHow do you interpret the value of 0.346?\n\n\n\n\nHow do you interpret the value of 0.02?"
  },
  {
    "objectID": "slides/week5-day1.html#section-7",
    "href": "slides/week5-day1.html#section-7",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "But how do we decide if the interaction model is “best” without a p-value??????"
  },
  {
    "objectID": "slides/week5-day1.html#section-8",
    "href": "slides/week5-day1.html#section-8",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "When investigating if a relationship differs…\n\n\n\nAlways start with the “interaction” / different slopes model.\n\n\n\n\nIf the slopes look different, you’re done!\n\n\n\n\nIf the slopes look similar, then fit the “additive” / parallel slopes model."
  },
  {
    "objectID": "slides/week5-day1.html#section-9",
    "href": "slides/week5-day1.html#section-9",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Different Enough?"
  },
  {
    "objectID": "slides/week5-day1.html#section-10",
    "href": "slides/week5-day1.html#section-10",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "What if they’re not very different?"
  },
  {
    "objectID": "slides/week5-day1.html#section-11",
    "href": "slides/week5-day1.html#section-11",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Parallel Slopes\n\n\nlm(average_sat_math ~ perc_disadvan + size, \n   data = MA_schools)\n\n\n\n\n# A tibble: 4 × 3\n  term          estimate std_error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept       588.       7.61 \n2 perc_disadvan    -2.78     0.106\n3 size: medium    -11.9      7.54 \n4 size: large      -6.36     6.92"
  },
  {
    "objectID": "slides/week5-day1.html#section-12",
    "href": "slides/week5-day1.html#section-12",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Group equations – Baseline\n\n\n\n# A tibble: 4 × 3\n  term          estimate std_error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept       588.       7.61 \n2 perc_disadvan    -2.78     0.106\n3 size: medium    -11.9      7.54 \n4 size: large      -6.36     6.92 \n\n\n\n\\[\\widehat{SAT}_{small} = 588 - 2.78 \\times \\text{percent disadvantaged}\\]"
  },
  {
    "objectID": "slides/week5-day1.html#section-13",
    "href": "slides/week5-day1.html#section-13",
    "title": "Introduction to Multiple Linear Regression",
    "section": "",
    "text": "Group equations – Offsets\n\n\n\n# A tibble: 4 × 3\n  term          estimate std_error\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 intercept       588.       7.61 \n2 perc_disadvan    -2.78     0.106\n3 size: medium    -11.9      7.54 \n4 size: large      -6.36     6.92 \n\n\n\n\n\\[\\widehat{SAT}_{medium} = (588 - 11.9) - 2.78 \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{medium} = 576.1 - 2.78 \\times \\text{percent disadvan}\\]\n\n\n\n\n\\[\\widehat{SAT}_{large} = (588 - 6.36) - 2.78 \\times \\text{percent disadvan}\\]\n\n\n\\[\\widehat{SAT}_{large} = 581.64 - 2.78 \\times \\text{percent disadvan}\\]"
  },
  {
    "objectID": "slides/week5-day1.html#find-your-data-group",
    "href": "slides/week5-day1.html#find-your-data-group",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Find Your Data Group!",
    "text": "Find Your Data Group!\nOnce you have found other students working on the same dataset, complete the sample selection activity."
  },
  {
    "objectID": "slides/week5-day1.html#steps-before-thursday-stat-313",
    "href": "slides/week5-day1.html#steps-before-thursday-stat-313",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Steps Before Thursday (STAT 313)",
    "text": "Steps Before Thursday (STAT 313)\n\nInsert the description of your dataset and variables (from the Midterm Proposal) into the “Introduction” of your project\nPose a research question about your selected variables, which can be addressed with multiple linear regression\nInsert the code to create the required two (or three) visualizations\nWrite a description of what you see in the visualizations\nMake a decision which model you believe is “best”"
  },
  {
    "objectID": "slides/week5-day1.html#steps-before-thursday-stat-513",
    "href": "slides/week5-day1.html#steps-before-thursday-stat-513",
    "title": "Introduction to Multiple Linear Regression",
    "section": "Steps Before Thursday (STAT 513)",
    "text": "Steps Before Thursday (STAT 513)\n\n\nCreate a nicely formatted version of your dataset\nInsert the description of your dataset and variables (from the Midterm Proposal) into the “Introduction” of your project\nPose a research question about your selected variables, which can be addressed with multiple linear regression\nWork with Dr. Theobold to write code to read in your data!\nInsert the code to create the required two (or three) visualizations\nWrite a description of what you see in the visualizations\nMake a decision which model you believe is “best”"
  },
  {
    "objectID": "slides/week5-day2.html#section",
    "href": "slides/week5-day2.html#section",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Lab 3 Revisions are due today (Thursday, February 8)\nLab 4 Revisions are due next Thursday (February 15)\n\n\n\n\n\n\n\nMaking a copy of your group’s Lab 4\n\n\nIf you were the recorder (typer) for your group, you need to make your project public. If you were not the recorder, you need to make a copy of your group’s project.\n\n\n\n\nThe first draft of your Midterm Project is due on Sunday at midnight."
  },
  {
    "objectID": "slides/week5-day2.html#for-every-dataset-in-stat-313",
    "href": "slides/week5-day2.html#for-every-dataset-in-stat-313",
    "title": "Midterm Project Work Day",
    "section": "For Every Dataset (in STAT 313)",
    "text": "For Every Dataset (in STAT 313)\n\nEvery dataset is included in an R package! You need to find which package that is and load it in!\nIf your dataset is associated with a publication, that publication should be referenced in your Introduction!"
  },
  {
    "objectID": "slides/week5-day2.html#introduction-versus-methods",
    "href": "slides/week5-day2.html#introduction-versus-methods",
    "title": "Midterm Project Work Day",
    "section": "Introduction Versus Methods",
    "text": "Introduction Versus Methods\n\nThe description of your data goes and your research question goes in your Introduction.\nThe description of your variables goes at the beginning of your Methods!"
  },
  {
    "objectID": "slides/week5-day2.html#specific-dataset-advice",
    "href": "slides/week5-day2.html#specific-dataset-advice",
    "title": "Midterm Project Work Day",
    "section": "Specific Dataset Advice",
    "text": "Specific Dataset Advice\n\nFor the and_vertebrates data, you should include species as an explanatory variable. If you don’t you are assuming the same relationship applies to trout AND salamanders.\nFor the hbr_maples data, you cannot use year as a numerical variable. There are only two years of data!\nFor the pie_crabs data:\n\nsize should be the response variable\nsite and latitude measure the same thing"
  },
  {
    "objectID": "slides/week5-day2.html#section-1",
    "href": "slides/week5-day2.html#section-1",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Step 0 – Read in Your Data\n\n\n\nSTAT 313\n\n\nLocate what package your data live in (found in the directions for the midterm project proposal)\nLoad in the package you need!\nGet started!\n\n\n\n\n\nSTAT 513\n\n\nUpload your formatted dataset to your Midterm Project on Posit Cloud\nWrite the following code to load in your dataset:\n\n\n\n\n\n\nname_you_want &lt;- read_xlsx(\"name of your dataset.xlsx\")\n\nOR\n\nname_you_want &lt;- read_csv(\"name of your dataset.csv\")"
  },
  {
    "objectID": "slides/week5-day2.html#section-2",
    "href": "slides/week5-day2.html#section-2",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Step 1 – Visualizations\n\n\n\nTwo Numerical Variables\n\n\n\n\n\n\nThree total visualizations\n\n\n\n\n\n\n\n\nVisualize the model with both variables – color gradient\nVisualize two simple linear regression models – one for each variable\n\n\n\n\n\nOne Categorical & One Numerical Variable\n\n\n\n\n\n\nTwo total visualizations\n\n\n\n\n\n\n\n\nVisualize the model using geom_smooth(method = \"lm\")\nVisualize the model using geom_parallel_slopes()\n\n\n\n\n\n\n\n\n\n\nNo theme_classic()\n\n\nI like grid lines! Please use theme_bw() instead!"
  },
  {
    "objectID": "slides/week5-day2.html#section-3",
    "href": "slides/week5-day2.html#section-3",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Step 2 – Decide the “Best” Model\n\n\n\nTwo Numerical Variables\n\n\nIf there appears to be a relationship with the colors – include both variables!\nIf the colors are equally dispersed throughout the plot – choose the one variable that has the stronger relationship (larger slope)!\n\n\n\n\n\nOne Categorical & One Numerical Variable\n\n\nLook at the plot where the lines are allowed to be different! Does it look like they are?"
  },
  {
    "objectID": "slides/week5-day2.html#section-4",
    "href": "slides/week5-day2.html#section-4",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Step 3 – Fit the regression model with lm()\n\n\n\nTwo Numerical Variables\n\n\nAre both variables included? Use a + to separate them!\n\n\n\n\n\nOne Categorical & One Numerical Variable\n\n\nAre the slopes different? You need to fit a different slopes model! Use a * to separate the variables!\nAre the slopes similar? You need to fit a parallel slopes model! Use a + to separate the variables!"
  },
  {
    "objectID": "slides/week5-day2.html#section-5",
    "href": "slides/week5-day2.html#section-5",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Step 4: Get the coefficients with get_regression_table()"
  },
  {
    "objectID": "slides/week5-day2.html#section-6",
    "href": "slides/week5-day2.html#section-6",
    "title": "Midterm Project Work Day",
    "section": "",
    "text": "Now interpret!"
  },
  {
    "objectID": "slides/week3-day1-alt.html#in-r",
    "href": "slides/week3-day1-alt.html#in-r",
    "title": "Incorporating Categorical Variables",
    "section": "In R…",
    "text": "In R…\ncategorical variables can have either character or factor data types\n\n\nfactor – structured & fixed number of levels / options\n\ncan be ordered or unordered\n\n\n\n\ncharacter – unstructured & variable number of levels\n\nis inherently unordered"
  },
  {
    "objectID": "slides/week3-day1-alt.html#incorporating-categorical-variables-into-data-visualizations",
    "href": "slides/week3-day1-alt.html#incorporating-categorical-variables-into-data-visualizations",
    "title": "Incorporating Categorical Variables",
    "section": "Incorporating Categorical Variables into Data Visualizations",
    "text": "Incorporating Categorical Variables into Data Visualizations\n\n\nAs a variable on the x- or y-axis\nAs a color / fill\nAs a facet"
  },
  {
    "objectID": "slides/week3-day1-alt.html#salamander-size",
    "href": "slides/week3-day1-alt.html#salamander-size",
    "title": "Incorporating Categorical Variables",
    "section": "Salamander Size",
    "text": "Salamander Size\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_2_mm)) + \n  geom_histogram(binwidth = 14) + \n  labs(x = \"Snout to Tail Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow would this histogram look if there was no variation in salamander length?\n\nWhat are possible causes for the variation in salamander length?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#faceted-histograms",
    "href": "slides/week3-day1-alt.html#faceted-histograms",
    "title": "Incorporating Categorical Variables",
    "section": "Faceted Histograms",
    "text": "Faceted Histograms\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm)) + \n  geom_histogram(binwidth = 14) + \n  facet_wrap(~ section, scales = \"free\") +\n  labs(x = \"Snout to Tail Length (mm)\")\n\n\nWhat do you think scales = \"free\" does?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#side-by-side-boxplots",
    "href": "slides/week3-day1-alt.html#side-by-side-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                     y = species)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\") \n\n\n\n\n\n\n\n\n\n\nggplot(data = salamander, \n       mapping = aes(y = length_1_mm, \n                     x = species)\n         ) + \n  geom_boxplot() + \n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Salamander Species\")\n\n\n\n\n\n\n\n\n\n\nWhich orientation do you prefer? Vertical or horizontal?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#colors-in-boxplots",
    "href": "slides/week3-day1-alt.html#colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Colors in Boxplots",
    "text": "Colors in Boxplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = unittype)\n         ) + \n  geom_boxplot() + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\", \n       color = \"Channel Type\")\n\n\n\nWhy are there only two boxplots for the Olympic torrent salamander?"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-colors-in-boxplots",
    "href": "slides/week3-day1-alt.html#facets-colors-in-boxplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = species, \n                       color = section)\n         ) + \n  geom_boxplot() + \n  facet_wrap(~ unittype) + \n  labs(x = \"Snout to Tail Length (mm)\", \n       y = \"Salamander Species\", \n       color = \"Section in Mack Creek\")"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-colors-in-boxplots-output",
    "href": "slides/week3-day1-alt.html#facets-colors-in-boxplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Colors in Boxplots",
    "text": "Facets & Colors in Boxplots"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-color-in-scatterplots",
    "href": "slides/week3-day1-alt.html#facets-color-in-scatterplots",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots\n\nggplot(data = salamander, \n       mapping = aes(x = length_1_mm, \n                       y = weight_g, \n                       color = section)\n         ) + \n  geom_point() + \n  facet_wrap(~species, scales = \"free\") +\n  labs(y = \"Snout to Tail Length (mm)\", \n       x = \"Year\", \n       color = \"Salamander Species\")"
  },
  {
    "objectID": "slides/week3-day1-alt.html#facets-color-in-scatterplots-output",
    "href": "slides/week3-day1-alt.html#facets-color-in-scatterplots-output",
    "title": "Incorporating Categorical Variables",
    "section": "Facets & Color in Scatterplots",
    "text": "Facets & Color in Scatterplots"
  },
  {
    "objectID": "slides/week3-day1-alt.html#your-turn",
    "href": "slides/week3-day1-alt.html#your-turn",
    "title": "Incorporating Categorical Variables",
    "section": "Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\n\n\n\nWhat are the aesthetics included in this plot?"
  },
  {
    "objectID": "slides/week1-day1.html#section",
    "href": "slides/week1-day1.html#section",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "About Me…"
  },
  {
    "objectID": "slides/week1-day1.html#section-1",
    "href": "slides/week1-day1.html#section-1",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What is Statistics?\n\n\n\nScientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data.\n\n\nStatistics is the study of how best to collect, analyze, and draw conclusions from data.\n\n\nIntroduction to Modern Statistics"
  },
  {
    "objectID": "slides/week1-day1.html#section-2",
    "href": "slides/week1-day1.html#section-2",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What Statistics Is To Me\n\n \n\n\n\n\nThe data science cycle – Wickham & Grolemund"
  },
  {
    "objectID": "slides/week1-day1.html#section-3",
    "href": "slides/week1-day1.html#section-3",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "What you can expect in STAT 313\n\nThis course will teach you the fundamentals of linear models—simple linear regression, multiple linear regression, and analysis of variance—and experimental design. You will extend the concepts covered in your Stat I course, to:\n\n\nwork with data in a reproducible way (using R)\nvisualize and summarize a variety of datasets (in R)\ncritically evaluate the use of Statistics\nperform statistical analyses to answer research questions (using R)"
  },
  {
    "objectID": "slides/week1-day1.html#section-4",
    "href": "slides/week1-day1.html#section-4",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Coding 🙀\n\n\nCoding is a huge part of how doing statistics in the wild looks.\n\n\n\nEveryone is coming from a different background\nDifferent aspects of the course will be difficult to different people\nYou will be given coding resources each week\nUse your peers to support your learning"
  },
  {
    "objectID": "slides/week1-day1.html#section-5",
    "href": "slides/week1-day1.html#section-5",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Course Components\n\n\n\n\nBefore Class\n\n\nReading Guides\nConcept Quizzes\nR Tutorials\n\n\n\n\n\nDuring Class\n\n\nGroup Discussion\nHands-on Activities\nLab Assignments\n\n\n\n\n\nOutside of Class\n\n\nStatistical Critiques\nMidterm Project\nFinal Project"
  },
  {
    "objectID": "slides/week1-day1.html#section-6",
    "href": "slides/week1-day1.html#section-6",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Specifications Based Grading\n\n\n\n\nEveryone is capable of earning an A!"
  },
  {
    "objectID": "slides/week1-day1.html#section-7",
    "href": "slides/week1-day1.html#section-7",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "How Smart are You?\n\n\n\n(2 minutes)\n\nWrite two criteria would you use to rank yourself compared to everyone else in this class\n\n\n\n\n(3 minutes)\n\nTalk with the person on your right about the criteria you proposed\n\n\n\n\n(5 minutes)\n\nShare out\nPerson with most vowels in name should be prepared to share!"
  },
  {
    "objectID": "slides/week1-day1.html#section-8",
    "href": "slides/week1-day1.html#section-8",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Cooperative Learning\n\nis a structured form of small-group learning\n\n\n\n\nRoles relate to how the work should be done\n\nRoles are not about breaking up the work intellectually\n\nRoles allow each person to contribute to the group in significant ways\n\nEach person’s participation is necessary to complete the task"
  },
  {
    "objectID": "slides/week1-day1.html#section-9",
    "href": "slides/week1-day1.html#section-9",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Group Norms\n\n\n\n\nZero tolerance for: racism, sexism, homophobia, transphobia, ageism, ableism\nRespect one another\nIntent and impact both matter\n\n\n\n\n\nNon-judgmental\nTake space, make space\nEmbrace discomfort\nMake decisions by consensus"
  },
  {
    "objectID": "slides/week1-day1.html#section-10",
    "href": "slides/week1-day1.html#section-10",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "How We Learn Together\n\n\n\n\n\nNo one is done until everyone is done\nYou have the right to ask anyone in your group for help\nYou have the duty to help anyone in your group who asks for help\n\n\n\n\n\n\n\nHelping someone means explaining your thinking not giving answers or doing the work for them\nProvide a justification when you make a statement\nThink and work together – don’t divide up the work"
  },
  {
    "objectID": "slides/week1-day1.html#section-11",
    "href": "slides/week1-day1.html#section-11",
    "title": "Welcome to Stat 313!",
    "section": "",
    "text": "Break"
  },
  {
    "objectID": "slides/week7-day2.html#course-updates",
    "href": "slides/week7-day2.html#course-updates",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Course Updates",
    "text": "Course Updates\n\nStatistical Critique 1 revisions are due tonight\nMidterm Projects will be graded by Sunday\nLab 6 revisions will be due next Thursday\n\n\n\n\n\n\n\nDon’t forget reflections!\n\n\nIf your reflections are not present by the deadline for revisions, your revisions are not eligible to be regraded. Please don’t forget your reflections!"
  },
  {
    "objectID": "slides/week7-day2.html#section",
    "href": "slides/week7-day2.html#section",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Approximate the variability you’d expect to see in other samples!\n\n\n\n\nBootstrapping!"
  },
  {
    "objectID": "slides/week7-day2.html#section-1",
    "href": "slides/week7-day2.html#section-1",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "A Bootstrap Resample\n\n\n\nAssumes the original sample is “representative” of observations in the population.\n\n\n\n\nUses the original sample to generate new samples that might have occurred with additional sampling.\n\n\n\n\n\nWe can use the statistics from these bootstrap resamples to approximate the true sampling distribution!"
  },
  {
    "objectID": "slides/week7-day2.html#section-2",
    "href": "slides/week7-day2.html#section-2",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Estimating a population parameter\n\n\n\nWe are interested in knowing how a statistic varies from sample to sample.\n\n\n\n\nKnowing a statistic’s behavior helps us make better / more informed decisions!\n\n\n\n\nThis helps us estimate what values are more or less likely for the population parameter to have."
  },
  {
    "objectID": "slides/week7-day2.html#section-3",
    "href": "slides/week7-day2.html#section-3",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "How do I get this plausible range of values?\n\n\n\n\nBootstrapping!\n\n\n\n\n\nFrom your original sample, resample with replacement the same number of times as your original sample.\n\n\n\n\n\n\nThis is your bootstrap resample.\n\n\n\n\n\n\nRepeat this process many, many times.\n\n\n\n\n\n\nCalculate a numerical summary (e.g., mean, median) for each bootstrap resample.\n\n\n\n\n\n\nThese are your bootstrap statistics"
  },
  {
    "objectID": "slides/week7-day2.html#section-4",
    "href": "slides/week7-day2.html#section-4",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Bootstrap Distribution\n\n\n\na distribution of the bootstrap statistics from every bootstrap resample\n\n\n\n\nDisplays the variability in the statistic that could have happened with repeated sampling.\n\n\n\nApproximates the true sampling distribution!"
  },
  {
    "objectID": "slides/week7-day2.html#statistic-beta_1",
    "href": "slides/week7-day2.html#statistic-beta_1",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Statistic: \\(\\beta_1\\)",
    "text": "Statistic: \\(\\beta_1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe relationship between penguin’s bill length and body mass for all penguins in the Palmer Archipelago"
  },
  {
    "objectID": "slides/week7-day2.html#in-this-sample-of-344-penguins",
    "href": "slides/week7-day2.html#in-this-sample-of-344-penguins",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "In this sample of 344 penguins…",
    "text": "In this sample of 344 penguins…\n\\[\\widehat{\\text{bill length}} = 26.899 + 0.004 \\times \\text{body mass}\\]\n\n\n\nWhat slope could have happened in a different sample of penguins?"
  },
  {
    "objectID": "slides/week7-day2.html#section-5",
    "href": "slides/week7-day2.html#section-5",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Generating bootstrap resamples and calculating bootstrap statistics\n\n\n\nStep 1: specify() your response and explanatory variables\n\n\nStep 2: generate() bootstrap resamples\n\n\nStep 3: calculate() the statistic of interest"
  },
  {
    "objectID": "slides/week7-day2.html#section-6",
    "href": "slides/week7-day2.html#section-6",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Step 1: Specify your variables!\n\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g)"
  },
  {
    "objectID": "slides/week7-day2.html#section-7",
    "href": "slides/week7-day2.html#section-7",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Step 2: Generate your resamples!\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g) %&gt;% \n  generate(reps = 500, type = \"bootstrap\")\n\n\n\nreps – the number of resamples you want to generate\n\n\"bootstrap\" – the method that should be used to generate the new samples"
  },
  {
    "objectID": "slides/week7-day2.html#section-8",
    "href": "slides/week7-day2.html#section-8",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Your turn!\n\n\n\nWhy do we resample with replacement when creating a bootstrap distribution?\n\nWhen we resample with replacement from our original sample what are we assuming about our sample?"
  },
  {
    "objectID": "slides/week7-day2.html#section-9",
    "href": "slides/week7-day2.html#section-9",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Step 3: Calculate your statistics!\n\n\npenguins %&gt;% \n  specify(response = bill_length_mm, \n          explanatory = body_mass_g) %&gt;% \n  generate(reps = 1000, \n           type = \"bootstrap\") %&gt;% \n  calculate(stat = \"slope\")\n\n\n\n\"slope\" – the statistic of interest"
  },
  {
    "objectID": "slides/week7-day2.html#section-10",
    "href": "slides/week7-day2.html#section-10",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "The final product\n\n\n\nvisualize(boot1) + \n  labs(title = \"Distribution of 1,000 Bootstrap Resamples\", \n       x = \"Slope Statistic\", \n       y = \"\")"
  },
  {
    "objectID": "slides/week7-day2.html#section-11",
    "href": "slides/week7-day2.html#section-11",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "A plausible range of values for \\(\\beta_1\\)\n\n\n\nvisualise(boot1) +\n  shade_confidence_interval(endpoints = boot1_CI, \n                            color = \"red\", \n                            fill = \"pink\") +  \n  labs(title = \"Distribution of 1,000 Bootstrap Resamples\", \n       x = \"Slope Statistic\", \n       y = \"\")"
  },
  {
    "objectID": "slides/week7-day2.html#section-12",
    "href": "slides/week7-day2.html#section-12",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "The 95% confidence interval is…\n\n\nget_confidence_interval(boot1, \n                        level = 0.95, \n                        type = \"percentile\")\n\n\n\n\n\n\n\nLower Bound\nUpper Bound\n\n\n\n\n0.00355\n0.00453\n\n\n\n\n\n\n\n\nWhat do we hope is captured by this interval?"
  },
  {
    "objectID": "slides/week7-day2.html#section-13",
    "href": "slides/week7-day2.html#section-13",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "How do we interpret this interval?\n\n\n\n\n“We are 95% confident the slope of the relationship between bill length and body mass for all penguins in the Palmer Archipelago is between 0.00355 and 0.00453\n\n\n\n\n\n\n\n“For every 1 gram increase in a penguin’s body mass, we are 95% confident the length of the penguin’s bill will increase between 0.00355 and 0.00453mm."
  },
  {
    "objectID": "slides/week7-day2.html#section-14",
    "href": "slides/week7-day2.html#section-14",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "",
    "text": "Classic interpretation mistakes\n\n\n\n\n“95% of the time the population parameter would fall between 0.00355 and 0.00453.”\n\n\n\n\n\n“We are 95% confident the sample statistic is in our interval.”"
  },
  {
    "objectID": "slides/week7-day2.html#how-does-the-relationship-between-bill-length-and-body-mass-change-based-on-a-penguins-sex",
    "href": "slides/week7-day2.html#how-does-the-relationship-between-bill-length-and-body-mass-change-based-on-a-penguins-sex",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "How does the relationship between bill length and body mass change based on a penguin’s sex?",
    "text": "How does the relationship between bill length and body mass change based on a penguin’s sex?"
  },
  {
    "objectID": "slides/week7-day2.html#what-changes",
    "href": "slides/week7-day2.html#what-changes",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "What changes?",
    "text": "What changes?\nThe original sample of 344 penguins were broken down into the following groups (plus 11 NA values):\n\n\n\n\n\nSex\nSample Size\n\n\n\n\nfemale\n165\n\n\nmale\n168\n\n\n\n\n\n\n\n\nBefore we resampled with replacement 344 times.\n\nIf we resample with replacement 333 times, are we guaranteed to get 165 female penguins and 168 male penguins in each sample?"
  },
  {
    "objectID": "slides/week7-day2.html#getting-our-observed-statistic",
    "href": "slides/week7-day2.html#getting-our-observed-statistic",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Getting our Observed Statistic",
    "text": "Getting our Observed Statistic\nStep 1: Fitting our Model\n\n\nobserved_fit &lt;- penguins %&gt;%\n  specify(bill_length_mm ~ body_mass_g * sex) %&gt;%\n  fit()\n\n\n\n\n\n\n\n\nSyntax changes\n\n\nWhen we have multiple explanatory variables, we need to use the “tilde” (~) syntax to specify our model. We also use the fit() function instead of the calculate() function."
  },
  {
    "objectID": "slides/week7-day2.html#getting-our-observed-statistic-1",
    "href": "slides/week7-day2.html#getting-our-observed-statistic-1",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Getting our Observed Statistic",
    "text": "Getting our Observed Statistic\nStep 2: Finding our Statistic\n\n\n\n\n\nterm\nestimate\n\n\n\n\nintercept\n25.5713814\n\n\nbody_mass_g\n0.0042787\n\n\nsexmale\n5.5160611\n\n\nbody_mass_g:sexmale\n-0.0010301\n\n\n\n\n\n\n\n\nWhat is our observed statistic for this investigation?"
  },
  {
    "objectID": "slides/week7-day2.html#generating-bootstrap-fits",
    "href": "slides/week7-day2.html#generating-bootstrap-fits",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Generating Bootstrap Fits",
    "text": "Generating Bootstrap Fits\n\n\nbootstrap_fits &lt;- penguins %&gt;%\n  specify(bill_length_mm ~ body_mass_g * sex) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  fit()"
  },
  {
    "objectID": "slides/week7-day2.html#obtaining-a-bootstrap-confidence-interval",
    "href": "slides/week7-day2.html#obtaining-a-bootstrap-confidence-interval",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Obtaining a Bootstrap Confidence Interval",
    "text": "Obtaining a Bootstrap Confidence Interval\n\n\nget_confidence_interval(bootstrap_fits,\n                        point_estimate = observed_fit,\n                        level = 0.90, \n                        type = \"percentile\")\n\n# A tibble: 4 × 3\n  term                lower_ci   upper_ci\n  &lt;chr&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n1 body_mass_g          0.00365  0.00487  \n2 body_mass_g:sexmale -0.00202 -0.0000684\n3 intercept           22.9     28.3      \n4 sexmale              0.995   10.2"
  },
  {
    "objectID": "slides/week7-day2.html#how-do-we-interpret-this-interval",
    "href": "slides/week7-day2.html#how-do-we-interpret-this-interval",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "How do we interpret this interval?",
    "text": "How do we interpret this interval?\n\n\n\n\n\nLower Bound\nUpper Bound\n\n\n\n\n-0.0020159\n-0.0000684\n\n\n\n\n\n\n\n\n\nWe are 90% confident that the for every 100 gram increase in a penguin’s body mass (~0.25lbs), the length of a male penguin’s bill is between 0.068 and 2.02mm shorter than a female penguin’s bill."
  },
  {
    "objectID": "slides/week7-day2.html#how-do-body-mass-and-flipper-length-influence-a-penguins-bill-length",
    "href": "slides/week7-day2.html#how-do-body-mass-and-flipper-length-influence-a-penguins-bill-length",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "How do body mass and flipper length influence a penguin’s bill length?",
    "text": "How do body mass and flipper length influence a penguin’s bill length?\n\n\n\nobserved_fit &lt;- penguins %&gt;%\n  specify(bill_length_mm ~ body_mass_g + flipper_length_mm) %&gt;%\n  fit()\n\nbootstrap_fits &lt;- penguins %&gt;%\n  specify(bill_length_mm ~ body_mass_g + flipper_length_mm) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  fit()"
  },
  {
    "objectID": "slides/week7-day2.html#visualizing-bootstrap-distributions",
    "href": "slides/week7-day2.html#visualizing-bootstrap-distributions",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Visualizing Bootstrap Distributions",
    "text": "Visualizing Bootstrap Distributions\n\nvisualise(bootstrap_fits)"
  },
  {
    "objectID": "slides/week7-day2.html#obtaining-and-interpreting-confidence-intervals",
    "href": "slides/week7-day2.html#obtaining-and-interpreting-confidence-intervals",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Obtaining and Interpreting Confidence Intervals",
    "text": "Obtaining and Interpreting Confidence Intervals\n\n\n\n\n\nterm\nLower Bound\nUpper Bound\n\n\n\n\nbody_mass_g\n-0.0002028\n0.0015676\n\n\nflipper_length_mm\n0.1728445\n0.2663523\n\n\nintercept\n-9.4472566\n3.5454740\n\n\n\n\n\n\n\n\nHow would you interpret the confidence interval for flipper_length_mm? For body_mass_g?"
  },
  {
    "objectID": "slides/week7-day2.html#holding-the-other-variables-constant",
    "href": "slides/week7-day2.html#holding-the-other-variables-constant",
    "title": "Confidence Intervals – Real Life Sampling Distributions",
    "section": "Holding the Other Variable(s) Constant",
    "text": "Holding the Other Variable(s) Constant\n\nHolding body mass constant, we are 95% confident that a 1mm increase in flipper length is associated with between a 0.173 and 0.266mm increase in a penguin’s bill length.\n\n\n\nHolding flipper length constant, we are 95% confident that a 100 gram increase in body mass is associated with between a 0.203mm decrease and a 0.157mm increase in a penguin’s bill length."
  },
  {
    "objectID": "slides/week6-day1.html#feedback-revisions",
    "href": "slides/week6-day1.html#feedback-revisions",
    "title": "Variable Selection in Multiple Regression",
    "section": "Feedback & Revisions",
    "text": "Feedback & Revisions\n\n\n\nYou are expected to directly state what variable(s) are associated with each aesthetic (e.g., the x-axis is associated with time).\nAesthetics are associated with variables\n\nIf there is only one color / shape / linetype being used in the plot, it is not associated with a variable!\nIf there are multiple plots, then there is a variable associated with how the plots are separated!\n\n\n\n\n\n\n\n\n\n\nRevision Deadline\n\n\nRevisions for Statistical Critiques are due next Thursday (February 22)"
  },
  {
    "objectID": "slides/week6-day1.html#research-questions",
    "href": "slides/week6-day1.html#research-questions",
    "title": "Variable Selection in Multiple Regression",
    "section": "Research Questions",
    "text": "Research Questions\n\n\nYour research question should be a question\nYour question should be able to be addressed with a multiple linear regression\n\nIs the relationship between stem length and stem dry mass different for watersheds treated with calcium versus without?\nWhat is the relationship between the size of a fiddler crab and the latitude and water temperature in which it lives?"
  },
  {
    "objectID": "slides/week6-day1.html#data-visualizations-coefficient-interpretations",
    "href": "slides/week6-day1.html#data-visualizations-coefficient-interpretations",
    "title": "Variable Selection in Multiple Regression",
    "section": "Data Visualizations & Coefficient Interpretations",
    "text": "Data Visualizations & Coefficient Interpretations\n\nDescriptions of your visualizations should address:\n\nform, direction, strength, and unusual points\n\nYou should interpret every coefficient from your regression equation(s)!\n\nEvery y-intercept and every slope"
  },
  {
    "objectID": "slides/week6-day1.html#study-limitations-scope-of-inference",
    "href": "slides/week6-day1.html#study-limitations-scope-of-inference",
    "title": "Variable Selection in Multiple Regression",
    "section": "Study Limitations – Scope of Inference",
    "text": "Study Limitations – Scope of Inference\nBased on how the study was designed, what population can you infer these results onto?\n\n\n\nEvery sugar maple?\nSugar maples in the Northeast?\nSugar maples in New Hampshire?\nSugar maples in the Hubbard Brook Experimental Forest?\nSugar maples in similar areas of the Hubbard Brook Experimental Forest to those that were sampled?\nThis sample of sugar maples?\n\n\n\n\nWhy?"
  },
  {
    "objectID": "slides/week6-day1.html#study-limitations-causal-inference",
    "href": "slides/week6-day1.html#study-limitations-causal-inference",
    "title": "Variable Selection in Multiple Regression",
    "section": "Study Limitations – Causal Inference",
    "text": "Study Limitations – Causal Inference\nBased on how the study was designed, what can you say about the relationships between the variables?\n\n\n\nCan you say that your explanatory variable(s) causes changes in your response variable?\nWhy or why not?\nWhat can you say about these relationships?"
  },
  {
    "objectID": "slides/week6-day1.html#writing-conclusions",
    "href": "slides/week6-day1.html#writing-conclusions",
    "title": "Variable Selection in Multiple Regression",
    "section": "Writing Conclusions",
    "text": "Writing Conclusions\n\nCircle back to your research question\nWhat did you learn in your visualizations?\nWhat did you learn in your regression model?\nWhat conclusions would you reach about your research question?\n\n\n\n\n\n\n\nNo “significance” & no p-values"
  },
  {
    "objectID": "slides/week6-day1.html#section",
    "href": "slides/week6-day1.html#section",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "What is model selection?\n\n\n \n\nWhy use model selection?"
  },
  {
    "objectID": "slides/week6-day1.html#section-1",
    "href": "slides/week6-day1.html#section-1",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Lots of available predictor variables\n\n\nevals:\n\n\n\n\n\nID\nprof_ID\nscore\nage\nbty_avg\ngender\nethnicity\nlanguage\nrank\npic_outfit\npic_color\ncls_did_eval\ncls_students\ncls_level\n\n\n\n\n429\n86\n3.9\n42\n7.833\nmale\nnot minority\nenglish\ntenured\nnot formal\nblack&white\n17\n20\nupper\n\n\n147\n27\n4.4\n52\n4.833\nmale\nminority\nnon-english\ntenured\nformal\ncolor\n13\n16\nupper\n\n\n134\n24\n4.5\n64\n4.167\nmale\nnot minority\nenglish\ntenured\nnot formal\ncolor\n51\n68\nupper"
  },
  {
    "objectID": "slides/week6-day1.html#section-2",
    "href": "slides/week6-day1.html#section-2",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Interested in prediction not explanation\n\n\n\n\nYou want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables \\(x\\). You don’t care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \\(y\\) using the information in \\(x\\).\nModernDive"
  },
  {
    "objectID": "slides/week6-day1.html#section-3",
    "href": "slides/week6-day1.html#section-3",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "How do you use model selection?\n\n\n\n\n\nStepwise Selection\n\nForward Selection\nBackward Selection\n\n\n\n\n\n\nResampling Methods\n\nCross Validation\nTesting / Training Datasets\n\n\n\n\n\n\n\nWith any of these methods, you get to choose how you decide if one model is better than another model."
  },
  {
    "objectID": "slides/week6-day1.html#section-4",
    "href": "slides/week6-day1.html#section-4",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "\\(R^2\\) – Coefficient of Determination\n\n\n\n\n\n\nWright, Sewall (1921). Correlation and causation. Journal of Agricultural Research 20: 557-585.\n\n\n\n\n\n\nIn statistics, the coefficient of determination, denoted \\(R^2\\) or \\(r^2\\) and pronounced “R squared,” is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-5",
    "href": "slides/week6-day1.html#section-5",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "\\(R^2 = 1 - \\frac{\\text{var}(\\text{residuals})}{\\text{var}(y)}\\)\n\n\n\\(\\text{var}(\\text{residuals})\\) is the variance of the residuals “leftover” from the regression model\n\\(\\text{var}(y)\\) is the inherent variability of the response variable\n\n\n\nSuppose we have a simple linear regression with an \\(R^2\\) of 0.85. How would you interpret this quantity?"
  },
  {
    "objectID": "slides/week6-day1.html#section-6",
    "href": "slides/week6-day1.html#section-6",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "But! \\(R^2\\) always increases as you increase the number of explanatory variables.\n\n\n\n\nThe variance of the residuals will always decrease when you include additional explanatory variables.\n\n\n\nSimple Linear Regression\n\\(0.85 = 1 - \\frac{0.75}{5}\\)\n\n\n\nOne Additional Variable\n\\(0.86 = 1 - \\frac{0.7}{5}\\)"
  },
  {
    "objectID": "slides/week6-day1.html#section-7",
    "href": "slides/week6-day1.html#section-7",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "Adjusted \\(R^2\\)\n\n\n\n\n\nMordecai Ezekiel (1930). Methods Of Correlation Analysis, Wiley, pp. 208-211.\n\n\n\n\n\n\nThe use of an adjusted \\(R^2\\) is an attempt to account for the phenomenon of the \\(R^2\\) automatically increasing when extra explanatory variables are added to the model.\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-8",
    "href": "slides/week6-day1.html#section-8",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "\\(R^2_{adj} = 1 - R^2 \\times \\frac{(n - 1)}{(n - k - 1)}\\)\n\n\n\\(n\\) is the sample size\n\\(k\\) is the number of coefficients needed to be calculated\n\n\n\nSuppose you have a categorical variable with 4 levels included in your parallel slopes multiple linear regression.\n\nWhat value will you use for \\(k\\) in the calculation of \\(n - k - 1\\)?"
  },
  {
    "objectID": "slides/week6-day1.html#section-9",
    "href": "slides/week6-day1.html#section-9",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "p-values\n\n\n\n\n\nFisher R. A. (1950). Statistical methods for research workers.\n\n\n\n\n\n\nIn null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis.\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-10",
    "href": "slides/week6-day1.html#section-10",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "AIC\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike, H. (1973). Information theory and an extension of the maximum likelihood principle.\n\n\n\n\n\n\nThe Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models.\nWikipedia"
  },
  {
    "objectID": "slides/week6-day1.html#section-11",
    "href": "slides/week6-day1.html#section-11",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "How do you use AIC to choose a “best” model?\n\n\n\n\n\n\n\n\n\nmodel\nAIC\nDelta AIC\n\n\n\n\nFull Model\n4724.970\n0.000000\n\n\nAll Variables Except Year\n4727.242\n2.272501\n\n\nAll Variables Except Flipper Length\n4757.214\n32.244605\n\n\nAll Variables Except Species\n4793.681\n68.710933\n\n\n\n\n\n\n\n\n\n\n\n\nIf you’ve ever assessed whether \\(\\Delta\\) AIC \\(&gt; 2\\) you have done something that is mathematically close to \\(p &lt; 0.05\\)."
  },
  {
    "objectID": "slides/week6-day1.html#backward-selection-by-hand",
    "href": "slides/week6-day1.html#backward-selection-by-hand",
    "title": "Variable Selection in Multiple Regression",
    "section": "Backward Selection by Hand",
    "text": "Backward Selection by Hand\n\nStart with “full” model (every explanatory variable is included)\n\nUse adjusted \\(R^2\\) to summarize the “fit” of this model\n\nDecide which one variable to remove\n\nHighest adjusted \\(R^2\\)\n\nDecide what one variable to remove next\n\nHighest adjusted \\(R^2\\)\n\nKeep removing variables until adjusted \\(R^2\\) doesn’t increase"
  },
  {
    "objectID": "slides/week6-day1.html#section-12",
    "href": "slides/week6-day1.html#section-12",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "What’s your best model?"
  },
  {
    "objectID": "slides/week6-day1.html#adding-a-constraint",
    "href": "slides/week6-day1.html#adding-a-constraint",
    "title": "Variable Selection in Multiple Regression",
    "section": "Adding a Constraint",
    "text": "Adding a Constraint\nRepeat the same process, but now for a variable to be removed the adjusted \\(R^2\\) must increase by at least 2% (0.02)."
  },
  {
    "objectID": "slides/week6-day1.html#section-13",
    "href": "slides/week6-day1.html#section-13",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "What’s your best model?"
  },
  {
    "objectID": "slides/week6-day1.html#section-14",
    "href": "slides/week6-day1.html#section-14",
    "title": "Variable Selection in Multiple Regression",
    "section": "",
    "text": "If you’re not interested in prediction, what should you use instead?\n\n\nIn fact, many statisticians discourage the use of stepwise regression alone for model selection and advocate, instead, for a more thoughtful approach that carefully considers the research focus and features of the data.\nIntroduction to Modern Statistics"
  },
  {
    "objectID": "slides/week6-day1.html#peer-review",
    "href": "slides/week6-day1.html#peer-review",
    "title": "Variable Selection in Multiple Regression",
    "section": "Peer Review",
    "text": "Peer Review\nPlease print your Midterm Project and bring it to class!"
  },
  {
    "objectID": "project/midterm-proposal-513.html",
    "href": "project/midterm-proposal-513.html",
    "title": "STAT 513: Midterm Project Proposal",
    "section": "",
    "text": "This week you will get started on your midterm project by selecting what dataset you wish to analyze and writing an introduction about the dataset you chose."
  },
  {
    "objectID": "project/midterm-proposal-513.html#option-1-use-your-own-data",
    "href": "project/midterm-proposal-513.html#option-1-use-your-own-data",
    "title": "STAT 513: Midterm Project Proposal",
    "section": "1.1 Option 1 – Use your own data",
    "text": "1.1 Option 1 – Use your own data\nIf you have a dataset from your research which you believe can be appropriately modeled with a linear regression, you can propose to use these data."
  },
  {
    "objectID": "project/midterm-proposal-513.html#option-2-acquire-a-dataset-from-your-adviser",
    "href": "project/midterm-proposal-513.html#option-2-acquire-a-dataset-from-your-adviser",
    "title": "STAT 513: Midterm Project Proposal",
    "section": "1.2 Option 2 – Acquire a dataset from your adviser",
    "text": "1.2 Option 2 – Acquire a dataset from your adviser\nIf you do not have a dataset from your research, then you are expected to work with your adviser to obtain a dataset relevant to your research / project."
  },
  {
    "objectID": "project/midterm-proposal-513.html#deliverable",
    "href": "project/midterm-proposal-513.html#deliverable",
    "title": "STAT 513: Midterm Project Proposal",
    "section": "1.3 Deliverable",
    "text": "1.3 Deliverable\nFor the Midterm Project Proposal assignment on Canvas, you are required to submit the data you have chosen to use as an Excel file (stored as either a .csv or .xlsx)."
  },
  {
    "objectID": "project/final-project-directions.html",
    "href": "project/final-project-directions.html",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "For this project, you are expected to use a two-way ANOVA to investigate the relationship between one numerical response variable and two categorical explanatory variables. You are permitted to use the same dataset as your Midterm Project, so long as there are at least two categorical variables to choose from.\n\n\n\n\n\n\nDiscretizing numerical variables\n\n\n\nIf you would like to analyze a discrete numerical variable (e.g., number of pets) as a categorical variable, you will need to convert that variable into a categorical variable in R as R assumes all variables with numbers should be numerical.\nYou will need Dr. Theobold’s help to perform this task. Dr. Theobold will work with you to convert your variable as long as you request help before Thursday, March 7 at 4pm.",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#your-task",
    "href": "project/final-project-directions.html#your-task",
    "title": "Final Project Guidelines",
    "section": "",
    "text": "For this project, you are expected to use a two-way ANOVA to investigate the relationship between one numerical response variable and two categorical explanatory variables. You are permitted to use the same dataset as your Midterm Project, so long as there are at least two categorical variables to choose from.\n\n\n\n\n\n\nDiscretizing numerical variables\n\n\n\nIf you would like to analyze a discrete numerical variable (e.g., number of pets) as a categorical variable, you will need to convert that variable into a categorical variable in R as R assumes all variables with numbers should be numerical.\nYou will need Dr. Theobold’s help to perform this task. Dr. Theobold will work with you to convert your variable as long as you request help before Thursday, March 7 at 4pm.",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#data-description",
    "href": "project/final-project-directions.html#data-description",
    "title": "Final Project Guidelines",
    "section": "1.1 Data Description",
    "text": "1.1 Data Description\nIn 4-6 sentences describe:\n\nhow the data were collected\nthe context of the data (e.g., are the data from from a published study?)\nthe background of the research problem (e.g., why were the data collected?)",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#questions-of-interest",
    "href": "project/final-project-directions.html#questions-of-interest",
    "title": "Final Project Guidelines",
    "section": "1.2 Questions of Interest",
    "text": "1.2 Questions of Interest\nState the question(s) of interest you will address with your statistical analysis. The more specific you define the question of interest here, the easier the rest of the analysis and report will be. The research questions should be phrased in terms of your alternative hypothesis, meaning it should start with “Does the mean ___ differ based on ___ …”. Your Findings section should directly address the question(s) you pose here.\n\n\n\n\n\n\nMultiple research questions\n\n\n\nThis week you are starting with research questions are appropriate for the one-way ANOVA models you are fitting. However, you may find that you need to add research questions based on the models you fit in Week 10!",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#one-way-anova-models",
    "href": "project/final-project-directions.html#one-way-anova-models",
    "title": "Final Project Guidelines",
    "section": "3.1 One-Way ANOVA Models",
    "text": "3.1 One-Way ANOVA Models\nEveryone starts here! In this section, you need to do the following:\n\nFit two one-way ANOVA models – one model for each categorical explanatory variable\nObtain the ANOVA table for the model\nBased on the ANOVA table, state what decision was reached for the hypothesis in the one-way ANOVA model\nBased on the decision you made, state what you can conclude regarding the relationship between your variables",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#two-way-anova-additive-model",
    "href": "project/final-project-directions.html#two-way-anova-additive-model",
    "title": "Final Project Guidelines",
    "section": "3.2 Two-Way ANOVA Additive Model",
    "text": "3.2 Two-Way ANOVA Additive Model\n\n\n\n\n\n\nWhen to proceed\n\n\n\nIf you rejected the null hypothesis for both one-way ANOVA models, you can proceed to fitting a two-way ANOVA model\n\n\n\nFit a two-way ANOVA additive model\nObtain the tidy ANOVA table for the model\nBased on the ANOVA table, state what decision was reached for each hypothesis in the two-way ANOVA additive model.\nBased on the decision you made, state what you can conclude regarding the relationship between your variables.",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-directions.html#model-validity",
    "href": "project/final-project-directions.html#model-validity",
    "title": "Final Project Guidelines",
    "section": "3.3 Model Validity",
    "text": "3.3 Model Validity\n\nEvaluate the LINE conditions of both one-way ANOVA models\n\n\n\n\n\n\n\nCondition violations\n\n\n\nIf you find through the study design and / or your visualizations that certain model conditions are violated, you are expected to do your best to remedy these violations. If you need help figuring out how to do this, email Dr. Theobold before Thursday, March 7 at 4pm!\n\n\n\nBased on your analysis of the conditions, describe whether you believe the tests you performed are “reliable.”",
    "crumbs": [
      "Projects",
      "Final Project Description"
    ]
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "",
    "text": "We will be using a “forward” selection process for deciding on the “best” ANOVA model. Meaning, we will start with a simple model and keep adding complexity until it seems like the complexity isn’t “worth it.”"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#testing-era",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#testing-era",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Testing era",
    "text": "Testing era\n\naov(rating ~ era, data = movies) %&gt;% \n  tidy()\n\n# A tibble: 2 × 6\n  term         df  sumsq meansq statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 era           3   4.59   1.53     0.669   0.575\n2 Residuals    52 119.     2.28    NA      NA    \n\n\nWith a p-value of 0.575 (from an F-statistic of 0.669 with 3 and 52 degrees of freedom) at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating differs for at least one era.\n\n\n\n\n\n\nHypotheses\n\n\n\nThe era line of the ANOVA table is testing the following hypotheses:\n\\(H_0\\): The mean movie rating is the same for every era\n\\(H_A\\): The mean movie rating is different for at least one era"
  },
  {
    "objectID": "project/final-project-help/owa-to-twa-model-selection-process.html#testing-genre",
    "href": "project/final-project-help/owa-to-twa-model-selection-process.html#testing-genre",
    "title": "One-Way ANOVA to Two-Way ANOVA Model Selection Process",
    "section": "Testing genre",
    "text": "Testing genre\n\n\n\n\n\n\nTwo groups\n\n\n\nNote that genre has only two levels—action and romance. So, when using simulation-based methods, we need to use a \"diff in means\" statistic instead of an \"F\" statistic.\n\n\n\nobs_diff &lt;- movies %&gt;% \n  specify(response = rating, \n          explanatory = genre) %&gt;% \n  calculate(stat = \"diff in means\", \n            order = c(\"Action\", \"Romance\")\n            )\n\npermutation_dist &lt;- movies %&gt;% \n  specify(response = rating, \n          explanatory = genre) %&gt;% \n  hypothesise(null = \"independence\") %&gt;% \n  generate(reps = 1000, type = \"permute\") %&gt;% \n  calculate(stat = \"diff in means\", \n            order = c(\"Action\", \"Romance\")\n            )\n\nvisualise(permutation_dist) +\n  labs(x = \"Simulated Difference in Mean IMDB Rating (Action - Romance)\")\n\n\n\n\n\n\n\nget_p_value(permutation_dist, \n            obs_stat = obs_diff, \n            direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.184\n\n\nWith a p-value of 0.184 from an observed difference in means of -0.578 at a significance level of 0.1, I fail to reject the null hypothesis. Thus, the data have unconvincing evidence that the mean movie rating of action movie is different from romance movies.\n\n\n\n\n\n\nHypotheses\n\n\n\nFor our hypothesis test, we are testing if the mean movie rating is different for at least genre, but technically there are two genres (action, romance). So, we are actually testing following hypotheses:\n\\(H_0\\): The mean movie rating is the same for romance and action movies\n\\(H_A\\): The mean movie rating is different for romance and action movies"
  },
  {
    "objectID": "project/final-project-help/assess_independence.html",
    "href": "project/final-project-help/assess_independence.html",
    "title": "Evaluating Independence",
    "section": "",
    "text": "Independence is about observations and relationships between observations."
  },
  {
    "objectID": "project/final-project-help/assess_independence.html#independence-within-groups",
    "href": "project/final-project-help/assess_independence.html#independence-within-groups",
    "title": "Evaluating Independence",
    "section": "Independence Within Groups",
    "text": "Independence Within Groups\n\nWhat are the groups in your analysis?\nAre the observations within those groups related?\n\nIs an observation included multiple times within a group?\nAre there relationships (e.g., biological, spatial, temporal) between the observations?"
  },
  {
    "objectID": "project/final-project-help/assess_independence.html#independence-between-groups",
    "href": "project/final-project-help/assess_independence.html#independence-between-groups",
    "title": "Evaluating Independence",
    "section": "Independence Between Groups",
    "text": "Independence Between Groups\n\nWhat are the groups in your analysis?\nAre the observations between those groups related?\n\nCan an observation appear in more than one group?"
  },
  {
    "objectID": "project/final-project-grading-rubric.html",
    "href": "project/final-project-grading-rubric.html",
    "title": "Final Project Rubric",
    "section": "",
    "text": "Introduction\nMethods\nFindings\nStudy Limitations\nConclusions\n\n\n\n\nExcellent\n\nProvides robust description of data and context.\nWell articulated research question(s) correctly phrased for a one-way ANOVA.\n\n\nOutlines variables considered for analysis, describes how variables were collected, including their associated units / levels.\nCreates data visualizations requested, axis labels allow visualization to be interpreted independent of additional information.\nProvides robust description of relationships seen in visualization.\n\n\nCorrectly fits one-way ANOVA models and obtains ANOVA table.\nProvides well written conclusion regarding the hypotheses tested in the both one-way ANOVAs, with few if any errors that do not bring into question understanding of the underlying concepts.\nCorrectly determines if an additive two-way ANOVA model should be fit.\n(If applicable) Provides well written conclusion regarding the hypotheses tested in the two-way ANOVA, with few if any errors that do not bring into question understanding of the underlying concepts.\nEvaluates conditions of one-way ANOVA model with few if any errors that do not bring into question understanding of the underlying concepts.\n\n\nClearly articulates sampling methodology used in study, connects methodology to whom the analysis can be inferred onto.\nClearly articulates the design of the study, connects study design to what relationships can be inferred between the variables.\nInterpretations are all described in the context of the data, not in general terms.\n\n\nProvides well articulated description of how the findings from the statistical model(s) connect to what was seen in visualizations.\nReaches a conclusion for the research question stated in the Introduction.\n\n\n\nSatisfactory\n\nProvides satisfactory description of data, but description lacks some details regarding the context.\nResearch question(s) articulated but contain instances of incorrect language for a one-way ANOVA.\n\n\nOutlines variables considered for analysis, omitting some details regarding how the variables were collected and / or their associated units / levels.\nCreates data visualizations requested, some additional information is needed to supplement axis labels.\nProvides satisfactory description of relationships seen in visualization, where some details have been omitted.\n\n\nCorrectly fits one-way ANOVA models and obtains ANOVA table.\nProvides well written conclusion regarding the hypotheses tested in the both one-way ANOVAs, with some errors that bring into question understanding of underlying concepts.\nCorrectly determines if an additive two-way ANOVA model should be fit.\n(If applicable) Provides well written conclusion regarding the hypotheses tested in the two-way ANOVA, with some errors that bring into question understanding of underlying concepts.\nEvaluates conditions of one-way ANOVA model with some errors that bring into question understanding of underlying concepts.\n\n\nDescribes sampling methodology used in study and states whom the analysis can be inferred onto, but connections between methods and inference are at times unclear.\nStatement regarding what relationships can be inferred between the variables is mostly clear, with connections to the design of the study.\nInterpretations are all described in the context of the study.\n\n\nDescription of connection between model findings and visualizations are mostly clear, but connections to the research question stated in the Introduction are at times unclear.\n\n\n\nProgressing\n\nProvides poor description of data, where description lacks multiple details regarding the context.\nResearch question(s) poorly articulated, with little to no general connection to a one-way ANOVA.\n\n\nOutlines variables considered for analysis, with very few details regarding how they were collected and / or their associated units / levels.\nCreates data visualizations requested, axis labels require additional information to understand the context.\nProvides poor description of relationships seen in visualization.\n\n\nDoes not correctly fit one-way ANOVA models and obtains ANOVA table.\nProvides written conclusion regarding the hypotheses tested in the both one-way ANOVAs, with many errors that bring into question understanding of underlying concepts.\nDoes not correctly determine if an additive two-way ANOVA model should be fit.\n(If applicable) Provides written conclusion regarding the hypotheses tested in the both two-way ANOVA, with many errors that bring into question understanding of underlying concepts.\nEvaluates conditions of one-way ANOVA model with many errors that bring into question understanding of underlying concepts.\n\n\nStatement regarding who the analysis can be inferred to is mostly unclear, with few if any connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with few if any connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study.\n\n\nDescription of connection between model findings and visualization is unclear with little to no connection to the research question stated in the Introduction.\n\n\n\nNo Credit\n\nProvides little to no description of data.\nResearch question(s) not articulated.\n\n\nOutlines variables considered for analysis, with no details regarding their associated units / levels.\nCreates data visualizations requested, axis labels are not changed.\nProvides little to no description of relationships seen in visualization.\n\n\nDoes not fit one-way ANOVA models and obtains ANOVA table.\nInterpretations of one-way ANOVA contain multiple errors that demonstrate little to no understanding of the underlying concepts.\nDoes not correctly determine if an additive two-way ANOVA model should be fit.\nEvaluation of model conditions bring into question any understanding of the underlying concepts.\n\n\nStatement regarding who the analysis can be inferred to is mostly unclear, with no connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with no connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study.\n\n\nLittle to no description of connection between models fit and visualization, no connection to the research question stated in the Introduction."
  },
  {
    "objectID": "project/final-project-grading-rubric.html#overall-project-grade",
    "href": "project/final-project-grading-rubric.html#overall-project-grade",
    "title": "Final Project Rubric",
    "section": "Overall Project Grade",
    "text": "Overall Project Grade\n\n\n\n\n\n\n\n\n\nExcellent Project\nSatisfactory Project\nProgressing Project\nNo Credit\n\n\n\n\nAt most one section is marked “Satisfactory” the remainder are marked “Excellent”\n\n\nPresentation is completed\nAt most one section is marked “Progressing” the remainder are marked “Satisfactory” or “Excellent”\n\nPresentation is completed\nAt most two sections are marked “Progressing” the remainder are marked “Satisfactory”\n\nPresentation is not completed\nAt most one section is marked “No Credit” and the remainder of the sections are marked “Progressing”\n\nPresentation is not completed"
  },
  {
    "objectID": "project/final-presentations/presentation-instructions.html",
    "href": "project/final-presentations/presentation-instructions.html",
    "title": "Final Project Presentations",
    "section": "",
    "text": "During finals week each of you will give a 3-minute presentation on one aspect of your final project you found the most interesting. Notice, you need to pick one aspect, since your presentation is so short.\nHere are some examples of what you could choose:\nOf course you could choose other topics, the only requirement is that your topic is related to what you did in your final project."
  },
  {
    "objectID": "project/final-presentations/presentation-instructions.html#slides",
    "href": "project/final-presentations/presentation-instructions.html#slides",
    "title": "Final Project Presentations",
    "section": "Slides",
    "text": "Slides\nFor your presentation you are allowed to make two slides:\n\nA title slide (make it fun!) with your name\nA content slide\n\nSlides are due by 5pm the night before your final exam timeslot. If you do not submit slides by the deadline, you will not be allowed to present.\n\nSTAT 313-02 – slides are due by Monday (June 10) by 5pm\nSTAT 313-01 – slides are due by Tuesday (June 11) by 5pm\n\n\n\n\n\n\n\nCaution\n\n\n\nYour slides must be submitted as a PDF. Make sure they are saved in the correct orientation! I do not have the time to open PowerPoint slides and convert them to a PDF. So, if you do not submit your slides as a PDF, you will not be allowed to present."
  },
  {
    "objectID": "project/final-presentations/presentation-instructions.html#presentations",
    "href": "project/final-presentations/presentation-instructions.html#presentations",
    "title": "Final Project Presentations",
    "section": "Presentations",
    "text": "Presentations\nFor ease, I will randomly order the presentations. If you have a strong preference for when you would like to present, you can request a position. However, I cannot guarantee that I can accommodate every request.\nI will publish the order of presentations by 8pm the day before the final exam."
  },
  {
    "objectID": "project/midterm-project-grading-rubric.html",
    "href": "project/midterm-project-grading-rubric.html",
    "title": "Midterm Project Rubric",
    "section": "",
    "text": "Introduction\nMethods\nFindings\nStudy Limitations\nConclusions\n\n\n\n\nExcellent\nProvides robust description of data, collection methods, context.\n\nWell articulated research question(s) correctly phrased for multiple linear regression analysis.\nOutlines variables considered for analysis, including all relevant regarding their associated units / levels.\nCreates data visualizations requested, axis labels allow visualization to be interpreted independent of additional information.\nProvides robust description of relationships seen in visualization, clearly connects what is seen in the visualizations to the statistical model chosen for analysis.\nCorrectly fits and obtains coefficients for model stated in Methods section.\nProvides well written regression equation(s) which clearly indicate the context of the regression.\nInterprets the coefficients associated with the regression model (intercept(s) and slope(s)) with few if any errors that do not bring into question understanding of the underlying concepts.\nClearly states who the analysis can be inferred to, connecting to sampling methodology used.\nClearly states what relationships can be inferred between the variables, connecting to the design of the study.\nInterpretations are all described in the context of the study.\nProvides well articulated description of how regression model connects to what was seen in visualization, reaches a conclusion for the research question stated in the Introduction.\n\n\nSatisfactory\nProvides satisfactory description of data with small elements omitted.\nResearch question(s) articulated but contain instances of incorrect language for a multiple linear regression analysis.\nOutlines variables considered for analysis, omitting some details regarding their associated units / levels.\nCreates data visualizations requested, some additional information is needed to supplement axis labels.\nProvides description of relationships seen in visualization, but connections to the statistical model chosen for analysis are at times unclear.\nCorrectly fits and obtains coefficients for model stated in Methods section.\nProvides regression equation(s) with some context, but meaning is sometimes unclear.\nInterprets the coefficients associated with the regression model (intercept(s) and slope(s)), with some errors that bring into question understanding of underlying concepts.\nStatement regarding who the analysis can be inferred to is mostly clear, with connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly clear, with connections to the design of the study.\nInterpretations are all described in the context of the study.\nDescription of connection between regression model and visualization are mostly clear, but connections to the research question stated in the Introduction are at times unclear.\n\n\nProgressing\nProvides poor description of data with multiple elements omitted.\nResearch question(s) poorly articulated, with little to no general connection to a multiple linear regression analysis.\nOutlines variables considered for analysis, with very few details regarding their associated units / levels.\nCreates data visualizations requested, axis labels require additional information to understand the context.\nProvides poor description of relationships seen in visualization,connections to the statistical model chosen for analysis are quite unclear.\nDoes not fit model stated in Methods section but obtains coefficients.\nProvides regression equation(s) with little to no context, meaning is often unclear.\nInterprets the coefficients associated with the regression model (intercept(s) and slope(s)), with many errors that bring into question understanding of underlying concepts.\nStatement regarding who the analysis can be inferred to is mostly unclear, with few if any connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with few if any connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study.\nDescription of connection between regression model and visualization is unclear with little to no connection to the research question stated in the Introduction.\n\n\nNo Credit\nProvides little to no description of data.\nResearch question(s) not articulated.\nOutlines variables considered for analysis, with no details regarding their associated units / levels.\nCreates data visualizations requested, axis labels are not changed.\nProvides little to no description of relationships seen in visualization,with no connections to the statistical model chosen for analysis.\nDoes not fit model stated in Methods section and does not obtain coefficients.\nProvides regression equation(s) with no context, meaning is almost entirely unclear.\nInterpretations of coefficients associated with the regression model (intercept(s) and slope(s)) bring into question any understanding of underlying concepts.\nStatement regarding who the analysis can be inferred to is mostly unclear, with no connections to the sampling methodology used.\nStatement regarding what relationships can be inferred between the variables is mostly unclear, with no connections to the design of the study.\nInterpretations are done in general statements, not in the context of the study.\nLittle to no description of connection between regression model and visualization,no connection to the research question stated in the Introduction."
  },
  {
    "objectID": "project/midterm-project-grading-rubric.html#overall-project-grade",
    "href": "project/midterm-project-grading-rubric.html#overall-project-grade",
    "title": "Midterm Project Rubric",
    "section": "Overall Project Grade",
    "text": "Overall Project Grade\n\n\n\n\n\n\n\n\n\nExcellent Project\nSatisfactory Project\nProgressing Project\nNo Credit\n\n\n\n\nAt most one section is marked “Satisfactory” the remainder are marked “Excellent”\nAt most one section is marked “Progressing” the remainder are marked “Satisfactory” or “Excellent”\nAt most two sections are marked “Progressing” the remainder are marked “Satisfactory”\nAt most one section is marked “No Credit” and the remainder of the sections are marked “Progressing”"
  },
  {
    "objectID": "project/midterm-project-help/model-selection.html#one-categorical-one-numerical-explanatory-variable",
    "href": "project/midterm-project-help/model-selection.html#one-categorical-one-numerical-explanatory-variable",
    "title": "Model Selection Process",
    "section": "One Categorical & One Numerical Explanatory Variable",
    "text": "One Categorical & One Numerical Explanatory Variable"
  },
  {
    "objectID": "project/midterm-project-help/model-selection.html#two-numerical-explanatory-variables",
    "href": "project/midterm-project-help/model-selection.html#two-numerical-explanatory-variables",
    "title": "Model Selection Process",
    "section": "Two Numerical Explanatory Variables",
    "text": "Two Numerical Explanatory Variables"
  },
  {
    "objectID": "project/final_project_template-313.html#variables",
    "href": "project/final_project_template-313.html#variables",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Variables",
    "text": "Variables"
  },
  {
    "objectID": "project/final_project_template-313.html#data-visualizations",
    "href": "project/final_project_template-313.html#data-visualizations",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Data Visualizations",
    "text": "Data Visualizations"
  },
  {
    "objectID": "project/final_project_template-313.html#description-of-relationships",
    "href": "project/final_project_template-313.html#description-of-relationships",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Description of Relationships",
    "text": "Description of Relationships"
  },
  {
    "objectID": "project/final_project_template-313.html#one-way-anova-of-and",
    "href": "project/final_project_template-313.html#one-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "One-Way ANOVA of  and ",
    "text": "One-Way ANOVA of  and"
  },
  {
    "objectID": "project/final_project_template-313.html#one-way-anova-of-and-1",
    "href": "project/final_project_template-313.html#one-way-anova-of-and-1",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "One-Way ANOVA of  and ",
    "text": "One-Way ANOVA of  and"
  },
  {
    "objectID": "project/final_project_template-313.html#additive-two-way-anova-of-and",
    "href": "project/final_project_template-313.html#additive-two-way-anova-of-and",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Additive Two-Way ANOVA of , , AND ",
    "text": "Additive Two-Way ANOVA of , , AND"
  },
  {
    "objectID": "project/final_project_template-313.html#model-validity",
    "href": "project/final_project_template-313.html#model-validity",
    "title": "Your Title Goes Here! Make it fun!",
    "section": "Model Validity",
    "text": "Model Validity"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#student-hours",
    "href": "course-support.html#student-hours",
    "title": "Course support",
    "section": "Student Hours",
    "text": "Student Hours\n\n\n\n\n\n\n\nDay\nTime\n\n\nMondays\n3:10 pm – 4:00 pm (in-person)\n\n\nWednesdays\n2:10 pm - 3:00 pm (individual meetings via Zoom)\n3:10 - 4:00 (in-person)\n\n\n\nA lot of questions are most effectively answered in-person, and student hours are the best place to come ask me questions! I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself.\n\nPersonal Meetings\nIf you would like to talk with me individually, I’ve reserved time on Wednesdays from 3:10pm to 4:00pm for individual appointments. You can make appointments through the following link: https://calendly.com/allisontheobold\nI do request that you make appointments at least 1-hour ahead of time, so I don’t miss our meeting!\nIf you need to meet, but none of the student hours work for you please let me know! It is possible we can communicate asynchronously through Discord or email, but I am happy to schedule a meeting during another time if necessary.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#discord",
    "href": "course-support.html#discord",
    "title": "Course support",
    "section": "Discord",
    "text": "Discord\nWe will use Discord to manage questions and responses regarding course content. There are channels for the different components of each week (e.g., Week 1 Lab Assignment). Please do not send an email about homework questions or questions about the course material. It is incredibly helpful for others in the course to see the questions you have and the responses to those questions. I will try to answer any questions posted to Discord within 3-4 hours (unless it is posted at midnight). If you think you can answer another student’s question, please respond!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should be posted to Discord), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Allison Theobold at atheobol@calpoly.edu.\nI do my best to reply to emails promptly and helpfully. However, I receive a lot of emails. To help both you and me, here are some specific expectations about emails:\n\nPlease tell me what course and section (by time or number) you are in!\nIf you email me between 9am and 4pm on Monday through Friday, I’ll try my best to reply to you on the same day.\nIf you email me outside of those hours, I will do my best to respond to you by the next working day. For my own mental health, I do not work on weekends. Thus, if you send an email after 4pm on Friday or during the weekend, you will not receive a response until Monday morning.\nIf your question is much easier to discuss face-to-face, I may ask you to meet with me in my office or on Discord (at a time that works for both of us) rather than answering directly in an email.\nInclude any relevant photos / screenshots / error messages / PDFs / links with your email.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#well-being-access-and-accommodations",
    "href": "course-support.html#well-being-access-and-accommodations",
    "title": "Course support",
    "section": "Well-being, Access, and Accommodations",
    "text": "Well-being, Access, and Accommodations\n\nWhat if I have accommodations or feel that accommodations would be beneficial to my learning?\nI enthusiastically support the mission of Disability Resource Center to make education accessible to all. I design all my courses with accessibility at the forefront of my thinking, but if you have any suggestions for ways I can make things more accessible, please let me know. Come talk to me if you need accommodation for your disabilities. I honor self-diagnosis: let’s talk to each other about how we can make the course as accessible as possible. See also the standard syllabus statements, which include more information about formal processes.\n\n\nI’m having difficulty paying for food and rent, what can I do?\nIf you have difficulty affording groceries or accessing sufficient food to eat every day, or if you lack a safe and stable place to live, and you believe this may affect your performance in the course, I urge you to contact the Dean of Students for support. Furthermore, please notify me if you are comfortable in doing so. This will enable me to advocate for you and to connect you with other campus resources.\n\n\nMy mental health is impairing my ability to engage in my classes, what should I do?\nNational surveys of college students have consistently found that stress, sleep problems, anxiety, depression, interpersonal concerns, death of a significant other and alcohol use are among the top ten health impediments to academic performance. If you are experiencing any mental health issues, I and Cal Poly are here to help you. Cal Poly’s Counseling Services (805-756-2511) is a free and confidential resource for assistance, support and advocacy.\n\n\nSomeone is threatening me, what can I do?\nI will listen and believe you if someone is threatening you. I will help you get the help you need. I commit to changing campus culture that responds poorly to dating violence and stalking.\n\n\nWhat if I can’t arrange for childcare?\nIf you are responsible for childcare on short notice, you are welcome to bring children to class with you. If you are a lactating parent, you many take breaks to feed your infant or express milk as needed. If I can support yo in navigating parenting, coursework, and other obligations in any way, please let me know.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that we will be using Duke’s computational resources in this course. These resources are freely available to you. As long as your computer can connect to the internet and open a browser window, you can perform the necessary computing for this course. All software we use is open-source and/or freely available.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "critique/critique-2.html#assignment-details",
    "href": "critique/critique-2.html#assignment-details",
    "title": "Statistical Critique 2: Exploring p-values",
    "section": "Assignment Details",
    "text": "Assignment Details\nIn your second statistical critique, you will focus on critiquing another key aspect of any statistical argument—statistical significance. No doubt you have seen \\(p\\)-values in a previous statistical course and / or disciplinary course, and this week you’re adding to that knowledge. For this critique you will compare the model you selected in your Midterm Project with what model you would have chosen based on a statistical test.\nThis critique involves coding! You can find a template for critique on Posit Cloud."
  },
  {
    "objectID": "critique/critique-2.html#footnotes",
    "href": "critique/critique-2.html#footnotes",
    "title": "Statistical Critique 2: Exploring p-values",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is my professional organization.↩︎"
  },
  {
    "objectID": "critique/grading-guides/critique-1-grading.html",
    "href": "critique/grading-guides/critique-1-grading.html",
    "title": "Stat Critique 1 Grading Guide",
    "section": "",
    "text": "Double Encoding\n\n\n\nIt’s okay if they only mention one of the aesthetics that a variable is associated with when it is mapped to multiple aesthetics.\n\n\nGrowing\n\nDoesn’t name variables mapped to each aesthetic (e.g., only says the x-axis, y-axis, and color are used)\n\nFeedback: We want you to state specifically the names of the variables mapped to each aesthetic!\n\nSays a color aesthetic is used but the same color is used for every observation\n\nFeedback: Careful! Aesthetics are places where variables are being incorporated into the plot. Is the color in this plot associated with a variable?\n\nMisses an aesthetic that is being used\n\nFeedback for general plots: You’ve got a great start, but there are some aesthetics in this plot that are missing from your description. Technically, there are [multiple plots / multiple colors / multiple lines] not just one. What variable is associated with these separate [plots / colors / lines]?\nFeedback for maps: You’ve got a great start, but there are some aesthetics in this plot that are missing from your description. Technically, we have “hidden” x- and y-variables. What variables are associated with the location of a state on the map?\n\n\n\nGrowing\n\nOnly says one thing the visualization does well\n\nFeedback: Careful! You were asked to provide TWO aspects of the visualization that are done well.\n\nAt least one statement about what the visualization does well is unclear\n\nFeedback: I’m having a hard time understanding what specific aspect you believe the visualization does well.\n\n\n\nGrowing\n\nOnly says one thing the visualization does well\n\nFeedback: Careful! You were asked to provide TWO aspects of the visualization that could be improved.\n\nAt least one statement about what the visualization does well is unclear\n\nFeedback: I’m having a hard time understanding what specific aspect you believe could be improved."
  },
  {
    "objectID": "critique/grading-guides/critique-1-grading.html#pop-visualization-nyt-scientific-visualization",
    "href": "critique/grading-guides/critique-1-grading.html#pop-visualization-nyt-scientific-visualization",
    "title": "Stat Critique 1 Grading Guide",
    "section": "",
    "text": "Double Encoding\n\n\n\nIt’s okay if they only mention one of the aesthetics that a variable is associated with when it is mapped to multiple aesthetics.\n\n\nGrowing\n\nDoesn’t name variables mapped to each aesthetic (e.g., only says the x-axis, y-axis, and color are used)\n\nFeedback: We want you to state specifically the names of the variables mapped to each aesthetic!\n\nSays a color aesthetic is used but the same color is used for every observation\n\nFeedback: Careful! Aesthetics are places where variables are being incorporated into the plot. Is the color in this plot associated with a variable?\n\nMisses an aesthetic that is being used\n\nFeedback for general plots: You’ve got a great start, but there are some aesthetics in this plot that are missing from your description. Technically, there are [multiple plots / multiple colors / multiple lines] not just one. What variable is associated with these separate [plots / colors / lines]?\nFeedback for maps: You’ve got a great start, but there are some aesthetics in this plot that are missing from your description. Technically, we have “hidden” x- and y-variables. What variables are associated with the location of a state on the map?\n\n\n\nGrowing\n\nOnly says one thing the visualization does well\n\nFeedback: Careful! You were asked to provide TWO aspects of the visualization that are done well.\n\nAt least one statement about what the visualization does well is unclear\n\nFeedback: I’m having a hard time understanding what specific aspect you believe the visualization does well.\n\n\n\nGrowing\n\nOnly says one thing the visualization does well\n\nFeedback: Careful! You were asked to provide TWO aspects of the visualization that could be improved.\n\nAt least one statement about what the visualization does well is unclear\n\nFeedback: I’m having a hard time understanding what specific aspect you believe could be improved."
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure-4",
    "href": "slides/week3-day2.html#choosing-what-to-measure-4",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure",
    "text": "Choosing What to Measure\nopenintro data on births – classifying race as white and non-white"
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure---gender",
    "href": "slides/week3-day2.html#choosing-what-to-measure---gender",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure - Gender",
    "text": "Choosing What to Measure - Gender\nKeller et al. (2017) designed a study to examine whether a community-based suicide prevention project could increase willingness to seek professional help for suicidal ideation among eastern Montana youth.\n. . .\n\nStudents attending the Let’s Talk theater workshop, were asked to report their gender, race, and age."
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure---gender-1",
    "href": "slides/week3-day2.html#choosing-what-to-measure---gender-1",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure - Gender",
    "text": "Choosing What to Measure - Gender\nResearchers provided students with the following question:\n\nWhat is your gender?\nMale, Female, Other\n\n. . .\n \n\nWhat information are the researchers missing?"
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure---sex",
    "href": "slides/week3-day2.html#choosing-what-to-measure---sex",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure - Sex",
    "text": "Choosing What to Measure - Sex\n\\(\\beta\\) blockers have been shown to improve survival in patients with congestive heart failure. These medicines block the effects of the hormone epinephrine (adrenaline). Research suggests that beta blockers have a differing effect for individuals with high levels of estrogen (Khan and Movahed 2000)."
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure---sex-1",
    "href": "slides/week3-day2.html#choosing-what-to-measure---sex-1",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure - Sex",
    "text": "Choosing What to Measure - Sex\nThe intake form for a local heart clinic asks the following question:\n\nWhat is sex were you assigned at birth?\nMale, Female, Other\n\n. . .\n \n\nWhat information are the doctors missing?"
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure---race",
    "href": "slides/week3-day2.html#choosing-what-to-measure---race",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure - Race",
    "text": "Choosing What to Measure - Race\nEvery year, the US publishes a large dataset on the public birth records for babies born that year. These data contain information about the baby, the pregnancy, and the birth parent(s)."
  },
  {
    "objectID": "slides/week3-day2.html#choosing-what-to-measure---race-1",
    "href": "slides/week3-day2.html#choosing-what-to-measure---race-1",
    "title": "Week 3 Day 2",
    "section": "Choosing What to Measure - Race",
    "text": "Choosing What to Measure - Race\nLooking at the births14 dataset, the race of the mother was classified as:\n\nwhite\nnonwhite\n\n. . .\n \n\nWhat information are researchers missing?"
  },
  {
    "objectID": "slides/week3-day2.html#section",
    "href": "slides/week3-day2.html#section",
    "title": "Week 3 Day 2",
    "section": "",
    "text": "If you wanted to find the mean mass for Cutthroat trout in each section, what would you do?"
  },
  {
    "objectID": "slides/week3-day2.html#group_by-multiple-variables",
    "href": "slides/week3-day2.html#group_by-multiple-variables",
    "title": "Week 3 Day 2",
    "section": "group_by() Multiple Variables",
    "text": "group_by() Multiple Variables\n\ntrout %&gt;% \n  group_by(section, unittype) %&gt;% \n  summarize(\n    mean_mass = mean(weight_g, \n                     na.rm = TRUE)\n            )\n\n# A tibble: 13 × 3\n# Groups:   section [2]\n   section    unittype mean_mass\n   &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n 1 Clear cut  C             8.47\n 2 Clear cut  P            13.4 \n 3 Clear cut  R             8.06\n 4 Clear cut  S             4.34\n 5 Clear cut  SC            4.80\n 6 Clear cut  &lt;NA&gt;         16.3 \n 7 Old growth C             7.58\n 8 Old growth I             9.81\n 9 Old growth IP            1.39\n10 Old growth P            10.4 \n11 Old growth R             6.66\n12 Old growth SC            5.38\n13 Old growth &lt;NA&gt;         13.4"
  },
  {
    "objectID": "slides/week3-day2.html",
    "href": "slides/week3-day2.html",
    "title": "Week 3 Day 2",
    "section": "",
    "text": "If you submitted revisions and your grade is still “Incomplete” – you forgot to submit reflections! Please add these as a comment to your assignment by the end of the day."
  },
  {
    "objectID": "slides/week3-day1.html",
    "href": "slides/week3-day1.html",
    "title": "Incorporating Categorical Variables",
    "section": "",
    "text": "How would you describe a categorical variable?"
  },
  {
    "objectID": "slides/week4-day1.html#step-4-write-estimated-regression-equation",
    "href": "slides/week4-day1.html#step-4-write-estimated-regression-equation",
    "title": "Introduction to Linear Regression",
    "section": "Step 4: Write Estimated Regression Equation",
    "text": "Step 4: Write Estimated Regression Equation\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n7.246\n0.046\n158.369\n0.000\n7.157\n7.336\n\n\nhabit: smoker\n-0.418\n0.128\n-3.270\n0.001\n-0.668\n-0.167\n\n\n\n\n\n\n\n\n\n\\[\\widehat{weight} = 7.23 - 0.4 \\cdot Smoker\\]\n\n\n\nBut what does \\(Smoker\\) represent???"
  },
  {
    "objectID": "slides/week4-day1.html#choosing-a-baseline-group",
    "href": "slides/week4-day1.html#choosing-a-baseline-group",
    "title": "Introduction to Linear Regression",
    "section": "Choosing a Baseline Group",
    "text": "Choosing a Baseline Group\n\n\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n7.246\n0.046\n158.369\n0.000\n7.157\n7.336\n\n\nhabit: smoker\n-0.418\n0.128\n-3.270\n0.001\n-0.668\n-0.167\n\n\n\n\n\n\n\n\nBased on the regression table, what habit group was chosen to be the baseline group?"
  },
  {
    "objectID": "slides/week4-day1.html#categorical-explanatory-variables",
    "href": "slides/week4-day1.html#categorical-explanatory-variables",
    "title": "Introduction to Linear Regression",
    "section": "Categorical Explanatory Variables",
    "text": "Categorical Explanatory Variables\n\n\\[\n  \\widehat{y} = b_0 + b_1 \\cdot x\n\\]\n\n\n\n\n\\(x\\) is a categorical variable with levels:\n\n\"nonsmoker\"\n\"smoker\"\n\n\n\n\nWe need to convert to:\n\na “baseline” group\n“offsets” / adjustments to the baseline"
  },
  {
    "objectID": "slides/week4-day1.html#rewriting-in-terms-of-indicator-variables",
    "href": "slides/week4-day1.html#rewriting-in-terms-of-indicator-variables",
    "title": "Introduction to Linear Regression",
    "section": "Rewriting in Terms of Indicator Variables",
    "text": "Rewriting in Terms of Indicator Variables\n\\[\\widehat{weight} = 7.23 - 0.4 \\cdot 1_{smoker}(x)\\]\nwhere\n\n\\(1_{smoker}(x) = 1\\) if the mother was a \"smoker\"\n\\(1_{smoker}(x) = 0\\) if the mother was a \"nonsmoker\""
  },
  {
    "objectID": "slides/week4-day1.html#project-proposal",
    "href": "slides/week4-day1.html#project-proposal",
    "title": "Introduction to Linear Regression",
    "section": "Project Proposal",
    "text": "Project Proposal\n\n\nChoose a dataset\nChoose one numerical response variable\nChoose one numerical explanatory variable\nChoose a second explanatory variable, it can be either numerical or categorical\n\n\n\n\n\n\n\n\nChecking values of your numerical variable(s)\n\n\n\nYour numerical variable cannot have a small number of values (e.g., 2 or 3). You can use the distinct() function to determine the unique values of your variable. For example, by running distinct(hbr_maples, year) I would discover that year only has two values (2003 and 2004), meaning year is not eligible to be a numerical response or explanatory variable. It could, however, be a categorical explanatory variable!\n\n\n\n\n\n\nWrite your Introduction"
  },
  {
    "objectID": "slides/week4-day1.html#causal-inference",
    "href": "slides/week4-day1.html#causal-inference",
    "title": "Introduction to Linear Regression",
    "section": "Causal Inference",
    "text": "Causal Inference\nWe just concluded that babies born to a \"smoker\" weigh, on average, 0.4 pounds less than babies born to a \"nonsmoker\".\n \n\nCan we conclude that smoking caused these babies to weigh less? Why or why not?"
  },
  {
    "objectID": "slides/week4-day2.html#would-you-be-in-this-class",
    "href": "slides/week4-day2.html#would-you-be-in-this-class",
    "title": "Week 4, Day 2",
    "section": "Would you be in this class?",
    "text": "Would you be in this class?\n\n\n\nIs your skin white?\nAre you blonde?\nDo you have blue eyes?\n\n\n\nWere your ancestors poor?\nAre you Muslim, Hindu, Buddhist, Sikh, Tao, or Jewish?\nDo you identify as LGBTQIQ+?"
  },
  {
    "objectID": "slides/week4-day2.html",
    "href": "slides/week4-day2.html",
    "title": "Week 4, Day 2",
    "section": "",
    "text": "Please make sure you submitted reflections with your revisions! If there are not revisions present when I start grading them tomorrow morning, your revisions are not eligible to be regraded."
  },
  {
    "objectID": "slides/week4-day1.html",
    "href": "slides/week4-day1.html",
    "title": "Introduction to Linear Regression",
    "section": "",
    "text": "Relationships Between Variables\n. . ."
  }
]