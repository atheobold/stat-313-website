---
title: "Introduction to Linear Regression"
format: 
  revealjs:
    theme: style.scss
embed-resources: true
editor: visual
---

```{r packages}
library(tidyverse)
library(openintro)
library(moderndive)

```

```{r ggplot-theme}
#| echo: false

my_theme <- theme_bw() + 
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20), 
        axis.text.x = element_text(size = 14), 
        axis.text.y = element_text(size = 14)
        )
```

```{r post-26-births-table}
births_post28 <- ncbirths %>% 
  drop_na(weight, weeks) %>% 
  filter(weeks > 28)

weeks_lm <- lm(weight ~ weeks, data = births_post28)

births_table <- get_regression_table(weeks_lm)
```

::: {style="font-size: 2em; color: #000000;"}
Relationships Between Variables
:::

. . .

-   In a statistical model, we generally have one variable that is the output and one or more variables that are the inputs.

::: columns
::: {.column width="40%"}
::: {.fragment .fade-in-then-semi-out}
::: {style="font-size: 0.65em;"}
-   Response variable
    -   a.k.a. $y$, dependent
    -   The quantity you want to understand
    -   In this class -- always numerical
:::
:::
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
::: {style="font-size: 0.65em;"}
::: fragment
-   Explanatory variable
    -   a.k.a. $x$, independent, explanatory, predictor
    -   Something you think might be related to the response
    -   Either numerical or categorical
:::
:::
:::
:::

## 

::: {style="font-size: 2em; color: #000000;"}
Visualizing Linear Regression
:::

```{r viz}
#| echo: false
#| eval: true
#| fig-align: center

ggplot(data = bdims, aes(y = wgt, x = hgt)) + 
  geom_point() +
  scale_x_continuous("Explanatory Variable", labels = NULL) + 
  scale_y_continuous("Response Variable", labels = NULL) + 
  theme_bw() + 
  theme(axis.title.x = element_text(size = 36),
        axis.title.y = element_text(size = 36)
        )
```

. . .

</br>

::: columns
::: {.column width="40%"}
-   The scatterplot has been called the most "generally useful invention in the history of statistical graphics."
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
-   It is a simple two-dimensional plot in which the two coordinates of each dot represent the values of two variables measured on a single observation.
:::
:::

## 

::: {style="font-size: 2em; color: #000000;"}
Characterizing Relationships
:::

</br>

::: incremental
-   Form (e.g. linear, quadratic, non-linear)

-   Direction (e.g. positive, negative)

-   Strength (how much scatter/noise?)

-   Unusual observations (do points not fit the overall pattern?)
:::

## 

::: {style="font-size: 2em; color: #000000;"}
Data for Today
:::

The [ncbirths]{style="color: #e28743;"} dataset is a random sample of 1,000 cases taken from a larger dataset collected in North Carolina in 2004.

::: {style="font-size: 0.75em;"}
> Each case describes the birth of a single child born in North Carolina, along with various characteristics of the child (e.g. birth weight, length of gestation, etc.), the child's mother (e.g. age, weight gained during pregnancy, smoking habits, etc.) and the child's father (e.g. age).
:::

## 

::: {style="font-size: 2em; color: #0F4C81;"}
Your Turn!
:::

::: columns
::: {.column width="65%"}
```{r}
#| echo: false
#| out-width: 100%

ncbirths %>% 
ggplot(aes(x = weeks, y = weight)) +
  geom_jitter() + 
  labs(x = "Length of pregnancy (in weeks)",
       y = "Birth weight of baby (in lbs)") +
  my_theme
```
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
How would your characterize this relationship?

-   form
-   direction
-   strength
-   outliers
:::
:::

## Cleaning & Filtering the Data

It seems like pregnancies with a gestation less than 28 weeks have a non-linear relationship with a baby's birth weight, so we will filter these observations out of our dataset.

</br>

```{r}
#| echo: true
#| code-line-numbers: false
#| label: filter-clean-births-post-26-weeks

births_post28 <- ncbirths %>% 
  drop_na(weight, weeks) %>% 
  filter(weeks > 28)
```

. . .

::: callout-caution
# Change in scope of inference

Removing these observations narrows the population of births we are able to make inferences onto!
:::

<!-- ##  -->

<!-- ::: {style="font-size: 2em; color: #000000;"} -->

<!-- What if you added another variable? -->

<!-- ::: -->

<!-- . . . -->

<!-- ::: columns -->

<!-- ::: {.column width="40%"} -->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- #| fig-align: center -->

<!-- ncbirths %>%  -->

<!-- ggplot(aes(x = weeks, y = weight)) + -->

<!--   geom_jitter() +  -->

<!--   labs(x = "Length of pregnancy (in weeks)", -->

<!--        y = "Birth weight of baby (in lbs)") +  -->

<!--   my_theme -->

<!-- ``` -->

<!-- ::: -->

<!-- ::: {.column width="60%"} -->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- #| fig-align: center -->

<!-- ncbirths %>%  -->

<!--   drop_na(habit) %>%  -->

<!--   ggplot(aes(x = weeks, y = weight, color = habit)) + -->

<!--   geom_jitter(alpha = 0.5) +  -->

<!--   labs(x = "Length of pregnancy (in weeks)", -->

<!--        y = "Birth weight of baby (in lbs)",  -->

<!--        color = "Habit") +  -->

<!--   my_theme + -->

<!--   theme(legend.text = element_text(size = 16),  -->

<!--         legend.title = element_text(size = 20)) -->

<!-- ``` -->

<!-- ::: -->

<!-- ::: -->

# Summarizing a Linear Relationship

## 

::: columns
::: {.column width="20%"}
</br> Correlation:
:::

::: {.column width="80%"}
> strength and direction of a *linear* relationship between two *quantitative* variables
:::
:::

. . .

-   Correlation coefficient between -1 and 1
-   Sign of the correlations shows direction
-   Magnitude of the correlation shows strength

## 

::: {style="font-size: 2em; color: #000000;"}
Anscombe Correlations
:::

::: columns
::: {.column width="65%"}
```{r}
#| echo: false
#| out-width: 100%

anscombe <- anscombe %>%
  mutate(id = 1:nrow(.)) %>%
  pivot_longer(cols = -id, names_to = "key", values_to = "value") %>%
  separate(key, into = c("variable", "set"), sep = 1) %>%
  pivot_wider(names_from = variable, values_from = value)

ggplot(data = anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~set)
```
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
::: {style="font-size: 0.75em;"}
Four datasets, very different graphical presentations

-   same mean and standard deviation in both $x$ and $y$
-   same correlation
-   same regression line
:::
:::
:::

. . .

<center>

[For which of these relationships is correlation a reasonable summary measure?]{style="font-size: 1.25em; color: #e28743;"}

## Calculating Correlation in R

```{r}
#| echo: true
#| code-line-numbers: false

get_correlation(births_post28, 
                weeks ~ weight)
```

. . .

</br>

What if I ran `get_correlation(births_post28, weight ~ weeks)` instead? Would I get the same value?

## 

::: {style="font-size: 2em; color: #000000;"}
The Importance of Language
:::

::: incremental
-   The word "correlation" has both a precise mathematical definition and a more general definition for typical usage in English.

-   These uses are obviously related and generally in sync.

-   There are times when these two uses can be conflated and / or misconstrued.
:::

## 

::: columns
::: {.column width="20%"}
</br> Linear regression:
:::

::: {.column width="80%"}
> we assume the the relationship between our response variable ($y$) and explanatory variable ($x$) can be modeled with a linear function, plus some random noise
:::
:::

. . .

::: centered
$response = intercept + slope \cdot explanatory + noise$
:::

## Writing the Regression Equation

</br>

::: columns
::: {.column width="47%"}
::: fragment
**Population Model**

$y = \beta_0 + \beta_1 \cdot x + \epsilon$

</br>

$y$ = response

$\beta_0$ = population intercept

$\beta_1$ = population slope

$\epsilon$ = errors / residuals
:::
:::

::: {.column width="3%"}
:::

::: {.column width="50%"}
::: fragment
**Sample Model**

$\widehat{y} = b_0 + b_1 \cdot x$

</br>

[Why does this equation have a hat on $y$?]{style="color: #e28743;"}
:::
:::
:::

# Linear Regression with One Numerical Explanatory Variable

## Step 1: Fit a linear regression

```{r}
#| echo: true
#| code-line-numbers: false
weeks_lm <- lm(weight ~ weeks, 
               data = births_post28)
```

## Step 2: Obtain coefficient table

```{r}
#| echo: true
#| code-line-numbers: false
#| eval: false
get_regression_table(weeks_lm)
```

```{r}
get_regression_table(weeks_lm) %>% 
  gt::gt()
```

## 

::: {style="font-size: 2em; color: #000000;"}
Our focus (for now...)
:::

![](images/table-focus-estimate-sd.jpeg)

## 

::: {style="font-size: 2em; color: #000000;"}
Estimated regression equation
:::

$$\widehat{y} = b_0 + b_1 \cdot x$$

. . .

```{r}
#| code-line-numbers: false
get_regression_table(weeks_lm) %>% 
  gt::gt()
```

. . .

</br>

<center>

[Write out the estimated regression equation!]{style="color: #e28743; font-size: 1.5em;"}

##  {background-color="#b76352"}

::: columns
::: {.column width="45%"}
::: {style="font-size: 1.75em; color: #000000;"}
How do you interpret the intercept value of `r filter(births_table, term == "intercept")$estimate`?
:::
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
::: fragment
::: {style="font-size: 1.75em; color: #000000;"}
How do you interpret the slope value of `r filter(births_table, term == "weeks")$estimate`?
:::
:::
:::
:::

## 

::: {style="font-size: 2em; color: #000000;"}
Obtaining Residuals
:::

</br>

$\widehat{weight} = `r filter(births_table, term == "intercept")$estimate`+`r filter(births_table, term == "weeks")$estimate` \cdot weeks$

</br>

. . .

[What would the residual be for a pregnancy that lasted 39 weeks and whose baby weighed 7.63 pounds?]{style="color: #e28743; font-size: 1.5em;"}

<!-- ##  -->

<!-- ::: {style="font-size: 2em; color: #000000;"} -->

<!-- A different explanatory variable -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| code-line-numbers: false -->

<!-- #| echo: true -->

<!-- weight_gain_lm <- lm(weight ~ gained, data = births_post26) -->

<!-- get_regression_table(weight_gain_lm) -->

<!-- ``` -->

<!-- . . . -->

<!-- </br> -->

<!-- [Write out this estimated regression equation!]{style="color: #e28743; font-size: 1.5em;"} -->

<!-- ##  -->

<!-- ::: {style="font-size: 2em; color: #000000;"} -->

<!-- How would you choose which model is better?! -->

<!-- ::: -->

<!-- . . . -->

<!-- ```{r} -->

<!-- #| echo: true -->

<!-- #| code-line-numbers: false -->

<!-- births_post26 %>%  -->

<!--   drop_na(weight, gained, weeks) %>%  -->

<!--   summarize(cor_weeks = cor(weight, weeks),  -->

<!--             R_sq_weeks = cor_weeks^2,  -->

<!--             cor_gained = cor(weight, gained), -->

<!--             R_sq_gained = cor_gained^2) -->

<!-- ``` -->

# Linear Regression with One Categorical Explanatory Variables

## Step 1: Finding `distinct` levels

```{r}
#| code-line-numbers: false
#| echo: true

distinct(births_post28, habit)
```

## 

::: {style="font-size: 1.75em; color: #000000;"}
Step 2: Fit a linear regression
:::

```{r}
#| code-line-numbers: false
#| echo: true
habit_lm <- lm(weight ~ habit,
               data = births_post28)

```

. . .

</br>

::: {style="font-size: 1.75em; color: #000000;"}
Step 3: Obtain coefficient table
:::

```{r}
#| code-line-numbers: false
#| echo: true
#| eval: false
get_regression_table(habit_lm)
```

```{r}
get_regression_table(habit_lm) %>% 
gt::gt()
```

. . .

::: centered
::: {style="font-size: 2em;"}
ðŸ¤”
:::
:::

## Estimated Regression Equation

```{r}
get_regression_table(habit_lm) %>% 
gt::gt()
```

</br>

. . .

$$\widehat{weight} = 7.23 - 0.4 \cdot Smoker$$

</br>

. . .

[But what does $Smoker$ represent???]{style="color: #e28743; font-size: 1.25em;"}

## 

::: {style="font-size: 2em; color: #000000;"}
Indicator Variables
:::

<center>$$
  \widehat{y} = b_0 + b_1 \cdot x
$$</center>

. . .

::: columns
::: {.column width="40%"}
$x$ is a categorical variable with levels:

-   `"nonsmoker"`
-   `"smoker"`
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
We need to convert to:

-   a "baseline" group
-   "offsets" / adjustments to the baseline
:::
:::

. . .

[Based on the regression table, what `habit` group was chosen to be the baseline?]{style="color: #e28743; font-size: 1.25em;"}

## A More Intuitive Equation

$$\widehat{weight} = 7.23 - 0.4 \cdot 1_{smoker}(x)$$

where

<center>

$1_{smoker}(x) = 1$ if the mother was a `"smoker"`

$1_{smoker}(x) = 0$ if the mother was a `"nonsmoker"`

## Obtaining Group Means

$$\widehat{weight} = 7.23 - 0.4 \cdot 1_{Smoker}(x)$$ </br>

Given the equation, what is the estimated mean birth weight for nonsmoking mothers?

</br>

For smoking mothers?

# Choose Your Adventure

## [Statistical Critique](../critique/critique-1.qmd)

::: small
1.  Get out the article you picked for *Statistics in Your Field* (Week 1)

2.  Choose a visualization from the article
:::

::: callout-tip
# Use a table if there are no visualizations
:::

::: small
3.  Where are **variables** being used in the visualization / table? Those are the aesthetics!

4.  Write a critique of the good / bad aspects of the plot

*Repeat with a "pop" visualization from the [New York Times](https://www.nytimes.com/column/whats-going-on-in-this-graph).*
:::

## Midterm Project Proposal

::: small
1.  Choose a dataset

2.  Choose a numerical response variable

3.  Choose a numerical explanatory variable

4.  Choose a second explanatory variable, either numerical or categorical
:::

::: callout-tip
# Checking values of your numerical variable(s)

::: small
Your numerical variable **cannot** have a small (e.g., 2 or 3) number of values. You can use the `distinct()` function to determine the unique values of your variable. For example, by running `distinct(hbr_maples, year)` I would discover that `year` only has two values (2003 and 2004), meaning `year` is not eligible to be a numerical response or explanatory variable. It could, however, be a categorical explanatory variable!
:::
:::

::: small
5.  Write your Introduction
:::
